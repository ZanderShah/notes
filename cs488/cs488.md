CS 488 (Exam Review)
=

> No shader questions, probably no coding questions. Set of [Sample Exam Questions](https://www.student.cs.uwaterloo.ca/~cs488/Fall2019/q.pdf), likely that a few appear on the exam. Questions that have not showed up on an exam in a long time are less likely to be asked.

2. History (Nothing).
3. Devices (Nothing).
4. Device Interfaces (Nothing).
5. [**Geometries**](#geometries) (Won't be tested in depth).
6. [**Affine Geometries & Transformations**](#affine-geometries-transformations).
7. [**Windows, Viewports, NDC**](#windows-viewports-ndc).
8. [**Line Clipping**](#line-clipping).
9. [**Projections**](#projections).
10. A2 (Nothing).
11. [**Polygons**](#polygons).
    - At least clipping, scan conversion (concept).
12. [**Hidden Surface Removal**](#hidden-surface-removal).
    - Backface Culling, Painter's Algorithm, Warnock, Z-Buffer.
13. [**Hierarchical Models & Transformations**](#hierarchical-models-transformations).
14. [**Rotations About Arbitrary Axis**](#rotations-about-arbitrary-axis).
    - Focused on Euler vs. Trackball.
15. Picking (Nothing)
16. [**Colour** (Minor things)](#colour).
17. [**Lighting**](#lighting).
    - Diffuse, Specular.
18. [**Shading**](#shading).
    - Flat, Gouraud, Phong.
19. Graphics Hardware (Nothing).
20. ++ [**Ray Tracing**](#ray-tracing) ++
    - Around 30% of the exam.
    - Shadows, CSG, Texture / Bump Mapping.
21. [**Aliasing**](#aliasing).
22. [**Bidirectional Tracing** (Not tested in depth)](#bidirectional-ray-tracing).
23. [**Radiosity** (Not tested in depth)](#radiosity).
24. [**Photon Mapping** (Not tested in depth)](#photon-mapping).
25. [**Shadows, Projective, Shadow Maps, Volumes** (Not tested in depth)](#25).
26. [**Modelling Stuff** (Short answers only if anything)](#modelling-stuff).
27. Polyhedral Data Structures (Nothing).
28. [**Splines, De Casteljau's Algorithm** (Lightly tested)](#splines-de-casteljaus-algorithm).
    - Know De Casteljau's.
29. [**Non-Photorealistic Rendering** (Very lightly tested)](#non-photorealistic-rendering).
30. Volume Rendering (Nothing).
31. [**Animation** (Might be short questions)](#animation).
32. Computational Geometry (Nothing).

## Geometries

### Vector Spaces

> Set of vectors $V$ with two operations.

1. **Addition**: $u + v \in V$.
2. **Scalar Multiplication**: $\alpha v \in V$, where $\alpha$ is a member of some field $\mathbb{F}$.

**Axioms**.

- **Addition Commutes**: $u + v = v + u$.
- **Addition Associates**: $(u + v) + w = u + (v + w)$.
- **Scalar Multiplication Distributes**: $\alpha(u + v) = \alpha u + \alpha v$.
- **Unique Zero Elements**: $0 + u = u$.
- **Field Unit Element**: $1 u = u$.

### Span

Suppose $B = \{v_1, v_2, ..., v_n\}$. $B$ **spans** $V$ if and only if any $v \in V$ can be written as $v = \sum_{i=1}^n \alpha_i v_i$ (**linear combination** of the vectors in $B$).

- Any minimal spanning set is a basis. All bases are the same size.
- The number of vectors in any basis is the **dimension**.

### Affine Spaces

> Now we use a set of points $P$ in addition to the set of vectors $V$.

- Points can be combined with vectors to make new points. $P + v \to Q$, with $P, Q \in P$ and $v \in V$.
- Basis now requires an affine extension. **Frame** is a vector basis plus a point $O$ (**origin**), with the same dimension as the basis.

**Inner Product Spaces**: Binary operator which is commutative, associative, scalar multiplication distributes, $u \cdot u \ge 0$ if and only if $u = 0$.

### Euclidean Spaces

**Metric Space**: Space with a **distance metric** $d(P, Q)$ defined. Requires distance be non-negative, zero if and only if the points are identical, commutative, and satisfy triangle inequality.

**Euclidean Space**: Metric space based on a dot (inner) product, $d^2(P, Q) = (P - Q) \cdot (P - Q)$.

- **Norm**: $|u| = \sqrt{u \cdot u}$.
- **Angle**: $cos(\angle u v) = \frac{u \cdot v}{|u||v|}$.
- **Perpendicularity**: $u \cdot v = 0 \Rightarrow u \perp v$. Perpendicularity is not an affine concept.

### Cartesian Space

An Euclidean Space with a standard **orthonormal frame** $(i, j, k, O)$.

- **Orthogonal**: $i \cdot j = j \cdot k = k \cdot i = 0$.
- **Normal**: $|i| = |j| = |k| = 1$.

> For notation, we specify the **Standard Frame** $F_s = (i, j, k, O)$.

Since points and vectors are different objects with different operations and behave differently under transformations how do we handle them?

**Coordinates**: We use an *extra coordinate*.

- $v = (v_x, v_y, v_z, 0)$.
- $P = (p_x, p_y, p_z, 1)$.

### Why Do We Need Affine Spaces?

- No metric, but we can add a metric to vector space.
- We want to represent objects efficiently, and we want to be able to translate, rotate, scale our objects in their representation. This is difficult to do with vectors.
    - For example, suppose we represent a position by the sum of two vectors. We cannot naively apply a translation to both vectors to translate the position.

## Affine Geometries & Transformations

### Linear Combinations

- Vector-Vector addition.
- $T(u + v) = T(u) + T(v)$. T(\alpha u) = \alpha T(u).
- Point-Vector addition.

### Affine Combinations

- **Point Subtraction**: $Q - P = v \in V$ such that $Q = P + v$, for $P, Q \in P$. So $\sum a_i P_i$ is a vector if and only if $\sum a_i = 0$.
- **Point Blending**: $Q = (1 - \alpha)Q_1 + \alpha Q_2$ such that $Q = Q_1 + \alpha(Q_2 - Q_1) \in P$. So $\sum a_i P_i$ is a point if and only if $\sum a_i = 1$.
    - $\frac{|Q - Q_1|}{|Q - Q_2|} = \frac{1 - \alpha}{\alpha}$.
- So when combining points, the result is a point if the coefficients sum to $1$, and the result is a vector if the coefficients sum to $0$.

### Affine Transformations

Let $T: A_1 \to A_2$, where $A_1, A_2$ are affine spaces.

- $T$ maps vectors to vectors and points to points.
- $T$ is a linear trasformation on the vectors.
- $T(P + u) = T(P) + T(u)$, where $P \in P$, $v \in V$.

Then $T$ is an affine transformation. Preserves affine combinations on the points.

Suppose $T$ is only defined on $P$. Then $T(v) = T(Q) - T(R)$, where $v = Q - R$.

### Mapping Through an Affine Transformation

Let $A, B$ be affine spaces, with $T: A \to B$ be an affine transformation. Let $F_A = (v_1, v_2, O_v)$ be a frame for $A$, let $F_B = (w_1, w_2, O_w)$ be a frame for $B$. Let$P$ be a point in $A$ whose *coordinates* relative to $F_A = (p_1, p_2, 1)$. What are the coordinates $(p_1^\prime, p_2^\prime, 1)$ of $T(P)$ relative to frame $F_B$?

$\begin{aligned}
T(P) &= T(p_1 v_1 + p_2 v_2 + O_v) \\
&= p_1T(v_1) + p_2 T(v_2) + T(O_v) \\
\end{aligned}$

### Geometric Transformations

- **Rotation**: $\begin{bmatrix}\cos(\theta) & -\sin(\theta) & 0 \\ \sin(\theta) & \cos(\theta) & 0\\ 0 & 0 & 1\end{bmatrix}$.
- **Shear**: $\begin{bmatrix}1 & \beta & 0 \\ \alpha & 1 & 0\\ 0 & 0 & 1\end{bmatrix}$.
- Translation is a shear in the next dimension.

### Change of Basis

Suppose we want to change coordinates relative to $F_1$ to coordinates relative to $F_2$. We know that $P = [x, y, 1]^T$ relative to $F_1 = (w_1, w_2, O_w)$. Solve $F_1 = F_2 M_{1, 2}$.

- When $F_2$ is orthonormal, $f_{i,j} = w_j \cdot v_i$, $f_{i,3} = (O_w - O_v) \cdot v_i$.
- Points "mapped" by a change of basis do not change, they are just represented in a different frame.
- To fully specify a transformation, we need a matrix, a domain space, a range space, and a coordinate frame in each space.

### World and Viewing Frames

- Standard frame is the **world frame**.
- Viewer may be anywhere and looking anywhere. Specified as $z, y$ with $z$ as the **view direction** and $z$ is the **up vector**.
- We change basis from the world frame to the viewers frame.

After we are in viewing coordinates, we place a clipping box around the scene relative to the viewing frame.

- The screen is 2D, so an **orthographic projection** is made by removing the z-coordinate.

### Transforming Normals

- Consider a non-uniform scale of a circle and the effect on the normal vector. The normal vector will be scaled as well which is incorrect.
- This is because normal vectors are **not** the difference in points.
    - But tangent vectors **are**, and normals are vectors perpendicular to all tangents at a point.
    - We can prove we should transform normals by the inverse transpose of the linear part of the transformation (upper 3 x 3 submatrix).
    - This is only an issue if there is a non-uniform scale or a shear transformation.

## Windows, Viewports, NDC

- **Window**: Rectangular area of interest in the scene.
- **Viewport**: Rectangular region on device.

Window to Viewport mapping is simple a scale based on the ratios and an offset based on their positions.

- When the ratios between the heights and lengths of the two regions are not the same, the image will be distorted.

### Normalized Device Coordinates

We want to specify the viewport in a generalized way so that it works on all plaforms / devices. So we use **Normalized Device Coordinates** as an intermediate coordinate system.

## Line Clipping

**Clipping** is removing points outside a region of interest.

- **Point clipping**: Points are either entirely inside the region or not.
- **Line clipping**: Halfspaces can be combined to bound a convex region.
    - Liang-Barsky algorithm efficiently clips line segments to a halfspace.
    - When a line segment is parially inside and partially outside a halfspace, we generate a new line to represent the part inside.

```python
for P, n in edges: # Halfspaces
    wecA = dot(A - P, n)
    wecB = dot(B - P, n)

    if wecA < 0 and wecB < 0:
        reject # Outside
    if wecA >= 0 and wecB >= 0:
        next # Inside

    t = wecA / (wecA - wecB)

    if wecA < 0:
        A = A + t * (B - A)
    else:
        B = A + t * (B - A)
```

## Projections

### Perspective Projection

- Identify all points with a line through the eyepoint.
- Take intersection with viewing plane as projection.
- This is **not** an affine transformation, but a perspective projection.
- Angles and distances are not preserved, but they are not preserved under affine transformations.
- Ratios of distances are not preserved, affine combinations are not preserved.
- Straight lines are mapped to straight lines.
- *Cross* ratios are preserved. $\frac{|AC| / |CD}{|AB| / |BC|}$.
- Compared to affine transformations, they require 1 extra point or vector $(n + 2)$ to define a projection map in $n$ dimensional space.

### Perspective Map

> For projection plane $z = d$.

- By similar triangles, $P = \left(\frac{xd}{z}, \frac{yd}{z}, d\right)$
- We need to know what is in front, which is impossible because the map loses $z$ information.
- $\begin{bmatrix}1 & 0 & 0 & 0 \\0 & 1 & 0 & 0\\0 & 0 & 1 & 1 \\ 0 & 0 & 1 & 0 \end{bmatrix}\begin{bmatrix}x \\y\\z\\1\end{bmatrix} = \begin{bmatrix}x\\y\\z+1\\z\end{bmatrix}$ maps $x, y$ to $\frac{x}{z}, \frac{y}{z}$, and maps $z$ to $1 + \frac{1}{z}$. So we retain depth information.
- $\begin{bmatrix}1 & 0 & 0 & 0 \\0 & 1 & 0 & 0\\0 & 0 & \frac{f+n}{f-n} & \frac{-2fn}{f-n} \\ 0 & 0 & 1 & 0 \end{bmatrix}\begin{bmatrix}x \\y\\z\\1\end{bmatrix} = \begin{bmatrix}x\\y\\\frac{z(f + n) - 2fn}{f - n}\\z\end{bmatrix}$ is used for mapping the near and far clipping planes to $[-1, 1]$.
- When fov-y is given, we need to include this in the matrix as $\cot(\theta / 2)$.

### 3D Clipping

- We should clip the near plane **before** projection to avoid divide by 0.
- Clipping to other planes *could* be done before projection but it is easier to clip after projection because we will have a cube instead of the truncated viewing pyramid.

## Polygons

> Area primitive.

- Simple polygon is planar set of ordered points, no holes, no crossing.
- Convention is to have points ordered in counter-clockwise order, so we have a defined interior and exterior.
- Affine transformations may introduce degeneracies. Orthographic projection may project entire polygon to a line segment.

### Polygon Clipping

- Window must be a convex polygon. Polygon to be clipped does not need to be convex.
- Given a polygon represented as $v_1,..., v_n$, we process all polygon edges in succession against a window edge, $w_1, ..., w_n$. Repeat for every window edge.

**Algorithm**: Four cases to consider.

1. Polygon edge is entirely inside the window edge. 
2. Polygon edge crosses window edge going out.
    - Intersection point $i$ is the next vertex of the resulting polygon.
3. Polygon edge is entirely outside the window edge.
    - No output.
4. Polygon edge crosses window going in.
    - Intersection point $i$ and $p$ are the next two vertices of the resulting polygon.

### Polygon Scan Conversion

- Complicated in general, we look at scan conversion of triangle.
- Split triangle with horizonal line at middle $y$ value, so we have an axis-aligned edge.
- Scan convert the triangle by calculating slopes and iterating over every horizontal line (floating point algorithm).

## Hidden Surface Removal

> When we have a lot of polygons, we want to draw only those visible to the viewer.

### Backface Culling

- Remove all *backfacing* polygons. Polygons are backfacing if their normal is facing away from the viewer. So cull the polygon if $N \cdot V > 0$.
- Not a complete solution (used in conjuction with more complete algorithm), but easy to integrate into hardware and usually improves performance by a factor of 2.

### Painter's Algorithm

- Sort polygons on farthest $z$.
- Resolve ambiguities where $z$'s overlap.
- Scan convert from largest $z$ to smallest $z$.
- Some cases are simple but others are not. $\Omega(n^2)$ algorithm.

### Warnock's Algorithm

> Divide and conquer algorithm.

- Draw the polygon-list if it is simple in the viewport, otherwise split the viewport vertically and horizontally then recursively draw.
- Simple means there is no more than one polygon in the viewport, or that the viewport is only 1 pixel in size (shade pixel based on closest polygon).
- $O(pn)$, where $p, n$ are the number of pixels and polygons.

### Z-Buffer Algorithm

- Perspective transformatoin maps polygons to polygons.
- Scan convert while also stepping in $z$.
- In addition to framebuffer, have a depth buffer ($z$ buffer) to write $z$ values.
- Update colour in framebuffer if the $z$ value is less than current.
- $O(p_c + n)$, where $p_c, n$ are the number of scan converted pixels and polygons.
- Easy to implement (hardware too), online algorithm.
- Doubles memory requirements (memory is cheap).
- Scale / device dependent.

## Hierarchical Models & Transformations

> How do we model complex objects and scenes?

- Define basic 3D primitives in some nice way in their own space.
- Use transformations to put primitives together.
- Use hierarchy of spaces to build complex models (DAG).
    - DFS of the DAG, using a **matrix stack**.
    - Primitives occur at leaf nodes, transformations occur at internal nodes.

## Rotations About Arbitrary Axis

> Rotation given by $a = (x, y, z)$ and $\theta$. Map $a$ onto one of the canonical axes, rotate by $\theta$, then map back.

1. Pick closest axis to $a$. Assume we chose the $x$-axis.
2. Project $a$ onto $b$ in the $xz$ plane.
3. Compute $\cos(\phi) = \frac{x}{\sqrt{x^2 + z^2}}$, $\sin(\phi) = \frac{z}{\sqrt{x^2+z^2}}$.
4. Create $R(-\phi)$.
5. Rotate $a$ onto the $xy$ plane using $R(-\phi)$. $c = R_y(-\theta)a$.
6. Compute $\cos(\gamma)$, $\sin(\gamma)$, where $\gamma$ is the angle of $c$ with the $x$ axis.
7. Use $\cos(\gamma)$ and $\sin(\gamma)$ to create $R_z(-\gamma)$.
8. Rotate $c$ onto the $x$-axis using $R_z(-\gamma)$.
9. Rotate around the $x$-axis by $\theta$, $R_x(\theta)$.
10. Reverse $z$-axis rotation.
11. Reverse $y$-axis rotation.

So the overall transformation is $R(\theta, a) = R_y(\phi)R_z(\gamma)R_x(\theta)R_z(-\gamma)R_y(-\phi)$.

### 3D Rotation User Interfaces

- **Virtual Sphere**: Given two sequential samples of mouse position $S, T$, map $S$ to point on sphere, map $ST$ to tangential velocity. Use to rotate. When $S$ is outside of the sphere we do $z$-axis rotation.
- **Arcball**: Rather than using $T$ as tangent, map $T$ to point on the sphere as well and rotate the ball so that $S$ moves to $T$.

## Colour

- Light sources emit intensity $I(\lambda)$, assigns intensity to each wavelength of light.
- Humans perceive $I(\lambda)$ as colour. Normal human retina has three types of colour receptors which respond most strongly to short, medium, or long wavelengths.

### Tri-Stimulus Colour Theory

- Model visual system as linear map, from wavelength to three dimensional vector space.

### Colour Systems

- RGB (Red, Green, Blue) Additive.
- CMY (Cyan, Magenta, Yellow) Subtractive. Complement of RGB.
- HSV (Hue, Saturation, Value). Cone shaped colour space.
- CIE XYZ. More complete colour space.
- YIQ. Backwards compatible with black-and-white TV.


## Lighting

> Given a point on the surface visible through a pixel, what colour should we assign it?

- Want to smoothly shade objects in the scene.
- Shading done quickly (interactive speeds).

**Initial Assumptions**

- Linearity of reflection.
- Energy conservation.
- Full spectrum of light is representable by three floats (RGB).

### Lambertian Reflection

> Assume that incoming light is partially absorbed, then the remainder of energy is propogated equally in all directions.

- Approximates matte materials.
- $L_{out}(v) = \rho(v, l)E_{in}(l)$. We assumed that outgoing radiance is equal in all directions so $\rho$ is a constant.
- $L_{out}(v) = k_d E_{in}(l) = k_d L_{in}(l) l \cdot n$. For the complete environment we take the integral over all possible directions. Taken as the sum over all light sources in practice.

### Attenuation

- No attenuation for directional lights, because the energy goes not spread out.
- For point light sources, $L_{in}(l) \propto \frac{1}{r^2}$, where $r$ is the distance from light to $P$.
    - Too harsh in practice because real lighting is from area sources, multiply reflections in environment.
    - We use $L_{in}(l) = \frac{I}{c_1 + c_2r + c_3r^2}$. We do not attenuate light from $P$ to the screen.

### Ambient Light

- Lambertian only models direct illumination.
- Ambient illumination is a simple approximation to global illumination.
- Assume everything gets uniform illumination in addition to direct illumination. $L_{out}(v) = k_a I_a + \sum_i \rho(v, l_i) I_i \frac{l_i \cdot n}{c_1 + c_2 r_i + c_3 r_i^2}$.

### Specular Reflection

- Lambertian models matte but not shiny.
- Shiny surfaces have highlights.
- Phong Bui-Tuong developed empirical model. $L_{out}(v) = k_a I_a + k_d (l \cdot n) I_d + k_s (r \cdot v)^p I_s$. Classic **Phong lighting model**.
    - Vector $r$ is $l$ reflected by the surface. $r = -l + 2(l \cdot n)n$.
    - Exponent $p$ controls sharpness of highlight. Small $p$ gives wide highlight, large $p$ gives narrow highlight.
- Blinn introduced variation, **Blinn-Phong lighting model**. $L_{out} k_a I_a + k_d(l \cdot n) I_d + k_s (h \cdot n)^p I_s$, with $h = \frac{v + l}{|v + l|}$ measuring deviation from the ideal mirror configuration.

## Shading

- We have lighting calculation for a point, so we need surface normals at every point.
- Surface is often polygonal.

### Flat Shading

- Shade entire polygon one colour.
- Surface will look faceted. This is acceptable if it really is a polygonal model, not good if it is an approximation of a curved surface.

### Gouraud Shading

- Interpolate colours across a polygon from the vertices.
- Lighting calculation only performed at vertices.
- Well-defined for triangles. Extends to convex polygons but better to just convert to triangles.
    - We could slice convex polygon horizontally then interpolate colours along each scanline. This will not produce consistent shading after rotations. 
    - Triangluation is expensive and it will be visible to the viewer.

### Phong Shading

- Interpolate lighting model parameters instead of colours.
- **Normal** is specified at every vertex of a polygon, interpolated using the Gouraud technique.
- Simulated with programmable vertex and fragment shaders on modern graphics hardware.

## Ray Tracing

- Want more realistic images with shadows and reflections.

```python
for pixel in pixels:
    ray = (eye, pixel - eye)
    Intersect(Scene, ray)
```

- **Setting**: eyepoint, virtual screen (array of virtual pixels).
- **Ray**: Half-line determined by eyepoint and a point associated with a chosen pixel.
- **Interpretations**: Ray is a path of photons that reach the eye.

### Intersection Computations

- Express ray in parametric form $E + t(P - E)$, $t > 0$.
- **Direct Implicit Form**: Express object as $f(Q) = 0$ when $Q$ is a surface point, intersection equation is solving for $t$ such that $f(E + t(P - E)) = 0$.
- **Procedural Implicit Form**: $f$ is defined procedurally.

#### Quadratic Surfaces.

- **Ellipsoid**: $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$.
- **Elliptic Paraboloid**: $\frac{x^2}{p^2} + \frac{y^2}{q^2} = 1$.
- **Hyperboloid of One Sheet**: $\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = 1$.
- **Elliptic Cone**: $\frac{x^2}{p^2} + \frac{y^2}{q^2} - \frac{z^2}{r^2} = 0$.
- **Elliptic Cylinder**: $\frac{x^2}{p^2} + \frac{y^2}{q^2} = 1$.
- Instead of solving generally, we can solve for simpler versions then just scale results (intersections in model space).

### Shading

- At closest intersection point, perform Phong shading.
- Before adding contribution from a light, cast a ray to the light. If the ray hits an object before the light, then don't shade. This gives **shadows**.

### Reflections

- Cast ray in the mirror direction.
- Add colour coming from this ray to the shade of the initial ray.

### Recursive Ray Tracing

- Eye-screen ray is the *primary* ray.
- Generate secondary rays for light sources, reflections, refractions, and recurse.
- Accumulate averaged information for primary ray.

### Ray Casting

- Stop before generating secondary rays.

### Surface Normals

- Illumination models require surface normal vectors at intersection points.
- Normals to polygons are provided by planar normal or cross product of adjacent edges.
- Normals to any implicit surface use calculus.

#### Normal Transformations

> How do affine transformations affect surface normals?

- Recall from [Affine Geometrics & Transformations](#affine-geometries-transformations) that normals are transformed by the inverse transpose of the upper 3 x 3 portion of the transformation matrix.

### Modeling and CSG

- Hierarchical modeling works for ray tracer too.
- In *Constructive Solid Geometry* all primitives are solids.
- New type of internal node in DAG, boolean operations (Intersection, Union, Difference).
    - Complete ray intersect object with every primitive, then perform boolean operations on the set of resulting line segments.
    - Segments must be transformed on the way back up the DAG similar to how the ray is transformed.

### Texture Mapping

> Adding detail by increasing model complexity is costly. When the detail is surface detail, we can use texture mapping.

- Scan a photo of the detail and paste it onto objects.
- Associate texture with polygon.
- Map pixel onto polygon then into texture map.
- Use weighted average of covered texture to compute colour.

### Bump Mapping

- Textures will still appear smooth (no shadows).
- We perturb the normal, use for lighting calculation.

### Solid Textures

- Hard to texture map onto curved surfaces.
- Use a 3D texture instead. Usually procedural.

### Bounding Boxes, Spatial Subdivision

- Ray tracing is slow, often because of ray intersect object.
- Improvements come in two forms; reduce the cost of ray intersect object or intersect the ray with fewer objects.
- **Bounding Boxes**: Place bounding box around complicated geometry. Only compute ray intersect object if the ray intersects the bounding box. Cheap to compute if the box is axis-aligned.

### Spatial Subdivision

- Divide space into subregions.
- When tracing ray, only interesect with objects in sub-regions the ray passes through.
- Useful when there are a lot of small objects in the scene.

## Aliasing

- Images sample a continuous function. When the samples are spaced too far apart, don't get a true representation of the scene.
    - Stairsteps.
    - Moire Patterns.
    - Loss of small objects.

### Nyquist Limit

- Sampling rate which is twice the highest frequency.
- Avoids aliasing.
- Doesn't work for man made objects because they have distinct edges with infinite frequency.

### Area Sampling

- Instead of sampling, we could integrate.

### Weighted Sampling

- Unweighted is just $I = \int_{x \in Box} I_x dI$.
- Weighted gives different weights depending on position in the pixel. For example, positions closer to the center of the pixel may be weighted higher than positions near the edge.

### Anti-aliasing in Ray Tracing

- Integration not possible, instead we point sample the image.
- **Super Sampling**: Take more samples and weight with filter. Sampling pattern should not be regular or we will still get aliasing.
- **Stochastic Sampling**: Jitter samples.

## Bidirectional Tracing

> Some effects such as caustics require tracing lights from light back to eye.

### Radiosity

- Model diffuse interaction of light only in steady state.
- Discretize scene into polygons and assume polygons emit constant light over surface.
- Determine interaction between pairs of polygons.
- Solve a large linear system which is expensive but the result is reusable.

### Distribution Ray Tracing

- Ray trace, but at every reflection cast multiple rays based on a distribution function that represents reflection properties.
- Distribution over reflected direction gives soft reflections, over light area gives soft shadows, over aperature gives depth of field, over time gives motion blur.
- Can handle caustics in theory but needs a lot of rays.

### Bidirectional Path Tracing

> Trace paths from eye and from light and connect them. Trace paths (single reflected ray).

- Reflect with distribution function.
- Lots of rays, low error, high noise.
- Less expensive than distribution ray tracing.

### Photon Maps

- Cast rays from lights, create a *photon map* wherever light hits.
- Use standard ray casting to determine lighting.
    - Density estimation to convert photon map to intensity.
- Fast, easy to program, high noise.

### Metropolis Algorithm

- Take one path from light to eye, perturb the *path* and see if it still reaches the eye.

## Radiosity

## Photon Mapping

## Shadows, Projective, Shadow Maps, Volumes

## Modelling Stuff

## Splines, De Casteljau's Algorithm

## Non-Photorealistic Rendering

## Animation
