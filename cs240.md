CS 240
=

# Algorithm Design

**Problem**: Given a problem instance, carry out a particular computational task.

**Problem Instance**: Input.

**Problem Solution**: Output.

**Size of a problem instance**: Size(I) is a positive integer that measures the size of instance I.

**Algorithm**: Step-by-step process for carrying out a series of computations, given an abitrary problem instance I.

For a problem $\Pi$, we can have several algorithms. For an algorithm $A$ solving $\Pi$, we can have several programs (implementations).

- Designing an algorithm $A$ that solves $\Pi$ -> **Algorithm Design**.
- Assessing correctness and efficiency of $A$ -> **Algorithm Analysis**.

# Analysis of Algorithms I

We may be interested in the amount of **time** or **memory**. Experimental shortcomings have shortcomings because of implementations, hardware, and testing against all inputs.

Instead we write code for Random Access Machines (RAM)

- Set of memory cells, each of which stores one item of data.
- Access to memory in O(1).
- Primitive operations are O(1).
- Runtime of a program is the number of memory accesses + the number of primitive operations.

The efficiency is measured in terms of it's **growth rate** (this is called the **complexity** of the algorithm).

```python
for i in range(n):
    for j in range(n):
        c[i][j] = 0
        for k in range(n):
            c[i][j] += a[i][k] * b[k][j]
```
*About* $n^3$ operations.

# Asymptotic Notation
O-Notation $f(n) \in O(g(n))$ if there exists constants $c > 0, n_0 > 0$ such that if $|f(n)| \le c|g(n)|$ $\forall n \ge n_0$.

Example: $f(n) = 75n + 500$ and $g(n) = 5n^2$.

> $f(n), g(n) \ge 0$ $\forall n \ge 1$    
> For $n \ge 20$    
> $n^2 \ge 20n$    
> $5n^2 \ge 100n$    
> $25n \ge 25 * 20 = 500$    
> $5n^2 \ge 75n + 500$    
> $g(n) \ge f(n)$    
> Taking $n_0 = 20, c = 1$, this proves that $f(n) \in O(g(n))$.    

We want a **tight** asymptotic bound.

$\Omega$-notation: $f(n) \in \omega(g(n))$ if $\exists c > 0, n_0 > 0$ such that $c|g(n)| \le |f(n)| \forall n \ge n_0$.

$\Theta$-notation: $\exists c1, c2 \ge 0, n_0 > 0$ such that $c1|g(n)| \le |f(n)| \le c2|g(n)| \forall n \ge n_0$.

How to express $f(n)$ is asymptotically strictly smaller than $g(n)$?

o-notation: $\forall c > 0, \exists n_0 > 0$ such that $|f(n)| < c|g(n)| \forall n \ge n_0$.

$\omega$-notation: $\forall c > 0, \exists n_0 > 0$ such that $0 \le c|g(n)| < f(n) \forall n \ge n_0$.

Example: $f(n) = 2000n^2 + 5000n$ and $g(n) = n^3$. Prove $f(n) \in o(g(n))$.

> Given $c > 0$, for $n > 0$, $f(n) = 2000n^2 + 5000n \le 7000n^2$.    
> $7000n^2 < cn^3 \Leftrightarrow 7000 < cn \Leftrightarrow n > \frac{7000}{c}$    
> So if we take $n_0 = \frac{7000}{c} + 1$, we have $f(n) < g(n) \forall n \ge n_0$. This proves that $f(n) \in o(g(n))$.    

## Asymptotic Identities

> $f(n) \in \Theta(g(n)) \Leftrightarrow g(n) \in \Theta(f(n))$    
> $f(n) \in O(g(n)) \Leftrightarrow g(n) \in \Omega(f(n))$    
> $f(n) \in o(g(n)) \Rightarrow f(n) \in O(g(n)), \nRightarrow f(n) \in \Omega(g(n))$    
> $f(n) \in \omega(g(n)) \Rightarrow f(n) \in \Omega(g(n)), \Rightarrow f(n) \notin O(g(n))$    
> *Identitity*: $f(n) \in \Theta(f(n))$    
> *Maximum*: $f(n) > 0, g(n) > 0 \forall n \ge n_0 \Rightarrow O(f(n) + g(n)) = O(\max\{f(n), g(n)\})$. Similar for $\Omega$.    
> *Transitivity*: $f(n) \in O(g(n)), g(n) \in O(h(n)) \Rightarrow f(n) \in O(h(n))$. Similar for $\Omega$.    

## Limit Test

Suppose $L = \lim_{n \to \infty} \frac{f(n)}{g(n)}$.    

$$f(n) \in \begin{cases}
o(g(n)), &L = 0 \\
\Theta(g(n)), &0 < L < \infty \\
\omega(g(n)), &L = \infty
\end{cases}$$

Example: $f(n) = \log(n) = \frac{\ln(n)}{\ln(2)}$, $g(n) = n$.

> $\lim_{n \to \infty} \frac{1}{n\ln(2)} = 0$ so $f(n) \in o(g(n))$ by the limit test.

# Analysis of Algorithms II

Example:

```python
def Test1(n):
    sum = 0
    for i in range(1, n + 1):
        for j in range(i, n + 1):
            sum += (i - j) * (i - j)
    return sum
```

> Let $T_1(n)$ be the runtime of $Test1(n)$.  
> $T_1(n) \in \Theta(S_1(n))$ where $S_1(n)$ is the number of times we enter the body of the inner loop on line 4.  
>
> $S_1(n) = \sum_{i = 1}^n \sum_{j = i}^n i$.

**1st solution**: Brute Force.

> $S_1(n) = \sum_{i = 1}^n (n - i + 1) = \sum_{i = 1}^n n - \sum_{i = 1}^n + \sum_{i = 1}^n 1 = n^2 - \frac{n(n+1)}{2} + n = \frac{n^2}{2} + \frac{n}{2}$.   
> Therefore we have $S_1(n) \in \Theta(n^2)$.

**2nd solution**: Separate $O$ and $\Omega$.

> $S_1(n) \leq \sum_{i = 1}^n \sum_{j = 1}^n 1 = n^2$ so $S_1(n) \in O(n^2)$.   
> $S_1(n) \geq \sum_{i = 1}^{n / 2} \sum_{j = 1}^n i \geq \sum_{i = 1}^{n / 2} \sum_{j = n / 2}^n 1 = \frac{n^2}{4}$ so $S_1(n) \in \Omega(n^2)$.  
> Therefore $S_1(n) \in \Theta(n^2)$.

Example:

```python
def Test2(A, n):
    max = 0
    for i in range(1, n + 1):
        for j in range(i, n + 1):
            sum = 0
            for k in range(i, j + 1):
                sum += A[k]
    return max
```

> Let $T_2(n)$ be the runtime of $Test2(n)$.  
> Then $T_2(n) \in \Theta(S_2(n))$, where $S_2(n)$ is the number of times we enter the body of the inner loop on line 6.
>
> $S_2(n) = \sum_{i = 1}^n \sum_{j = i}^n \sum_{k = i}^j 1$.

**1st Solution**: Brute Force.

> Input to Wolfram $\alpha$ and see that $S_2(n) = \frac{n^3}{6} + ...n^2 + ...n + ...$ so $S_2(n) \in \Theta(n^3)$.

**2nd Solution**: Separate $O$ and $\Omega$.

> $S_2(n) \leq \sum_{i = 1}^n \sum_{j = 1}^n \sum_{k = 1}^n 1 = n^3$ so $S_2(n) \in O(n^3)$.   
> $S_2(n) \geq \sum_{i = 1}^{n / 3} \sum_{j = i}^n \sum_{k = i}^j 1 \geq \sum_{i = 1}^{n / 3} \sum_{j = 2n \ 3}^n \sum_{k = n / 3}^{2n / 3} 1 = (\frac{n}{3})^3 \in \Omega(n^3)$  
> Therefore $S_2(n) \in \Theta(n^3)$.

Example:

```python
# Insertion Sort
def Test3(A, n):
    for i in range(1, n):
        j = i
        while j > 0 and A[j] > A[j - 1]:
            A[j], A[j - 1] = A[j - 1], A[j]
            j -= 1
```

> Let $T_A(I)$ denote the running time of an algorithm $A$ on instance $I$.   
> **Worse-case**: $T_A(n) = \max \{T_A(I): Size(I) = n\}$.  
> **Average-case**: $T_A^{avg}(n) = \frac{1}{|\{I: Size(I) = n\}} \sum_{\{I: Size(I) = n\}} T_A(I)$.

It is important to try and not make **comparisons** between algorithms using $O$-notation.

- Worse-case run-time may happen rarely.
- $O$-notation is an upper bound, we should use $\Theta$-notation.

# *MergeSort* Example

**Input**: Array $A$ of $n$ integers.   

1. We split $A$ into two subarrays. $A_L$ consists of the first $\lceil\frac{n}{2}\rceil$ elements and $A_R$ consists of the last $\lfloor\frac{n}{2}\rfloor$ elements.
2. Recursively run *MergeSort* on $A_L$ and $A_R$.
3. After $A_L$ and $A_R$ are sorted, *merge* them together.

```python
def MergeSort(A, l = 0, r = n - 1):
    if r <= l:
        return
    else:
        m = (l + r) / 2
        MergeSort(A, l, m)
        MergeSort(A, m + 1, r)
        Merge(A, l, m, r)

def Merge(A, l, m, r):
    # We should only copy over A[l, r] to S.
    S = A
    i_L = l, i_R = m + 1
    for k in range(l, r + 1):
        if i_L > m:
            A[k] = S[i_R]
            i_R += 1
        elif i_R > r:
            A[k] = S[i_L]
            i_L += 1
        elif S[i_L] <= S[i_R]:
            A[k] = S[i_L]
            i_L += 1
        else:
            A[k] = S[i_R]
            i_R += 1
```

*Merge* takes time $\Theta(r - l + 1) = \Theta(n)$ time for merging $n$ elements.

## Analysis of *MergeSort*

Let $T(n)$ denote the time to run *MergeSort* on an array of length $n$.

1. Step 1 takes $\Theta(n)$.
2. Step 2 takes $T(\lceil\frac{n}{2}\rceil)$ + $T(\lfloor\frac{n}{2}\rfloor)$.
3. Step 3 takes $\Theta(n)$.

### **Sloppy** Recurrence

$$ T(n) = \begin{cases}
  2T(\frac{n}{2}) + cn, &\text{if } n > 1 \\
  c, &\text{if } n = 1
\end{cases}$$

> Exact and sloppy recurrences are **identical** when $n$ is a power of 2.  
> The recurrence can be easily solved by various methods when $n = 2^k$.

$T(n) = 2T(\frac{n}{2}) + cn$.  
For $n$ a power of 2, $n = 2^k$.  
$$\begin{aligned}
T(2^k) &= 2T(2^{k - 1}) + c2^k \\
&= 2(2T(2^{k - 2}) + c2^{k - 1}) + c2^k \\
&= 2^3 T(2^{k - 3}) + 3c2^k \\
&= 2^kT(2^{k - k}) + kc2^k \\
&= 2^k + c2^kk \\
&= n + cn\log(n)
\end{aligned}$$   
Therefore $T(n) \in \Theta(n\log(n))$, for $n = 2^k$.

# Priority Queue

> An ADT consisting of a collection of items (each having a *priority*)

- **Insert**: Insert an item with a given *priority*.
- **DeleteMax**: Remove the item with *highest priority*

```python
def PQ_Sort(A):
    pq = PriorityQueue()
    for k in range(n):
        pq.insert(k)
    for k in reversed(range(n)):
        pq.deleteMax()
```

The runtime will be determined by how efficient our **insert()** and **deleteMax()** functions are.

> $O(n + n \cdot insert + n \cdot deleteMax)$

## Inserting Into Dynamic Array
Let $T(n)$ be the total cost of insertion from length $n$ to $2n - 1$, $n = 2^k$. Then $T(n)$ is the cost of doubling from $n$ to $2n$ plus the cost of inserting $n$ items. $T(n) \in O(n)$.

## Binary Heaps

> A (binary) heap is a certain type of tree.

Any binary tree with $n$ nodes has height at least $\log(n + 1) - 1 \in \Omega(\log(n))$.
1. **Structural Property**: All levels of the heap are completely filled, except for the last level potentially. The entries in the last level are *left-justified*.
2. **Heap-order Property**: For any node $i$, the *key* of $i$'s parent is greater or equal to the *key* of $i$.

These are the properties for a **max-heap**. A **min-heap** is the same but with opposite order property.

**Lemma**: The height of a heap with $n$ nodes is $\Theta(\log(n))$.

> Given the height of the tree $h$.   
> $2^n \le n \le 2^{h + 1}$   
> $h \le \log(n) \le h + 1$  
> $\log(n) - 1 \le h \le \log(n)$

### Storing Heaps in Arrays

> Heaps should **not** be stored as binary trees!

Let $H$ be a heap of $n$ items and let $A$ be an array of size $n$. We can store the root in $A[0]$ and continue with elements *level-by-level* from top to bottom. In each level *left-to-right*.

- The **root** node is at $A[0]$.
- The **left child** of $A[i]$ is $A[2i + 1]$.
- The **right child** of $A[i]$ is $A[2i + 2]$.
- The **right child** of $A[i]$ is $A[\frac{i - 1}{2}]$.
- The **last** node is $A[n - 1]$.

> We should hide implementation using helper functions.

### Operations in Binary Heaps

#### Insertion in Heaps

- Place the new key in the first free leaf.
- The heap-order property might be violated so we may need to perform a *bubble-up*.
  - Keep swapping with the parent node until the heap-order property is satisfied.
  - $O(\text{height of heap}) = O(\log(n))$.

#### Delete in Heaps

- The maximum item of a heap is just the root node.
- We replace the root by the last leaf.
- Heap-order property might be violated so we may need to perform a *bubble-down*.
  - Keep swapping with the larger child until the heap-order property is satisfied.
  - $O(\text{height of heap}) = O(\log(n))$.

**HeapSort** is not used as often as **MergeSort** or **QuickSort** because it has poor data locality.

### Building Heaps by Bubble-Up

> Given an array, build a heap out of them.

```python
def heapify(A):
    n = len(A)
    for i in reversed(range(parent(last(n)) + 1)):
        fix_down(A, n, i)
```

> Careful analysis yields a worst-case complexity of $\Theta(n)$.

Whenever you reach node $i$, you know that the subtrees are both heaps because you have fixed them before.

Let $T(n)$ be the worse case runtime of heapify for an array of size $n$. It is related to the worse case number of swaps $S(n)$, so $T(n) \in \Theta(S(n))$.

$S(1) = 0$.  
$S(2) = 1$ because we *bubble-down* the root.   
$S(3) = 1$ because we *bubble-down* the root.  
$S(4) = 1 + 2 = 3$.   
$S(5) = 3$.

> If we add $n$ nodes to a heap of size $n$, ($n \to 2n$). The new nodes form a *layer* of leaves. Every single original node now has to perform 1 more swap because the length of the path has been increased by 1. The new nodes do not have to perform any swaps because they are all leaves.

$S(2n) = S(n) + n$

> For $n$ a power of 2, $n = 2^k$. 

$$\begin{aligned}
S(n) = S(2^k) &= S(2^{k - 1}) + 2^{k - 1} \\
&= S(2^{k - 2}) + 2^{k - 2} + 2^{k - 1} \\
&= \sum_{i = 0}^{k - 1} 2^i \\
&= 2^k - 1 \\
&= n - 1
\end{aligned}$$

Therefore $T(n) \in \Theta(n)$ for $n = 2^k$.

### Selection

> The $k$th-max problem asks to find the **$k$th largest item** in an array $A$ of $n$ numbers.

1. Scan the array and maintain the $k$ largest numbers seen so far in a **min-heap**. $\Theta(n\log(k))$.
2. Make a **max-heap** by calling $heapify(A)$. Call $deleteMax(A)$ $k$ times. $\Theta(n + k\log(n))$.

# Sorting and Randomized Algorithms

## QuickSelect

> Given an array of $A$ of $n$ numbers, $0 \le k < n$, find the element that be at position $k$ of the sorted array.

The best heap-based algorithm had a running time of $O(n + k\log n)$. For *median finding*, this is $O(n\log n)$ which is the same cost as our best sorting algorithms.

The **QuickSelect** algorithm can do this in linear time.

**QuickSelect** and the related **QuickSort** rely on two subroutines.

1. **chose_pivot(A)**: Choose an index $p$ in $A$.
2. **partition(A, p)**: Rearrange $A$ and return **pivot index** $i$ such that $A[i] = v$, $A[0, ..., i - 1] \le v$, $v \le A[i + 1, ..., n - 1]$.

Partitioning in-place can be done by using two pointers $i = 0$, $j = n - 1$, and moving them to the middle, swapping while they are on the wrong side.

```python
def partition(A, p):
    A[n - 1], A[p] = A[p], A[n - 1]

    i = 0, j = n - 2, v = A[n - 1]
    while True:
        while i < n and A[i] < v:
            i += 1
        while j >= 0 and A[j] > v:
            j -= 1
        if j >= i:
            break
        A[i], A[j] = A[j], A[i]

    A[i], A[n - 1] = A[n - 1], A[i]
```

### QuickSelect Algorithm

```python
def quick_select_1(A, k):
    p = choose_pivot(A)
    i = partition(A, p)
    if i == k:
        return A[i]
    elif i > k:
        return quick_select_1(A[0, ..., i - 1], k)
    elif i < k:
        return quick_select_1(A[i + 1, ..., n - 1], k - i - 1)
```

### Analysis of *quick_select_1*

**Worse Case Analysis**: Recursive calls could always have size of $n - 1$. The recurrence is given by.
$$T(n) = \begin{cases}
T(n - 1) + cn, &n \ge 2 \\
c, &n = 1
\end{cases}$$
For some constant $c > 0$. **Solution**: $T(n) = cn + c(n - 1) + ... + c(2) + c \in \Theta(n^2)$.

**Best Case Analysis**: First chosen pivot could be the $k$th element. We have no recurisve calls, so the total cost would be $\Theta(n)$.

### Sorting Permutations

> We want to take the average runtime of *quick_select_1* over all inputs.

How do we characterize all inputs of size $n$? We can make a **simplifying assumption** that all input numbers are **distinct**. *quick_select_1* does not depend on the actual value, only their *relative order*. Therefore, we can characterize our input based on **sorting permutation**: the permutation that would put the input in order.

If we assume that all $n!$ permutations are equally likely, the average cost is the sum of the total cost for all permutations, divided by $n!$.

Define $T(n)$ to be the average case for selecting from a size $n$ array. Fix one $0 \le i \le n - 1$. There are $(n - 1)!$ permutations for which the pivot value $v$ is the $i$th smallest item.

$$\begin{aligned}
T(n, k) &= \frac{1}{n!}\sum_{\text{permutations A of } \{1, ..., n\}} T(A, k) \\
T(n) &= \max_{k} T(n, k) \\
&\le c\cdot n + \frac{1}{n}\sum_{i = 0}^{n - 1}\max\{T(i), T(n - i + 1)\} \\
\end{aligned}$$

**Theorem**: $T(n) \in \Theta(n)$.

**Sloppy Proof**: We want $T(n) \le \lambda n$. Suppose $T(i) \le \lambda i$, $i < n$.
$$\begin{aligned}
T(n) &\le cn + \frac{1}{n}\sum_{i = 0}^{n - 1}\max \{\lambda i, \lambda(n - i - 1)\} \\
&\le cn + \frac{\lambda}{n}\sum_{i = 0}^{n - 1} \max \{i, n - i - 1\}
\end{aligned}$$
We can see visually (taking max of $i$ and $n - i - 1$) that $\sum \approx \frac{3}{4}n^2$. Therefore we have $T(n) \le cn + \frac{\lambda}{n}\frac{3}{4}n^2 = (c + \frac{3}{4}\lambda) n$. We want $T(n) \le \lambda n$, let us take $\lambda$ such that $c + \frac{3}{4}\lambda = \lambda \Leftrightarrow \lambda = 4c$.

> **Clean Proof**: We can rewrite the proof that we had before using $T(n) \le 4cn$ by induction. We also have to prove that $\sum_{i = 0}^{n - 1} \max\{1, n - i - 1\} \le \frac{3}{4}n^2$ which can be done by a case discussion when $n$ is even or odd.

## Randomized Algorithms

> No more bad instances, just unlucky numbers.

### Expected Running Time

> Define $T(I, R)$ be the running time of the randomized algorithm for instance $I$ and sequence of random numbers $R$.

$T^{(exp)}(I) = E[T(I, R)] = \sum_{R}T(I, R)P(R)$.

$T^{(exp)}_{worst}(n) = \max_{\{I: size(I) = n\}}T^{(exp)}(I)$.

$T^{(exp)}_{avg}(n) = \frac{1}{|\{I: size(I) = n\}|}\sum_{\{I : size(I) = n\}}T^{(exp)}(I)$.

## Average-Case Analysis of QuickSort

$$\begin{cases}
T(n) = cn + \frac{1}{n}\sum_{i = 0}^{n - 1}(T(i) + T(n - (i + 1))) \\
T(0) = T(1) = 0
\end{cases}$$

We prove that $T(n) \le 2cn\log(n)$, $n \ge 1$.

Define $S(n) = \sum_{i = 1}^{n - 1}i\log(i)$. We claim that $S(n) \le \frac{1}{2}n^2\log(n) - \frac{1}{4}n^2$. We skip the proof because it is too difficult.

Assume that for $i = 1, ..., n - 1$, $T(i) \le 2ci\log(i)$.

$$\begin{aligned}
T(n) &= cn + \frac{1}{n}\sum_{i = 0}^{n - 1}T(i) + \frac{1}{n}\sum_{i = 0}^{n - 1}T(n - (i + 1)) \\
&= cn + \frac{2}{n}\sum_{i = 0}^{n - 1}T(i) \\
&= cn + \frac{2}{n} \sum_{i = 1}^{n - 1}T(i) \\
&\le cn + \frac{2}{n} \sum_{i = 1}^{n - 1}2ci\log(i) \\
&\le cn + \frac{4c}{n} \sum_{i = 1}^{n - 1}i\log(i) \\
&\le cn + \frac{4c}{n} S(n) \\
&\le cn + \frac{4c}{n} (\frac{1}{2}n^2\log(n) - \frac{1}{4}n^2) \\
&\le cn + (2cn\log(n) - cn) \\
&\le 2cn\log(n)
\end{aligned}$$

The auxiliary space is $\Theta(n)$ worst-case. We can reduce this to $\Theta(\log(n))$ by recursing in smaller sub-array first and replacing the other recursion by a while loop.

```python
def quicksort_iterative(A, l, r):
    while l < r:
        i = partition(A, l, r)
        if i - l < r - i:
            quicksort_iterative(A, l, i - 1)
            l = i + 1
        else:
            quicksort_iterative(A, i + 1, r)
            r = i - 1
```

One should stop recursing when $n \le 10$. Then one run of InsertionSort at the end sorts everything in $O(n)$ because all items are within 10 units of their required position.

Arrays with many duplicates can be dealt with more efficiently by creating a third partition for equals.

## Comparison Model

In the **comparison model**, data can only be accessed in 2 ways.

1. Comparing two elements.
2. Moving elements around (e.g. copying, swapping).

We can represent comparison models as **decision trees**.

**Theorem**: Any correct **comparison-based** sorting algorithm requires at least $\Omega(n\log n)$ comparisons.

In a decision tree that sorts n integers, there are at least n! leaves.

We claim that in a binary tree of height $h$, with $L$ leaves, $L \le 2^{h + 1}$. We see that $L \le \#\ nodes$ and that is less than or equal to the number of nodes in a complete heap of height $h$.

$$\begin{aligned}
n! &\le 2^{n + 1} \\
\log(n!) &\le n + 1 \\
\end{aligned}$$
We know that $\log(n!) \in \Theta(n\log n)$ so $h \in \Omega({n \log n})$.

## Non-Comparison-Based Sorting

- Assume all keys are numbers are in base (radix).
- Assume all keys have the same number $m$ of digits.
  - Can achieve by padding with leading 0s.

### Bucket Sort

- Sort numbers by a single digit.
- Create a "bucket" for each possible digit.

```python
def bucket_sort(A, d):
    Initialize an array B[0 .. R - 1] of empty lists.
    for i in range(len(A)):
        B[dth digit of A[i]].append(A[i])
    i = 0
    for j in range(R):
        while len(B[j]) > 0:
            A[i] = B[j]
            i += 1
```

This is **stable**, equal items stay in order. The runtime is $O(n + R)$, auxiliary space $\Theta(n + R)$.

### Count Sort

- We can save auxiliary memory by counting how many elements there are in each B.

```python
def key_indexed_countsort(A, d):
    count = [0] * R
    for i in range(len(A)):
        count[dth digit of A[i]] += 1
    
    idx = [0] * R
    for i in range(1, R):
        id[i] = id[i - 1] + count[i - 1]

    aux = [0] * len(A)
    for i in range(len(A)):
        aux[id[dth digit of A[i]]] = A[i]
        id[dth digit of A[i]] += 1

    A = aux
```

```python
def MSD_radixsort(A, l, r, d):
    if l < r:
        key_index_countsort(A[l .. r], d)
        if d < m:
            for i in range(R):
                l_i, r_i = boundaries of ith bin
                MSD_radixsort(A, l_i, r_i, d + 1)
```

Drawback is that we have many recursive calls.

```python
def LSD_radixsort(A):
    for d in reversed(range(0, m)):
        key_index_countsort(A, d)
```

Time complexity is $\Theta(m(n + R))$. Auxiliary memory is $O(n + R)$. This can be $o(n\log(n))$ for special cases.

# Dictionaries and Balanced Search Trees

## Dictionary ADT

- A collection of items, each of which contains a **key**, and **data** (*key-value pair*). Keys can be compared and are typically unique.

1. search(k)
2. insert(k, v)
3. delete(k)

Common assumptions are that the dictionary has $n$ KVPs, each of which uses constant space. We also assume that keys can be compare in constant time.

## Binary Search Trees
**$search(k)$**: Start at the root, compare $k$ to the current node. Stop if found or node is external, else recurse at child.

**$insert(k, v)$**: Search for $k$, then insert $(k, v)$ as new node.

**$delete(k)$**: Search for node $x$ that contains the key $k$.

1. If $x$ is a leaf, then delete it.
2. If $x$ has one child, then move that child up.
3. If $x$ has two children, swap at $x$ with the key at **successor** or **predecessor** node and then delete that node.

> **Remark**: The successor of node $x$ does not have to be a leaf but it cannot have a left child.

### Height of a BST
All of the functions above have cost $\Theta(h)$, where $h$ is the height of the tree.

If $n$ items are inserted, the worst case if $\Theta(n)$. The best case is $\Theta(\log(n))$ because any binary tree with $n$ nodes has height $\ge \log(n + 1) - 1$. It can be shown that the average case is $\Theta(\log(n))$.

## AVL Trees

> Introduced by Adel’son-Vel’ski˘ı and Landis in 1962.

AVL Trees are BST with an additional **height-balance** property that the height of the left subtree L and right subtree R differ by at most 1. The height of an empty tree is defined to be -1.

At each non-empty node, we require $height(R) - height(L) \in \{-1, 0, 1\}$.

> - -1, **left-heavy**
> - 0, **balanced**
> - 1, **right-heavy**

We need to store the height of the subtree rooted at each node. It can be shown that it is sufficient to store $height(R) - height(L)$ at each node which uses fewer bits but more complicated code (especially for deleting).

### Height of an AVL Tree
**Theorem**: an AVL Tree with $n$ nodes has $\Theta(\log(n))$ height.

**Proof**: We prove the upper and lower bounds separately.

1. The height $h$ of an AVL with $n$ keys is $\Omega(\log(n))$ because $h \ge \log(n + 1) - 1$.
2. The height $h$ of an AVL with $n$ keys is $O(\log(n))$.

    > Define $N_h$ as the minimum number of keys in an AVL of height $h$. Our plan is to prove $N_h \in \Omega(c^h)$ for some constant $c$. Then we would have $\log(n) \in \Omega(h)$.     
    >
    > We see that $N_h$ has a relation to Fibonacci numbers, $N_h = F_{h + 3} - 1$. This is because to build $N_h$, we can take the tree for $N_{h - 1}$ as a left child and the tree for $N_{h - 2}$ as a right child. We can prove $N_{h} = N_{h - 1} + N_{h - 2} + 1$ with generating series. We show the inductive proof that $N_h = F_{h + 3} - 1$.
    >
    > $$\begin{aligned} N_{h + 2} &= N_{h + 1} + N_{h} + 1 \\ &= (F_{h + 4} - 1) + (F_{h + 3} - 1) + 1 \\ &= F_{h + 4} + F_{h + 3} - 1 \\ &= F_{h + 5} - 1\end{aligned}$$
    >
    > We claim $F_h \in \Theta(\phi^h)$, $\phi = \frac{1 + \sqrt 5}{2}$ (*239*). Then $N_h \in \Theta(\phi^h)$ and $\log(N_h) \in \Theta(h)$ (*a1*). For any AVL with height $h$, $n$ nodes, $\log(n) \ge \log(N_h)$ so $\log(n) \in \Omega(h)$. Therefore $h \in O(\log(n))$.

### AVL Insertion

1. Insert $(k, v)$ into $T$ with usual BST insertion.
2. We assume that this returns the new leaf $z$ where the key was sorted.
3. Move up the tree from $z$ and update heights.
    - We assume that we have parent links. This can be avoided if insert returns the full path to $z$.
4. If the height difference becomes $\pm 2$ at node $z$, then $z$ is **unbalanced**. We must re-structure the tree to rebalance.

```python
def AVL_insert(r, k, v):
    z = BST_insert(r, k, v);
    z.height = 0
    while z is not None:
        set_height_from_children(z)
        if abs(z.left.height - z.right.height) == 2:
            AVL_fix(z)
            break
        else:
            z = z.parent

def set_height_from_children(u):
    u.height = 1 + max(u.left.height + u.right.height)
```

**Fixing a slightly unbalanced AVL tree**. This is applied at a node $z$ that has balance $\pm 2$, but the subtrees at $z$ are AVL.

```python
def AVL_fix(z):
    # Find child and grand-child that go deepest.
    if z.right.height > z.left.height:
        y = z.right
        if y.left.height > y.right.height:
            x = y.left
        else:
            x = y.right
    else:
        y = z.left
        if y.right.height > y.left.height:
            x = y.right
        else:
            x = y.left
    # Apply appropriate rotation to restructure at x, y, z
    ...
```

How do we "fix" an unbalanced AVL? Notice that there are many different BSTs with the same keys. Our **goal** is to change the *structure* among the three nodes without changing the *order* such that the subtree becomes balanced.

![](https://www.dropbox.com/s/tiq9xylng9hq53n/Screenshot%202018-06-05%2010.28.55.png?raw=1)

#### Right Rotation

![](https://www.dropbox.com/s/1uxwmuxn2aasmp0/Screenshot%202018-06-05%2010.30.23.png?raw=1)

```python
def rotate_right(z):
    y = z.left
    make y.right the new left child of z
    make y the new root of the subtree
    make z the new right child of yb
    set_height_from_children(z)
    set_height_from_children(y)
```

Recall that we need to update the parent links as well!

- **Left Rotation** is symmetrical.

#### Double Right Rotation

1. Left rotation at $y$.
2. Right rotation at $z$.

**Rule**: The median of $x, y, z$ becomes the new root.

![](https://www.dropbox.com/s/czwsn0mtnmxc9l5/Screenshot%202018-06-05%2010.50.43.png?raw=1)

### AVL Deletion

- Remove the key $k$ with BST_delete
- We assume that BST_delete returns the place where the *structural* change happened (the parent $z$ of the node that got deleted).
- Go back up to the root, update heights, and rotate if needed.

# Other Dictionary Implementations

Arrays are a simple and popular implementation. Can we do something to make the search more effective in practice?

**Yes**, if the items are not equally likely to be accessed and we have a probability distribution of the items.
    - Intuition is that frequently accessed items should be in the front.

## Optimal Static Ordering

- Expected access cost in $L$ is $E(L) = \sum_{i = 1}^n P(x_i)T(x_i) = \sum_{i=1}^n P(x_i)*i$
    - $P(x_i)$ is the access probability for $x_i$.
    - $T(x_i)$ is the position of $x_i$ in $L$.

Is this better than our linear time solution?

Consider $L=[x_1, ..., x_n]$, $P(x_1) = \frac{1}{2}$, $P(x_2) = \frac{1}{4}$, ..., $P(x_{n-1}) = P(x_n) = \frac{1}{2^{2n-1}}$. $E(L) = 1\cdot\frac{1}{2} + 2\cdot\frac{1}{4} + ... + (n-1)\cdot \frac{1}{2^{n-1}} + n\cdot\frac{1}{2^{n-1}}$. $\lim_{n \to \infty}E(L) = 2$.

**Claim**: Over all possible static orderings, the one that sorts items by non-increasing access-probability minimizes the expected cost. We can prove by contradiction.

## Dynamic Ordering

- What if we do not know the access probabilities ahead of time?
- **Temporal locality**: recently accessed items are likely to be used soon again.
- **Move-To-Front** (*MTF*): Upon a successful search, move the accessed item to the front of the list.
- **Transpose**: Upon a successful search, swap the accessed item with the item immediately preceding it.

### Performance of Dynamic Ordering

- MTF works well in practice.
- We can show that MTF is "2-competitive". $C_{OPT} \le C_{MTF} \le 2C_{OPT}$.

## Skip Lists

> **Randomized** data structure for dictionary ADT.

- A heirarchy $S$ of ordered linked lists (*levels*) $S_0, S_1, ..., S_h$.
    - Each list $S_i$ contains the special keys $-\infty$ and $+\infty$.
    - List $S_0$ contains the KVPs of $S$ in non-decreasing order.
    - Each list is a subsequence of the previous one, $S_0 \supseteq S_1 \supseteq ... \supseteq S_h$.
    - List $S_h$ only contains the two special keys.
- Each node $p$ has references $below(p)$ and $after(p)$.

### Search in Skip Lists

```python
def skip_search(l, k):
    P = [p]
    while below(p) is not None:
        p = below(p)
        while key(after(p)) < k:
            p = after(p)
        P.append(p) 
    return P
```

### Insert in Skip Lists

- In an **ideal** skiplist, we have $2n$ keys and $\log(n)$ height. The search costs $\Theta(\log(n))$.

- Randomly compute the height of a new item $k$, flip a coin until you get tails.
- Let $i$ be the number of heads, this will be the height of $k$.
    - $P(height \ge l) = (\frac{1}{2})^l$.
- Increase the height of the skip list if needed, to have $h > i$ levels.
- Search for $k$  with skip_search to get stack $P$.
    - The top $i$ items of $p$ are the predecessors $p_0, p_1, ..., p_i$ where $k$ should be in each list $S_0, S_1, ... S_i$.
    - Insert $(k, v)$ after $p_0$ in $S_0$, and $k$ after $p_j$ in $S_j$ for $1 \le j \le i$.

![](https://www.dropbox.com/s/bsniny3fq8a39fk/Screenshot%202018-06-07%2010.52.03.png?raw=1)

### Delete in Skip Lists

- Search for $k$ with skip_search to get stack $P$.
- $P$ contains all predecessors $p_0, p_1, ..., p_h$ of $k$ in lists $S_0, ..., S_h$.
- For each $0 \le j \le h$, if $key(after(p_j)) = k$, then remove $after(p_j)$ from list $S_j$.
- Remove all but one of the lists $S_i$ that contain only the two special keys.

![](https://www.dropbox.com/s/rqrx1by20ge232p/Screenshot%202018-06-07%2010.56.23.png?raw=1)

### Expected Performance

- $O(n)$ space.
- $O(\log(n))$ height. A skip list with $n$ items has height at most $3\log(n)$ with probability at least $1 - \frac{1}{n^2}$.
- $O(\log(n))$ search, insert, and delete.