CS 240
=

# Algorithm Design

**Problem**: Given a problem instance, carry out a particular computational task.

**Problem Instance**: Input.

**Problem Solution**: Output.

**Size of a problem instance**: Size(I) is a positive integer that measures the size of instance I.

**Algorithm**: Step-by-step process for carrying out a series of computations, given an abitrary problem instance I.

For a problem $\Pi$, we can have several algorithms. For an algorithm $A$ solving $\Pi$, we can have several programs (implementations).

- Designing an algorithm $A$ that solves $\Pi$ -> **Algorithm Design**.
- Assessing correctness and efficiency of $A$ -> **Algorithm Analysis**.

# Analysis of Algorithms I

We may be interested in the amount of **time** or **memory**. Experimental shortcomings have shortcomings because of implementations, hardware, and testing against all inputs.

Instead we write code for Random Access Machines (RAM)

- Set of memory cells, each of which stores one item of data.
- Access to memory in O(1).
- Primitive operations are O(1).
- Runtime of a program is the number of memory accesses + the number of primitive operations.

The efficiency is measured in terms of it's **growth rate** (this is called the **complexity** of the algorithm).

```python
for i in range(n):
  for j in range(n):
    c[i][j] = 0
    for k in range(n):
      c[i][j] += a[i][k] * b[k][j]
```
*About* $n^3$ operations.

# Asymptotic Notation
O-Notation $f(n) \in O(g(n))$ if there exists constants $c > 0, n_0 > 0$ such that if $|f(n)| \le c|g(n)|$ $\forall n \ge n_0$.

Example: $f(n) = 75n + 500$ and $g(n) = 5n^2$.

> $f(n), g(n) \ge 0$ $\forall n \ge 1$    
> For $n \ge 20$    
> $n^2 \ge 20n$    
> $5n^2 \ge 100n$    
> $25n \ge 25 * 20 = 500$    
> $5n^2 \ge 75n + 500$    
> $g(n) \ge f(n)$    
> Taking $n_0 = 20, c = 1$, this proves that $f(n) \in O(g(n))$.    

We want a **tight** asymptotic bound.

$\Omega$-notation: $f(n) \in \omega(g(n))$ if $\exists c > 0, n_0 > 0$ such that $c|g(n)| \le |f(n)| \forall n \ge n_0$.

$\Theta$-notation: $\exists c1, c2 \ge 0, n_0 > 0$ such that $c1|g(n)| \le |f(n)| \le c2|g(n)| \forall n \ge n_0$.

How to express $f(n)$ is asymptotically strictly smaller than $g(n)$?

o-notation: $\forall c > 0, \exists n_0 > 0$ such that $|f(n)| < c|g(n)| \forall n \ge n_0$.

$\omega$-notation: $\forall c > 0, \exists n_0 > 0$ such that $0 \le c|g(n)| < f(n) \forall n \ge n_0$.

Example: $f(n) = 2000n^2 + 5000n$ and $g(n) = n^3$. Prove $f(n) \in o(g(n))$.

> Given $c > 0$, for $n > 0$, $f(n) = 2000n^2 + 5000n \le 7000n^2$.    
> $7000n^2 < cn^3 \Leftrightarrow 7000 < cn \Leftrightarrow n > \frac{7000}{c}$    
> So if we take $n_0 = \frac{7000}{c} + 1$, we have $f(n) < g(n) \forall n \ge n_0$. This proves that $f(n) \in o(g(n))$.    

## Asymptotic Identities

> $f(n) \in \Theta(g(n)) \Leftrightarrow g(n) \in \Theta(f(n))$    
> $f(n) \in O(g(n)) \Leftrightarrow g(n) \in \Omega(f(n))$    
> $f(n) \in o(g(n)) \Rightarrow f(n) \in O(g(n)), \nRightarrow f(n) \in \Omega(g(n))$    
> $f(n) \in \omega(g(n)) \Rightarrow f(n) \in \Omega(g(n)), \Rightarrow f(n) \notin O(g(n))$    
> *Identitity*: $f(n) \in \Theta(f(n))$    
> *Maximum*: $f(n) > 0, g(n) > 0 \forall n \ge n_0 \Rightarrow O(f(n) + g(n)) = O(\max\{f(n), g(n)\})$. Similar for $\Omega$.    
> *Transitivity*: $f(n) \in O(g(n)), g(n) \in O(h(n)) \Rightarrow f(n) \in O(h(n))$. Similar for $\Omega$.    

## Limit Test

Suppose $L = \lim_{n \to \infty} \frac{f(n)}{g(n)}$.    

$$f(n) \in \begin{cases}
o(g(n)), &L = 0 \\
\Theta(g(n)), &0 < L < \infty \\
\omega(g(n)), &L = \infty
\end{cases}$$

Example: $f(n) = \log(n) = \frac{\ln(n)}{\ln(2)}$, $g(n) = n$.
> $\lim_{n \to \infty} \frac{1}{n\ln(2)} = 0$ so $f(n) \in o(g(n))$ by the limit test.

# Analysis of Algorithms II

Example:

```python
def Test1(n):
  sum = 0
  for i in range(1, n + 1):
    for j in range(i, n + 1):
      sum += (i - j) * (i - j)
  return sum
```

> Let $T_1(n)$ be the runtime of $Test1(n)$.  
> $T_1(n) \in \Theta(S_1(n))$ where $S_1(n)$ is the number of times we enter the body of the inner loop on line 4.  
>
> $S_1(n) = \sum_{i = 1}^n \sum_{j = i}^n i$.

**1st solution**: Brute Force.
> $S_1(n) = \sum_{i = 1}^n (n - i + 1) = \sum_{i = 1}^n n - \sum_{i = 1}^n + \sum_{i = 1}^n 1 = n^2 - \frac{n(n+1)}{2} + n = \frac{n^2}{2} + \frac{n}{2}$.   
> Therefore we have $S_1(n) \in \Theta(n^2)$.

**2nd solution**: Separate $O$ and $\Omega$.
> $S_1(n) \leq \sum_{i = 1}^n \sum_{j = 1}^n 1 = n^2$ so $S_1(n) \in O(n^2)$.   
> $S_1(n) \geq \sum_{i = 1}^{n / 2} \sum_{j = 1}^n i \geq \sum_{i = 1}^{n / 2} \sum_{j = n / 2}^n 1 = \frac{n^2}{4}$ so $S_1(n) \in \Omega(n^2)$.  
> Therefore $S_1(n) \in \Theta(n^2)$.

Example:
```python
def Test2(A, n):
  max = 0
  for i in range(1, n + 1):
    for j in range(i, n + 1):
      sum = 0
      for k in range(i, j + 1):
        sum += A[k]
  return max
```
> Let $T_2(n)$ be the runtime of $Test2(n)$.  
> Then $T_2(n) \in \Theta(S_2(n))$, where $S_2(n)$ is the number of times we enter the body of the inner loop on line 6.
>
> $S_2(n) = \sum_{i = 1}^n \sum_{j = i}^n \sum_{k = i}^j 1$.

**1st Solution**: Brute Force.
> Input to Wolfram $\alpha$ and see that $S_2(n) = \frac{n^3}{6} + ...n^2 + ...n + ...$ so $S_2(n) \in \Theta(n^3)$.

**2nd Solution**: Separate $O$ and $\Omega$.
> $S_2(n) \leq \sum_{i = 1}^n \sum_{j = 1}^n \sum_{k = 1}^n 1 = n^3$ so $S_2(n) \in O(n^3)$.   
> $S_2(n) \geq \sum_{i = 1}^{n / 3} \sum_{j = i}^n \sum_{k = i}^j 1 \geq \sum_{i = 1}^{n / 3} \sum_{j = 2n \ 3}^n \sum_{k = n / 3}^{2n / 3} 1 = (\frac{n}{3})^3 \in \Omega(n^3)$  
> Therefore $S_2(n) \in \Theta(n^3)$.

Example:
```python
# Insertion Sort
def Test3(A, n):
  for i in range(1, n):
    j = i
    while j > 0 and A[j] > A[j - 1]:
      A[j], A[j - 1] = A[j - 1], A[j]
      j -= 1
```
> Let $T_A(I)$ denote the running time of an algorithm $A$ on instance $I$.   
> **Worse-case**: $T_A(n) = \max \{T_A(I): Size(I) = n\}$.  
> **Average-case**: $T_A^{avg}(n) = \frac{1}{|\{I: Size(I) = n\}} \sum_{\{I: Size(I) = n\}} T_A(I)$.

It is important to try and not make **comparisons** between algorithms using $O$-notation.
- Worse-case run-time may happen rarely.
- $O$-notation is an upper bound, we should use $\Theta$-notation.

# *MergeSort* Example
**Input**: Array $A$ of $n$ integers.   
1. We split $A$ into two subarrays. $A_L$ consists of the first $\lceil\frac{n}{2}\rceil$ elements and $A_R$ consists of the last $\lfloor\frac{n}{2}\rfloor$ elements.
2. Recursively run *MergeSort* on $A_L$ and $A_R$.
3. After $A_L$ and $A_R$ are sorted, *merge* them together.

```python
def MergeSort(A, l = 0, r = n - 1):
  if r <= l:
    return
  else:
    m = (l + r) / 2
    MergeSort(A, l, m)
    MergeSort(A, m + 1, r)
    Merge(A, l, m, r)

def Merge(A, l, m, r):
  # We should only copy over A[l, r] to S.
  S = A
  i_L = l, i_R = m + 1
  for k in range(l, r + 1):
    if i_L > m:
      A[k] = S[i_R]
      i_R += 1
    elif i_R > r:
      A[k] = S[i_L]
      i_L += 1
    elif S[i_L] <= S[i_R]:
      A[k] = S[i_L]
      i_L += 1
    else:
      A[k] = S[i_R]
      i_R += 1
```
*Merge* takes time $\Theta(r - l + 1) = \Theta(n)$ time for merging $n$ elements.

## Analysis of *MergeSort*

Let $T(n)$ denote the time to run *MergeSort* on an array of length $n$.
1. Step 1 takes $\Theta(n)$.
2. Step 2 takes $T(\lceil\frac{n}{2}\rceil)$ + $T(\lfloor\frac{n}{2}\rfloor)$.
3. Step 3 takes $\Theta(n)$.

### **Sloppy** Recurrence
$$ T(n) = \begin{cases}
  2T(\frac{n}{2}) + cn, &\text{if } n > 1 \\
  c, &\text{if } n = 1
\end{cases}$$
> Exact and sloppy recurrences are **identical** when $n$ is a power of 2.  
> The recurrence can be easily solved by various methods when $n = 2^k$.

$T(n) = 2T(\frac{n}{2}) + cn$.  
For $n$ a power of 2, $n = 2^k$.  
$$\begin{aligned}
T(2^k) &= 2T(2^{k - 1}) + c2^k \\
&= 2(2T(2^{k - 2}) + c2^{k - 1}) + c2^k \\
&= 2^3 T(2^{k - 3}) + 3c2^k \\
&= 2^kT(2^{k - k}) + kc2^k \\
&= 2^k + c2^kk \\
&= n + cn\log(n)
\end{aligned}$$   
Therefore $T(n) \in \Theta(n\log(n))$, for $n = 2^k$.

# Priority Queue
> An ADT consisting of a collection of items (each having a *priority*)
- **Insert**: Insert an item with a given *priority*.
- **DeleteMax**: Remove the item with *highest priority*

```python
def PQ_Sort(A):
  pq = PriorityQueue()
  for k in range(n):
    pq.insert(k)
  for k in reversed(range(n)):
    pq.deleteMax()
```
The runtime will be determined by how efficient our **insert()** and **deleteMax()** functions are.
> $O(n + n \cdot insert + n \cdot deleteMax)$

## Inserting Into Dynamic Array
Let $T(n)$ be the total cost of insertion from length $n$ to $2n - 1$, $n = 2^k$. Then $T(n)$ is the cost of doubling from $n$ to $2n$ plus the cost of inserting $n$ items. $T(n) \in O(n)$.

## Binary Heaps
> A (binary) heap is a certain type of tree.
Any binary tree with $n$ nodes has height at least $\log(n + 1) - 1 \in \Omega(\log(n))$.
1. **Structural Property**: All levels of the heap are completely filled, except for the last level potentially. The entries in the last level are *left-justified*.
2. **Heap-order Property**: For any node $i$, the *key* of $i$'s parent is greater or equal to the *key* of $i$.

These are the properties for a **max-heap**. A **min-heap** is the same but with opposite order property.

**Lemma**: The height of a heap with $n$ nodes is $\Theta(\log(n))$.
> Given the height of the tree $h$.   
> $2^n \le n \le 2^{h + 1}$   
> $h \le \log(n) \le h + 1$  
> $\log(n) - 1 \le h \le \log(n)$