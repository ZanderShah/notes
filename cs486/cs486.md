CS 486
=

## Rational Agent Paradigm

> An entity that perceives and acts.

- Function from percepts to actions.
- Performance measures.
    - Goal achievement, resource consumption.
- **Caveat**: Computational limitations and environmental constraints means we do not have perfect rationality.

### Task Environments

To design a rational agent the task environment must be defined.

- Performance measures.
- Environment.
- Actuators.
- Sensors.

#### Properties of Task Environment

- **Fully Observable** vs. **Partially Observable**.
- **Deterministic** vs. **Stochastic**.
    - Is the next state completely determined by the current state and action executed?
- **Episodic** vs. **Dynamic**.
- **Discrete** vs. **Continuous**.
- **Static** vs. **Dynamic**.
- **Single Agent** vs. **Multiagent**.

# Search

> **Search problem** consists of a **state space**, a **successor function**, a **start space**, and a **goal test**.

- **Solution** is a sequence of actions (plan) from the start state to some goal state.

**Example**: Sliding Tiles Problem.

> - **State**: Board configuration.
> - **Start**: Any state.
> - **Actions**: Slide the blank tile into an adjacent space.
> - **Goal**: Does it match picture?

**Example**: N Queens Problem.

> - **State**: $0$ to $N$ queens.
> - **Start**: $0$ queens.
> - **Actions**: Add a queen to an empty space.
> - **Goal**: $N$ queens none attacking.

Alternate representation which is more complicated but has a smaller search space.

> - **State**: $0$ to $N$ queens, first $n$ columns not attacking each other.
> - **Start**: $0$ queens.
> - **Actions**: Add a queen to the first empty column none attacking.
> - **Goal**: $N$ queens. And babu is cutie

## State Space

- The **world space** includes every last detial in the environment.
- A **search space** keeps only the details needed for planning (abstraction).

## Representing State

- **State space graph**.
    - Vertices correspond to states with one vertex for each space.
    - Edges correspond to successors.
    - Goal test is a set of goal nodes.
- We search for a solution by building a **search tree** and traversing it to find a goal state.

### Search Tree

- State state is the root of the tree.
- Children are the successors.
- Plan is a path in the tree. A solution is a path from the root to a goal node.

> For most problems we do not actually generate the entire tree.

- We expand a node by applying all legal actions on it and adding the new states to the tree.

## Generic Search Algorithm

- Initialize with initial state of the problem.
- **Repeat**.
    - If no candidate nodes, **faliure**.
    - Choose leaf node for expansion according to **search strategy**.
    - If node contains goal state, return **solution**.
    - Otherwise, expand the node. Add resulting nodes to the tree.
- Nodes can be classified as **start** node, **explored** nodes, **frontier**, **unexplored** nodes.

### Key Properties

- **Completeness**: Is the algorithm guaranteed to find a solution if one exists?
- **Optimality**: Does the algorithm find the optimal solution?
- **Time complexity**.
- **Space complexity**: Size of the fringe.
- $b$: Branching factor.
- $m$: Maximum depth.
- $d$: Depth of the nearest goal node.

**Example**: DFS.

> - **Complete**: No. Infinitely stuck in a loop. If $m$ is finite then it is.
> - **Optimal**: No. Finds the first goal, not necessarily the optimal.
> - **Time complexity**: Whole tree, $O(b^m)$.
> - **Space complexity**: Fringe and related path information. $O(m \cdot b)$.

**Example**: BFS.

> - **Complete**: Yes.
> - **Optimal**: Depends on whether the shallowest goal node is the one with the least cost.
> - **Time complexity**: Whole tree, $O(b^{d + 1})$.
> - **Space complexity**: $O(b^d)$.

### Iterative Deepened Search

> Combine search methods to take advantage of DFS space complexity and BFS completeness and shallow solution advantage?

- **Complete**: Yes.
- **Optimal**: Depends on whether the shallowest goal node is the one with the least cost.
- **Time complexity**: Whole tree, $O(b^d)$.
- **Space complexity**: $O(m \cdot b)$.

## Cost-Sensitive Search

### Uniform Cost Search

- **Strategy**: Expand cheapest node first.
- **Implementation**: Priority queue.
- **Complete**: Yes.
- **Optimal**: Yes if costs are all greater or less some $\epsilon$.
- **Time Complexity**: $O(b^{1 + \frac{C^*}{\epsilon}})$, where $C^*$ is the optimal cost.
- **Space Complexity**: Same as BFS.

# Informed Search

Uninformed search expands nodes on the distance from the start node. Why not try to expand on the distance to the goal?

## Heuristics

> A function that **estimates** the cost of reaching a goal from a given state.

- If $h(n_1) < h(n_2)$ we guess that it is cheaper to reach the goal from $n_1$ than $n_2$.
- We require $h(n, goal) = 0$.

**Example**: Best First Search.

> **Search strategy**: Expand the most promising node according to the heuristic.
> - Not complete (infinite expansion). Time complexity, space complexity $O(b^m)$.

- **$A^\star$ Search**: Expand according to the cost of path and heuristic. $f(n) = g(n) + h(n)$. *Note: Goal test must be done while expanding the node, not when it is first discovered.* Time complexity is $O(b^{\delta d})$, where $\delta$ is the relative error of the heuristic. We are required to store all expanded nodes in memory.
- **Admissible Heuristic**: $0 \le h(n) \le h^\star(n)$, where $h^\star(n)$ is the true cost.
- Heuristic is **consistent** if $h(a) \le cost(a, b) + h(b)$. Required if we have a graph (multiple paths to a goal).

> Let $G$ be the optimal goal. Let $G^\prime$ be a sub-optimal goal. So $f(G) < f(G^\prime)$. Assume by way of contradiction that $G^\prime$ is selected over some $n$. So $f(G^\prime = g(G^\prime) > g(G) = cost(s, n) + cost(n, G) \ge g(n) + h(n)$. This is a contradiction since $n$ would have been selected first.

- **Dominated Heuristic**: $h_1(n)$ dominates $h_2(n)$ if for all $n$, $h_1(n) \ge h_2(n)$ and there exists one strict inequality.
- **Theorem**: If $h_1(n)$ dominates $h_2(n)$, then $A^\star$ will never expand more nodes.

> Let $c^\star$ be the cost of the optimal solution. $A^\star$ expands all nodes such that $f(n) < c^\star$, so $h(n) \le c^\star - g(n)$. $g(n)$ is constant across all heuristics, so since $h_2(n) \le h_1(n)$, a node expanded in $h_1$ must also have been expanded in $h_2$.

# Constraint Satisfaction

> Special subset of search problems.

- **States** are defined by **variables** $X_i$ with values from **domains** $D_i$.
- **Goal test** is a **set of constraints** specifying allowable combinations of values for subsets of variables.

## Types of CPSs

- **Discrete variables**.
    - **Finite domains**: If domain has size $d$, there are $O(d^n)$ complete assignments.
    - **Infinite domains**: Linear constraints are solvable but non-linear are undecidable.
- **Continuous variables**: Linear programming polynomial time.
- **Unary constraints**.
- **Binary constraints**: Representable with a constraint graph.
- **Higher-order constraints**.
- **Soft constraints**: Constrained optimization problem.

## Commutativity

> **Key insight** is that CPSs are commutative.

- Order of actions do not affect outcome.
- Algorithm takes advantage of this.

## Backtracking

> Basic search algorithm for CSPs.

```
Select unassigned variable X
For every value {x_1, ..., x_n} in domain of X
    If value satisfies constraints, assign X = x_i and exit loop
If an assignment is found
    Move to next variable
If no assignment is found
    Back up to preceding variable and try a different assignment
```

### Backtracking Efficiency

#### Ordering

> Which variables should be tried first? In what order should the variable's values be tried?

- **Most constrained variable**: Try the variable with the fewest remaining *legal* moves. Also known as **minimum remaining values**.
- **Most constraining variable**: Try the variable with the most constraints on the remaining variables.
- **Least constraining variable**: Try the variable which rules out the fewest values in the remaining variables.

#### Filtering

> How do we detect faliure early?

- **Forward checking**: Keep track of remaining legal values for unassigned varibles. Terminate search when any variable has no legal values.
    - Does not detect all future faliures early.
- **Arc consistency**: Given domains $D_1, D_2$, an arc is consistent if for all $x \in D_1$, there exists $y \in D_2$ such that $x, y$ are consistent.
- **K-consistency**: For all sets of $K - 1$ variables and consistent assignment of values, a consistent value is always assignable to any $K$th variable.

#### Structure

> Is it possible to exploit the problem structure? **Idea**: Break down the graph into connected components and solve each component separately.

- **Independent Subproblems**: Solve each connected component separately.
- **Tree Structures**: If the graph is a DAG, topological sort and solve in $O(nd^2)$.
- **Cutsets**: For a general graph, we define a subset of variables $S$ such that the when removed the graph is a tree. Try all values of the subset $O(d^{|S|})$ and see if there is a solution in the tree $O((n - |S|)d^2)$, total runtime is $O(d^{|S| + 2}(n-|S|))$.
- **Tree Decomposition**: Group nodes into mega-nodes forming a tree. All variables occur in at least one mega-node. Variables connected by constraints must appear together. If a variable is in multiple mega-nodes it must be in the entire path. If $w$ is the width of the tree (1 less than the size of the largest sub-problem), the runtime is $O(nd^{w + 1}$. Finding min-width decomposition is NP-hard, but we can use heuristics.

# Constraints and Local Search

> For many problems, the path is unimportant.

## Iterative Improvement Methods

- Start at some potential solution. Generate all possible points to move to. If stuck then reset, otherwise move to one of the points.

## Hill Climbing (Gradient Descent):

- Take a step in the direction which improves the current solution value the most.
- Not necessarily complete (flat optima), not optimal, gets stuck at local optima and plateaus. Random restarts fixes local optima issue.

## Simulated Annealing

> Escape local optima by allowing "downhill" movements.

- Take selected move if it improves the solution, otherwise take it with probability $p$.

### Boltzman Distribution

- $e^{\frac{\Delta V}{T}}$, $T > 0$ is the temperature parameter.
- When $T$ is high, bad moves have a chance, **exploration phase**.
- When $T$ is low, bad moves have low probability of being selected, **exploitation phase**.
- If $T$ decreases slowly enough, then we are *theoretically* guaranteeed to reach optimum.

## Genetic Algorithms

- Encoded candidate solution is an **individual**.
- Individuals have **fitness** which corresponds to the quality of the solution.
- Populations change over generations by applying operators.

1. **Selection**: Fitness proportional selection could lead to overcrowding. Ranking by percentile of fitness gets around this. Softmax (Boltzman) selection similar to simulated annealing works.
2. **Crossover**: Select random crossover point. Implemented with bitmasks.
3. **Mutate**: With small probability, modify a feature.

- In a new generation, for every child, select parents, crossover, then mutate with low probability.

# Adversarial Search

- Focused on zero-sum games.
- **MAX** player maximizes utility, **MIN** player minimizes utility.
- **Optimal stategy** leads to outcomes at least as good as any other strategy, given that MIN is playing optimally.
- **Nash Equilibrium**: $s^\star \in S$ is a Nash Equilibrium if for all $i$, $u_i(s^\star) \ge u_i(s_i, s_{-i}^\star)$, for all $s_i \in S_i$.
- **Theorem** (Kuhn): Every finite extensive form game has a subgame perfect equilibria.

## Minimax

$M(n) = \begin{cases}u(n),\ &\text{$n$ is terminal} \\ \max_{c \in succ(n)} M(c),\ &\text{$n$ is MAX node} \\ \min_{c \in succ(n)} M(c),\ &\text{$n$ is MIN node} \end{cases}$.

- Complete for fintite games, time complexity is $O(b^m)$, space complexity $O(bm), where $m$ is the number of moves in the game. Optimal.

### Alpha-Beta Pruning

> Compute minimax without searching the entire tree.

- **Alpha**: Value of the best choice so far on path for MAX.
- **Beta**: Value of the best (lowest) choice so far on path for MIN.
- We update **alpha** and **beta** as we compute $M$. Prune as soon as the current node is known to be worse than our current best result.
- Results in the same outcome as full minimax. In the worst-case it offers no improvement.

## Optimizations

> We might not have enough resources to compute full results.

- **Evaluation Functions**: Returns on estimate of the expected utility. Needs to be fast to compute. Often is a weighted function of the state features.
- **Cutting Off Search**: Instead of searching to terminal states, we search *low*, and then use evaluation functions.
    - Typically, we search deeper when the node is clearly good. Avoids **horizon effect**, where we are surprised by a bad node in the future.

### Monte-Carlo Tree Search (MCTS)

- Build a search tree according to outcomes of simulated plays.

1. **Selection**: Traverse the tree following a policy using Upper Confidence Bounds until you run out of information required to progress.
    - $v_i + c\sqrt{\frac{\ln N}{n_i}}$, where $n_i$ is the number of times we have visited the node, $N$ is the total number of runs, and $v_i$ is the expected value of the node given the information we have.
2. **Expansion**: Eventually we run out of information to progress, so expand a random child.
3. **Simulate**: Quickly simulate a game from the node.
4. **Back-propogation**: Based on simulations, update expected values.

## Stochastic Games

> Element of randomness.

- Modelled by adding **chance** nodes between MIN and MAX layers, with weights equal to the probability of every option.
- Compute **expected values** for minimax.

# Machine Learning

- **Learning**: Improving behaviour based on experience. Range of behaviour, accuracy on tasks, or speed of execution are considered improvements.
- **Supervised Classification**: Given a set of pre-classified, classify on a new instance.
    - **Feedback**: Supervices learning is explicitly given what must be learned by each example.
- **Unsupervised Learning**: Find natural classes for examples.
    - **Feedback**: No feedback is given, learner has to find patterns themselves.
- **Reinforcement Learning**: Determine what to do based on rewards and punishments.
    - **Feedback**: Feedback is only given after a sequence of actions.
- **Transfer Learning**: Learning from an expert.
- **Active Learning**: Actively seeking to learn.
- **Representation**: Richer representations are more useful for subsequent problem solving but are harder to learn.
- Measured against how well the agent performs with **new examples**.
- Tendency to prefer one hypothesis over another is a **bias**.

Given representations, data, and bias, we have a **search problem**. We search through the space of possible representations to best fit the data. The search space is typically too large for systematic search, so we use iterative improvement. So a **learning problem** is made up of a search space, an evaluation function, and a search method.

## Supervised Learning

> Given a set of input features $X_1, ..., X_n$, a set of target features $f(x)$, and a set of training examples, predict the target features for a set of test examples. This is done by returning a function that approximates $f$.

- **Classification**: $f$ is discrete.
- **Regression**: $f$ is continuous.
- **Inductive Learning Hypothesis**: We hope that the approximation of $f$ which performs well over a sufficiently large set of training examples will also perform well over any unobserved examples.
- **Evaluating Performance**: Suppose $y$ is a feature and $e$ is an example. $y(e)$ is the true value of $y$ for $e$. $y^\star(e)$ is the predicted value. The **error** is a measure of how close $y^\star(e)$ is to $y(e)$.
    - **Receiver Operator Curve**: $Recall = Sensitivity = \frac{TP}{TP + FN}$. $Specificity = \frac{TN}{TN + FP}$. $Precision = \frac{TP}{TP + FP}$. $F\text{-}measure = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}$.

### Decision Trees

> Classify instructions by storting them down the tree from root to leaf.

- Leaves are the classifications.
- Any boolean function is representable using decision trees.
- Data needs to be discrete.

#### **Inducing Decision Tree**

> Recursively choose the *most significant* attribute as the root of the subtree.

- **Entropy**: Measure of unpredictability, uncertainty of random variables.
- $I(P(v_1), ..., P(v_n)) = \sum_{i=1}^n -P(v_i) \log_2(P(v_i))$ from Information Theory. Assume $0\log(0) = 0$.
- High entropy inplies that we gain information by observing that value.
- **Information Gain**: Gain on attribute $A$ is the expected reduce in entropy. Given that attribute $A$ divides the training set $E$ into subsets $E_1, ..., E_v$, $remainder(A)$ is the weighted sum of their information. $remainder(A) = \sum_{i=1}^v \frac{p_i + n_i}{p + n} I(\frac{p_i}{p_i + n_i}, \frac{n_i}{p_i + n_i})$.
    - So $IG(A) = I(\frac{p}{p + n}, \frac{n}{p + n}) - remainder(A)$. We choose the attribute $A$ which maximizes the information given, so the minimum remainder.

## Assessing Performance

- Divide large set of examples into disjoint sets. Apply learning algorithm to training set an dmeasure performance against test set.
- **Overfitting**: Hypothesis $h \in H$ overfits training data if there exists $h^\prime$, $h \neq h^\prime$ such that $h$ has a smaller error on the training examples but $h^\prime$ has a smaller error on the entire distribution of instances.
    - Reduces performance of decision trees by 10%-25%.
    - Errors causes by bias, variance in data, noise in data.
- **Bias-Variance Trade-off**: Complicated models will not have enough data (low bias, high variance). Simple models with will have lots of data (high bias, low variance).

### Avoiding Overfitting

1. **Regularization**: Prefer small decision trees over large ones. Add complexity penalty to stopping criteria.
2. **Pseudocounts**: Add data based on previous knowledge.
3. **Cross Validation**: Split training set into training an validation. Use validation as pretend test set. Optimize hypothesis to perform well against validation set.
    - **K-fold Validation**: Divide training set into $K$ subsets, pull one out as validation and train on the rest.

## Linear Classifiers

> Data is of the form $(x, f(x))$, $x \in \mathbb{R}^n$, $f(x) \in \{0, 1\}$.

- Want $w$ with $h_w(x) = \begin{cases}1,\ &w \cdot x \ge 0\\0,\ &w \cdot x < 0\end{cases}$. Find $w$ which minimizes the loss function $L(h_w) = \sum_{i=1} (y_i - h_w(x_i))^2$.
- **Gradient Descent**: $\frac{\partial}{\partial w_i} L(h_w) = 2(y-h_w(x))(-x_i)$. Iteratively until convergence, we update $w_i = w_i - \alpha \frac{\partial}{\partial w_i} L(h_w)$, where $\alpha$ is the step size or learning rate.

## Ensembles

> Use many hypothesis and combine their predictions.

### Bagging

> Majority vote.

- Assume each hypothesis makes error with probability $p$.
- Probability that the majority is wrong is $\sum_{k = \lceil \frac{n}{2} \rceil}^n {n \choose k}p^k (1-p)^{n-k}$.
- Example: Random Forests.

> Randomly sample subsets of training data and features, learn decision trees off the subsets. Classify using majority vote of the forest.

### Boosting

- In reality, hypothesis are not equally correct and independent.
- Want to increase the weight of good hypothesis and decrease the weight of bad hypothesis, then use a weighted majority.
- Similarily, we want to increase the weight of misclassified examples.
- Example: AdaBoost.

> Compute accuracy $p$ for every hypothesis. Update weights of correct predictions by $\frac{p}{1 - p}$. Update weight of hypothesis by $\log(\frac{1-p}{p})$.

# Neural Networks

> Relationships between input and output may be complicated.

- Want methods for learning arbitrary relationships.
- **Neurons**: Dendrites are inputs, soma is activity, axon is output, synapse is links. Learning changes how efficiently signals transfer.
- **Artificial Neuron**: $in(i) = \sum_{j}w_{i, j}a(j)$, $a(i) = g(in(i))$. Activation functions should be non-linear and mimic real neurons, so output close to 0 or 1.
- **Common Activation Functions**.
    - **Rectified Linear Unit**: $g(x) = \max\{0, x\}$.
    - **Sigmoid Function**: $g(x) = \frac{1}{1 + e^x}$.
    - **Hyperbolid Tangent**: $g(x) = \tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}$.
    - **Threshold**: $g(x) = \begin{cases}1,\ &x \ge b\\0,\ &x < b\end{cases}$.
**Network Structure**:
