ECE 358
=

# 1: Introduction
## 1.1 What *is* the internet?

Millions of connected computing devices.
- **hosts** = **end systems**
- running **network apps**

## Internet protocol stack
**Application**: Running the network applications that you want (HTTP, FTP, SMTP). Implemented using software only.   
**Transport**: Responsible for communication of processes between layers. Mainly implemented using software.  
**Network**: Responsible for routing from source to destination. Implemented using software and hardware.     
**Link**: Transfering to data between physically connected (adjacent) hubs (e.g. Computer -> Router, Router -> Router ...). Hardware.    
**Physical**: How you are transfering physical bits on the wire. Not coverted in this course.

> Before the internet, there were two extra section that were planned.

**Presentation**: Applications to interpret meaning of data (e.g. encryption, compression, etc.).   
**Session**: Synchronization, checkpointing.

If you want these features, it must be implemented in the application layer.

### Encapsulation
**Switch**: Link and physical layers.
**Router**: Network, link, and physical layers.

Information is added from the above layer to the lower.

> **Transport**: Segment  
> **Network Layer**: Datagram     
> **Link Layer**: Frame   

When passing data through, you need to pop the stack (**decapsulation**) and then replace (**encapsulation**).

## Network Security

Internet was not desigend with security in mind. How can bad guys attack and how can we defend? How can we design architechtures that are immune to attacks.

In real life: Hackers hack your system and then you fix the error and catch up.

**Malware** any harmful software.   
> **Virus**: Interaction required.  
> **Worm**: Weakness in application you are already running.    
> **Spyware**: Record keystrokes, web sites, etc.

Infected hosts can be enrolled in **botnet** to help in future attacks.

### Examples of Attacks

**Denial of Service (DoS)**: Attackers infect local computers with a worm in order to all send messages to the target at the same time. Target will be busy services request so they cannot service normal customers.

**Packet "sniffing"**: Passive program that is on the link which listens to and records packets. 

**IP spoofing**: Send a message with a false source address.

**Routers**: Implements the bottom 3 layers (network, data link, physical).

**Switches**: Implements the bottom 2 layers (data link, physical).

# 2: Application Layer

## 2.1: Principles of network applications

### Creating a network app

Only write to run on end systems (not on the internal network). Does not matter if they are servers or clients. We also only care about what we want to do on the application layer and then push to the transport layer which will take care of transmitting the data. This allows for fast developement.

**Client-Server**: Server is always on with a static IP. Client will communicate with servers (intermittent). Client IP addresses will be dynamic.  
**P2P**: Arbitrary end systems directly communicate. Peers request service from other peers and provice service in return (*self scalability*). There is more complex management because there is no central network that controls all uses. Security is also a big issue.

### Process communication
**Process**: program running within a host.
- Within same host, two process communication using *inter-process communication* defined by the OS.
- Processes in different hosts communicate by exchaning messages.

**Client process**: Process that initiates communication.   
**Server process**: Process that waits to be contacted.

Process sends / recieves messages to / from it's **socket**.    
**Socket**: The gate between the application layer and the transport layer. Sockets are similar to doors. Sending process shoves message out of door and relies on the other side of the door to deliver message to the socket at the recieving process.

Example: Chrome tabs. How do we know which tab to put data? There is an identifier for each socket to route the data to the correct tab (You will always talk to port 80 of a server for HTTP requests but you will use a random port).

**Sockets** consists of two parts (for now).
1. Address of host.
2. Port number.

The combination of the two parts will create a **unique identifier** for the socket. Each process has it's own unique socket (there are over 65k ports).

### Application layer protocol defines ...

> **Rules** for when and how process send and respond to messages.

**Types of messages exchanged**: Request, response.     
**Message syntax**: What does each field mean and how fields are delineated.    
**Message semantics**: What do you want the information in the fields to mean.

Examples of open protocols are **HTTP** and **DNS** which are defined in RFCs (*Request For Comments* is a formal document from the Internet Engineering Task Force). An example of a proprietary protocol is **Skype**.

### What transport services does an application need?

> Will be implemented at the application level.

1. **Data Integrity**. Some applications require 100% data transfer whereas other applications (e.g. audio) can tolerate some loss.
2. **Timing**. Time sensitive applications (e.g. Internet telephone, interactive games) require low delay to be *effective*.
3. **Throughput** (How fast the data can be transfered). Some applications require a minimum amount to be *effective*.
4. **Security**. Encryption.

Application | Data Loss | Throughput | Time Sensitive
---|---|---|---
File Transfer | No | Elastic | No
E-Mail | No | Elastic | No
Web Documents | No | Elastic | No
Real-Time AV | Tolerant | Minimum requirements | Yes, 100's ms
Stored AV | Tolerance | Minimum requirements | Yes, few seconds
Interactive Games | Tolerant | Minimum requirements | Yes, 100's ms
Text Messaging | No | Elastic | Yes and No

### Internet transport protocol services

> What is offered by the transport layer? Which protocol should I Use? There are two main protocols ...

1. **TCP** (*Transport Control Protocol*)
    - **Reliable transport** between sending and recieving process. Everything will get there without errors and in order.
    - **Flow control**: Sender won't overwhelm reciever.
    - **Congestion control**: Throttle sender when network is overloaded.
    - **Does not provide**: Timing, minimum throughput guarantee, security.
    - **Connection-oriented**: Setup required between client and server processes.
2. **UDP** (*User Datagram Protocol*)
    - **Unreliable data transfer**: No data integrity or reliability between sending and recieving process.
    - **Does not provide**: Reliability, flow control, congestion control, timing, throughput guarantee, security, or connection setup.

Why bother with UDP?
- **UDP** is faster than **TCP**. There is no connection setup.
- There are no **overhead** bits required.

Application | Application Protocol | Transport Protocol
---|---|---
Email | SMTP | TCP
Remote Terminal Access | Telnet | TCP
Web | HTTP | TCP
File Transfer | FTP | TCP
Streaming Multimedia | HTTP, RTP | TCP or UDP (Usually UDP)
Internet Telephone | SIP, RTP | TCP or UDP

### Securing TCP

**TCP** and **UDP** have no encryption. Plaintext passwords send into sockets.

If you want security, you can use **SSL** (*secury socket layer*) which provices encryption for **TCP** connection. **SSL** is used at the application layer (need to use a **SSL** library such as OpenSSL).

## 2.2: Web and HTTP

> A *web page* consists of *objects*. (HTML, JPEG, Java applet, auto file, ...).
- Web page consists of **base HTML-file** which includes several referenced objects.
- Each object is addressable by a **URL** (Uniform Resource Locator).

**HTTP** (hypertext transfer protocol): We have a client / server model. The client sends requests to the server and the server replies with an HTTP request.

1. HTTP uses TCP. The client initiates TCP connection (creates socket) to server, port 80. (A random port is used on the client side).
2. Server accepts TCP connection from client.
3. HTTP messages (application-layer protocol messages) exchanged between browser (HTTP client) and Web server (HTTP server).
4. TCP connection closed.

*HTTP is "stateless"*. The server maintains no information about past client requests.
> Protocols that maintain "state" are complex. Past history must be maintained and if the client / server crashes, their views of the "state" may be inconsistent.

### Types of HTTP Connections
1. **Non-Persistent HTTP**
    - At most one object sent over TCP connection.
    - Downloading multiple objects requires multiple connections.
    - Server is free more frequently to server other users.
2. **Persistent HTTP**
    - Multiple objects sent over a single TCP connection.
    - Avoid eccessive overhead when requesting multiple objects.

Example: www.someschool.edu/
// TODO - Add it all in

### Response Time
**RTT** (*Round Trip Time*): Time it takes a small packet to travel from client to server and back.

**HTTP Response Time**
- One RTT to initiate TCP
- One RTT for HTTP request and first few bytes of HTTP response in return.
- File transmition time (*transmission delay*).
> Non-Persistent HTTP Response Time = 2RTT + transmission time.    
> Persistent HTTP Response Time = RTT + n(RTT + transmission time).

For non-persistent HTTP, we require 2 RTT per object. There is OS overhead for *each* TCP connection. Browsers often open parallel TCP connections to fetch referenced objects.

For persistent HTTP, subsequent HTTP messages between client / server will be over the same connection so the client will sent requests as soon as it encounters a referenced object. There is as little as on RTT per referenced object.

### HTTP Request
1. Request Line (method URL version).
    > GET /index.html HTTP/1.1\r\n
2. Header Lines (header_field_name value).
    > Host: www-net.cs.umass.edu\r\n    
    > User-Agent: Firefox/3.6.10\r\n    
    > Keep-Alive: 115\r\n   
    > Connection: keep-alive\r\n (requesting *persistent*)  
    > \r\n (carriage return, line feed at start of line indicates end of header lines)
3. Body.

### HTTP/1.0
- GET (request data).
- POST (send data).
- HEAD (leave requested object out of response).

### HTTP/1.1
- PUT (uploads file in entry to path specified in URL).
- DELETE (delete files specified in URL).

### HTTP Response
1. Status Line (protocol status_code status_phrase).
2. Header Lines (content_length).
3. Data Requested (e.g. HTML file).

### Cookies
> Used to identify users. ID is stored in client side which is sent in the header of subsequent messages.
1. Cookie header line in HTTP *response* from server.
2. Cookie hedaer line in next HTTP *request* from client.
3. Cookie file kept client-side.
4. Back-end database for server.

Cookies are used to keep **state**.

### Web Caches (Proxy Server)
> Satisfy client request without involving the original server.

All requests are sent through a proxy server instead which caches requests and responses. If the request is already cached, it does not have to travel all the way to the original server.
- Typically the cache is insalled by ISP.

Example:
> Average object size: $100\ K bits$.   
> Average request rate: $15\ /\ sec$.     
> RTT from insitutional router to original server: $2\ sec$     
> Access link rate: $1.54\ Mbps$   
>
> We then have LAN utilization: $\frac{1.5\ Mbps}{1\ Gbps} = 0.15\%$   
> Access link utilization: $\frac{1.5\ Mbps}{1.54\ Mbps} = 97.4\%$    
> Total Delay = Internet Delay + Access Delay + LAN Delay   
> Access Delay: $\frac{100\ K}{(1 - 0.974)1.54\ Mbps}$. (New users can only use whatever is left of the access link).   
> LAN Delay: $\frac{100\ K}{(1 - 0.0015)1\ Gbps}$   
> Total Delay $= 2 + 2.5 + 0.0001$

Solutions:
1. Get a faster access link. If the access link rate was increased to $154\ Mbps$, the access link delay drops to $0.0007$ so the total delay is only $2$ sec.

2. Caching. If the cache hit rate was 0.4, 40% of requests are satisfied at the cache. The data rate to browsers over the access link is $0.6 \cdot 1.50\ Mbps = 0.9\ Mbps$. Utilization becomes $0.58$. Total Delay $= 0.6 \cdot$(delay from original servers) + $0.4 \cdot$(delay when satisfied by the cache) $= 0.6(2.15) = 1.29\ sec$.

**Conditional GET**
- If the cache has an up-to-date version, don't send the object.
- The web server sends a conditional get with the *If-modified-since*.
    - Response code **304 Not Modified** if the cached version is fine.
    - Response code **200 OK** will send the updated data.

## 2.3: DNS
> Domain Name System.

Internet uses 32-bit IP address to identify hosts connected to the internet. The public cannot remember the real address so we use easy to remember names.

- **Distributed Database**: Implemented in hierarchy of many *name servers*
- Main function is to resolve names into addresses (IP address).

> For complex objects, we push complexity to the edge (host systems).

### DNS Services
- Hostname to IP address translation.
- Host aliasing.
- Mail server aliasing.
- Load distribution.
    - Many IP addresses correspond to the same name.

### Why Not Centralize DNS?
> Not scalable.
- Single point of faliure.
- Traffic volume.
- Distant centralized database.
- Maintenance.

### Hierarchy
- Bottom Tier: amazon.com, umass.edu.
- Middle Tier: com DNS servers, edu DNS servers.
- Top Tier: Root DNS servers.

If the client wants the IP for www.amazon.com.
> 1. Queries root domain server to find com DNS server.
> 2. Queries .com DNS server to get amazon.com DNS server.
> 3. Queries amazon.com DNS server to get IP address for www.amazon.com.

### TLD Servers
- Responsible for com, org, etc.
- Network Solutions maintains servers for .com TLD
- Educause for .edu TLD

### Authorative DNS Servers
- Organization's own DNS server(s), providing authorative hostname to IP mappings for organization.

### Local DNS Name Server
- Each ISP has one, acts as a proxy and forwards query to hierarchy.

If the host at cis.poly.edu wants IP address for gaia.cs.umass.edu.
> **Iterated Query**:
> - Contacted server replies with name of server to contact.
> - "I don't know this name, but ask this server".
>
> **Recursive Query**:
> - Puts burden of name resolution on contacted name server.
> - Heavy load at upper levels of hierarchy?

Once (any) name server learns the mapping, it **caches** mapping.
- Cache entries timeout after some time (TTL).
- TLD servers typically cached in local name servers.
- Cached entries may be *out-of-date* until everyone's TTLs expire.

### DNS Records
> Distributed database storing resource records (RR).

**RR Format: (Name, Value, Type, TTL).**
- type = A.
    - **name** is hostname.
    - **value** is IP address.
    - (www.amazon.com, 220.180.90.10, A, TTL).
- type = NS.
    - **name** is domain (e.g. foo.com).
    - **value** is hostname of authorative name server for this domain.
    - (uwaterloo.ca, dns.uwaterloo.ca, NS, TTL).
- type = CNAME.
    - **name** is alias name for some "canonical" (the real) name.
    - **value** is canonical name.
    - (www.ibm.com, servereast.backup2.ibm.com, CNAME, TTL).
- type = MX.
    - **value** is the name fo mailserver associated with **name**.

### DNS Protocol, Messages
> *Query* and *reply* messages have the same *message format*.

**msg Header**:
> Chapter 2 slides have a better visualization for format.
- **Identification** (*2 bytes*)16 bit number for query, reply to user uses the same number.
- **Flags** (*2 bytes*):
    - Query or reply.
    - Recursion desired.
    - Recursion available.
    - Reply is authorative.
- **Questions**: Name, type fields for a query.
- **Answers**: RRs in response to a query.
- **Authority**: Records for authoritative servers.
- **Additional Info**: Additional "helpful" info that may be used.

### Attacking DNS
1. DDoS Attacks.
    - Bombard root servers with traffic.
        - Not successful to date.
        - Traffic filtering (not all traffic provited to root servers).
        - Local DNS servers cache IPs of TLD servers, allow the root server to be bypassed.
    - Bombard TLD servers.
        - Potentially more dangerous.
2. Redirect Attacks.
    - Man-in-middle.
        - Attacker intercepts query and returns a *modified* response. They can redirect you to their own websites.
    - DNS poisoning.
        - Inserts an incorrect record to the DNS server.
3. Exploit DNS for DDoS.
    - Use already existing DNS to attack.
    - Send a DNS query and claim it is from the victim.
        - The victim is busy processing all of the responses.
    - Requires ampliciation (difficult to launch because you need to query many servers at the same time).

# 3: Transport Layer

## 3.1: Transport-Layer Services
- Provices **logical communication** between a process on the source to a process on the reciever.
- Transport protocols run in end systems.
    - Source side: breaks app messages into *segments*, passes into network layer.
    - Recieving side: Reassembles segments into *messages*, passes to the application layer.
- More than one transport protocol available to apps.
    - Internet: TCP and UDP.

### Transport Layer vs. Network Layer
- **Network layer**: Logical communication between hosts.
- **Transport layer**: Logical communication between processes.
    - Relies on enhanced network layer services.

**Household Analogy**
> 12 kids in Ann's house sending letters to 12 kids in Bill's house.
> - **hosts**: houses.
> - **processes**: kids.
> - **app messages**: letters in envelopes.
> - **transport protocol**: Ann and Bill who demux to in-house siblings.
> - **network protocol**: postal service.

### Internet Transport-Layer Protocols
- Reliable, in-order delivery (TCP).
    - Congestion control (about the network itself).
    - Flow control (sender will not overflow the reciever).
    - Connection setup.
- Unreliable, unordered delivery (UDP).
    - No-frills extension of *best-effort* IP.
- Services not available are **delay guarantees** and **bandwidth guarantees**.

## 3.2: Multiplexing / Demultiplexing

**Multiplexing at sender**: Handle data from multiple sockets, add transport header (later used for demultiplexing).

**Demultiplexing at reciever**: Use header info to deliver recieved segments to the correct socket.

### How Demultiplexing Works
- Host recieves IP datagrams.
    - Datagram has a source IP address, destination IP address.
    - Datagram carries one transport-layer segment.
    - Segment has source, destination port number.
- Host uses *IP addresses* and *port numbers* to direct segment to appropriate socket.

### Connectionless Demultiplexing
- When creating a datagram to send into UDP socket, we must specify the destination IP address and port number.
- When a host recieves a UDP segment, it checks the destination port number and directs the segment to the proper socket.
- If IP datagram shave the same destination port but different source IP addresses or source port, numbers will be directed to the **same socket** at the destination.

### Connection-oriented Demux
- TCP socket identified by a 4 tuple: (source IP address, source port number, destination IP address, destination port number).
- Web servers have different sockets for each connecting client.
    - Non-persistent HTTP will have a different socket for each request. Multiple segments will be sent to the host at port 80 but will be demultiplexed to different sockets.

## 3.3 Connectionless Transport: UDP
> When we want the speed to be as fast as possible and we are loss tolerant.

- "No frills", "bare bones" Internet transport protocol.
- "Best effort" service, UDP segments may be lost or delivered out-of-order to app.
- **Connectionless** meaning that there is no handshaking between UDP sender and reciever. Each UDP segment is handled independently of others.
- UDP is used in streaming multimedia apps, DNS, SNPM (Simple Network Management Protocol).
- Reliable transfer over UDP can be achieved by adding reliability at the application layer such as application-specific error recovery.

UDP adds a segment header of 64 bits. The first 32 are used for the source port number and the destination port number. The next 16 are used for the length in bytes of the segment (including header), and the final 16 are used for **checksum**.

### UDP Checksum
> We want to detect "errors" (e.g. flipped bits) in transmitted segmet.

The sender treats the segment's contents as a sequence of 16-bit integers. The checksum is the addition (one's complement sum) of segment contents. The reciever then computes their own checksum and compares. (When adding numbers, a carryout from the msb gets re-added to the result).

## 3.4 Principles of Reliable Data Transfer
- When we have a reliable channel, we do not have to worry about reliable data transfer.
- Characteristics of unreliable channels will determine the complexity of reliable data transfer protocol (rdt).

rdt_send() is called from the application layer whenever we have data to send which passes udt_send() to transfer the packet over the unreliable channel to reciever. It is recieved by rdt_rcv() which gets passed to the protocol on the transport layer. After some processing, it is passed to the upper application layer through deliver_data().

### RDT 1.0: Reliable transfer over a reliable channel.
> Underlying channel is perfectly reliable. There are no bit errors and no loss of packets.

We have separate Finite State Machines (FSMs) for sender, reciever. The sender sends data into the underlying channel and the reciever reads data from the underlying channel.

### RDT 2.0: Channel with Bit Errors
> Underlying channel may flip bits in packet which is detected by the checksum.

- **ACKs** (*acknowledgements*): The reciever explicity tells sender that the packet was recieved ok.
- **NAKs** (*negative acknowledgements*): The reciever explicity tells sender that the packet has errors.
    - Sender retransmits packet on receipt of NAK.

- The sender needs to add a checksum to the data and will wait for ACK or NAK.
- There is a **fatal flaw** because if ACK / NAK is corrupted, the sender does not know what happened at the reciever.

#### RDT 2.1
- The sender retransmits the current packet if ACK / NAK is corrupted and adds a *sequence number* to each packet.
    - We only need 2 sequence numbers (0, 1) so that we can distinguish between consecutive messages.
- The reciever discards duplicate packets.

> **Stop and Wait** protocol: sender sends 1 packet and then waits for receiver response.

#### RDT 2.2: NAK-free
- We can have less state transitions if we only use ACKs.
- The reciever can send a packet number as well.
- Duplicate ACK at sender results in the same action as NAK: *retransmit the current packet*.

### RDT 3.0: Channels with Errors *and* Loss
> Now the underlying channel can also lose packets (data, ACKs).
- Checksum, sequence number, ACKs, retransmission will be of help ... but it is not enough.

> The sender will wait a "reasonable" amount of time for ACK. It will retransmit if no ACK is received in time.
- If packet of ACK is delayed (not lost), the retransmission would be a duplicate, but we already handle this with sequence numbers.
- This approach works but the performance is bad.
    - Example: 1 Gbps link, 15 ms propogation delay, 8000 bit packet.
    > $D_{trans} = \frac{L}{R} = \frac{8000\ bits}{10^9\ bits/sec} = 8\ \mu s$.     
    > $U_{sender}$: **utilization**, the fraction of the time the sender is busy sending.   
    > $U_{sender} = \frac{L/R}{RTT + L/R} = \frac{0.008}{30.008} = 0.00027$.    
    - Throughput is just the utilization multiplied by the link rate.
- The solution is to transfer more packets!

## Pipelined Protocols
> Sender allows multiple, "in-flight", packets.
- The range of sequence numbers must be increased.
- There is buffering at the sender and / or reciever.

### Go-Back-N
- The sender can have up to N unACKed packets in a pipeline.
- The receiver only sends a **cumulative ACK** (doesn't ACK packet if there is a gap).
- Sender has a timer for the oldest unACKed packet. When the timer expires, it retransmits *all* unACKed packets.
- Any out of order packet (at the receiver) or duplicate ACK (at the sender) will be discarded.

### Selective Repeat
- The sender can have up to N unACKed packets in the pipeline.
- The receiver sends **individual ACK** for each packet.
- Sender maintains a timer for each unACKed packet and only transmits that specific packet when the timer expires.

#### Sender
- **Data from above**: If next available sequence number in window, send packet.
- **Timeout($n$)**: Resent packet $n$, restart timer.
- **ACK($n$)** ($n \in [send\_base, send\_base + N - 1]$): Mark packet $n$ as received, if $n$ is the smallest unACKed packet, advace the window base to the next unACKed sequecne number.

#### Receiver
- **Packet $n \in [rcv\_base, rcv\_base + N - 1]$**: Send ACK($n$). If $n$ is out of order we need to buffer. If $n$ is in order, we can deliver until the next not received packet.
- **Packet $n \in [rcn\_base - N, rcv\_base - 1]$**: ACK($n$) (We need to acknowledge duplicate packets to move the sender's window).

**Selective Repeat: Dilemma**: While sliding windows, we may accidentally accept old data as new. The problem is that the receiver knows nothing about the sender. We can *fix* this by setting the window size to be equal to less than half of the sequence number space.

### Performace of Sliding Window
**No Errors**: $\rho = \min(1, \frac{W\frac{L}{C}}{t_T})$.

## Automatic Repeat Request (*ARQ*) Protocols
> Applied at the transport and link layers.
1. Stop-and-Wait Protocol.
2. Go-Back-N.
3. Selective Repeat.

## 3.5 Connection-Oriented Transport: TCP
- **Connection-Oriented**: Handshaking (exchanging control mesages), initializes sender and receiver state before data exchange.
- **Point-To-Point**: One sender, one receiver.
- **Reliable, In-Order *Byte* Steam**: No "message boundaries", byte by byte.
- **Full Duplex Data**: Bidirectional. MSS: Maximum Segment Size.
- **Pipelined**: TCP congestion and flow control set window size.
- **Flow Controlled**: Sender will not overwhelm receiver.

### TCP Segment Structure
- 16 bits for source port number, 16 for destination port number.
- 32 bits for sequence number (for the bytes).
- Acknowledgement number (for the bytes).
- Header length, not used (URG: urgent data, ACK: ACK # valid, PSH: push data now, RST, SYS, FIN: connection establishment).
- Receive window for the number of bytes the receiver is willing to accept.
- 16 bits for checksum, Urg data pointer for the last byte of urgent data (16 bits).
- Options (variable length).
- Application data (variable length).

Default header length is 20 bytes compared to UDP 8 bytes.

Acknowledgements include the sequence number of the next byte expected from the other side (cumulative ACK). The TCP spec doesn't say how to handle out-of-order segments, it is up to the implementor. There are two choices, discard or buffer. Buffering is used in practice.

### TCP Round Trip Time, Timeout
> How to set TCP timeout value?
It should be longer than the RTT, but the RTT varies. If it is too short, we will have unnecessary retransmissions. If it is too long, we will have slow reactions to segment loss.

We estimate RTT by sampling from segment transmissions. $SampleRTT$ is the measured time from segment transmission to ACK receipt (ignoring retransmissions). We use an exponential weighted moving average, $EstimatedRTT = (1 - \alpha)\cdot EstimatedRTT + \alpha \cdot SampleRTT$. The typical value is $\alpha = 0.125$.

Is this estimation enough? We want to add a *safety margin* so that a large variation in $EstimateRTT$ results in a larger safety margin. We use 4 standard deviations. $DevRTT = (1 - \beta)\cdot DevRTT + \beta \cdot |SampleRTT - EstimatedRTT|$. The typical value is $\beta = 0.25$.

The timeout interval is then $TimeoutInterval = EstimatedRTT + 4 \cdot DevRTT$.

### TCP Reliable Data Transfer

TCP Created a RDT service on top of IP's unreliable service. It uses pipelined segments, cumulative ACKs, and single retransmission timer (similar to Go-Back-N). Retransmission are triggered by timeouts or duplicate ACKs.

Let's initially consider a simplified TCP sender with no duplicate ACKs and ignoring flow / congestion control.

### TCP Sender Events:
> Three major events in sender.
1. Data recieved from application layer.
    - Create segment with sequence number. 
    - Sequence number is byte-stream number of the first data byte in the segment.
    - Start timer if not already running.
2. Timeout.
    - Retransmit segment that caused timeout.
    - Restart timer.
3. ACK recieved.
    - If ACK acknowledges previously unACKed segments, we update and then start timer if there are still unACKed segments.

### TCP ACK Generation
Event At Receiver | TCP Receiver Action
--- | ---
Arrival of in-order segment with expected sequence number. All data up to the expected sequence number are already ACKed. | Delayed ACK. Wait up to 500ms for the next segment. If no next segment, send ACK.
Arrival of in-order segment with expected sequence number. One other segment has ACK pending. | Immediately send single cumulative ACK, ACKing both in-order segments.
Arrival of out-of-order segment (heigher than expected sequence number). Gap detected. | Immediately send *duplicate ACK*, indicating sequence number of the next expected byte.
Arrival of segment that partially or completely fills the gap. | Immediate send ACK, provided that the segment starts at the lower end of the gap.

### TCP Fast Retransmit
If you receive 3 duplicate ACKs, it is an indicator that a packet has been lost. The sender will retransmit immediately instead of waiting for the timeout.

### TCP Flow Control
> Receiver controls sender, so that the sender won't overflow receiver's buffer by transmitting too much, too fast.

Applications may remove data from TCP socket buffers slower than the TCP receiver is delivering (sender is sending).

- Receiver "advertises" free buffer space by including **rwnd** value in the TCP header of receiver-to-sender segments.
    - **RcvBuffer** size is set via socket options (typical default is 4096 bytes).
    - Many operating systems auto adjust **RcvBuffer**.
- Sender limits the amount of unACKed (*in-flight*) data to the receiver's **rwnd** value.
- This guarantees that the buffer will not overflow.

### Connection Management
> Before exchanging data, the sender and receiver "handshake" (agree to establish connection, agree on connection parameters).

## 3.6 Principles of Congestion Control
> Too many sources sending too much data too fast for the **network** to handle.
- Different from flow control.
- Manifests as lost packets (buffer overflow at routers) and long delays (queueing in router buffers).

### Causes / Costs of Conegestion
1. Infinite buffer, 2 senders and 2 receivers use the same link with an output link capacity of $R$.
    - Maximum per-connection throughput: $\frac{R}{2}$.
    - Large delay as the arrival rate $\lambda_{in}$ approaches $\frac{R}{2}$.
2. Finite buffer. Input and output rates are the same.
    - Transport-layer input includes *retransmissions*: $\lambda_{in}^\prime \ge \lambda_{in}$ (*offered load*).
3. Multiple routers.
    - One connection may take up all of the routers' time.
    - More routers involved in delivering data means more wasted network efforts. This is the reason for sudden drop on throughput.
    - When packet is dropped, any "upstream" transmission capacity is wasted!

### Approaches to Congestion Control
> Two broad approaches.
1. End-End Congestion Control.
    - No explicit feedback from network.
    - Congestion inferred from end-system observed loss, delay.
    - Approach taken by TCP.
2. Network-Assisted Congestion Control.
    - Routers provide feedback to end systems.
        - Single bit indicating congestion.
        - Explicit rate for sender to send at.

## 3.7 TCP Congestion Control
> Additive increase, multiplicative decrease (**AIMD**).
- Sender increases transmission rate (window size), probing for usable bandwidth, until loss occurs.
    - Increase **cwnd** by 1 MSS (*maximum signal size*) every RTT until loss detected.
    - Cut **cwnd** in half after loss.
- Recall that the sender limits transmission.
    - $LastByteSent - LastByteAcked \le \min(cwn, cwnd)$.
- TCP sending rate is roughly sending **cwnd** bytes, then waiting an RTT for ACKs.
    - $\frac{cwnd}{RTT}$ bytes / sec.

### TCP Slow Start
- Initially **cwnd** is 1 MSS and is doubled for every RTT.
    - This is done by incrementing **cwnd** for every ACK received.
- Initial rate is slow but ramps up exponentially fast.

### Detecting, Reacting to Loss
- Loss indicated by timeout.
    - **cwnd** set to 1 MSS.
    - Window then grows exponentially (as in slow start) to the threshold, then grows linearly.
- Loss indicated by 3 duplicate ACKs (*TCP RENO*).
    - Duplicate ACKs indicate that the network is capable of delivering some segments.
    - **cwnd** is cut in half and then the grows linearly.
- (*TCP TAHOE*) always sets **cwnd** to 1 (timeout of 3 duplicate ACKs).

### Switching from Slow Start to CA (*Congestion Avoidance*)
> When should the exponential increase switch to linear?    
> When **cwnd** gets to $\frac{1}{2}$ of its value before timeout.

Implemented with a variable **ssthresh**. On a loss event, **ssthresh** is set to $\frac{1}{2}$ of **cwnd** before loss event.

### TCP Throughput
- Average TCP throughput as a function of window size, RTT?
    - Ignore slow start, assume we always have data to send.
- Throughput fluctuates between the max of $\frac{W}{RTT}$ (*congestion*) and $\frac{W}{2RTT}$ (*after congesion, drop rate*).
    - Average window size is $\frac{3W}{4}$.
    - Average throughput is $\frac{3W}{4RTT}$ bytes / sec.