<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>stat230</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/contrib/auto-render.min.js"></script><script>document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body);
  });</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="stat-230">STAT 230</h1>
<p>Example: Suppose you have 20 distinct books, 7 of which are written by Mark Twain.</p>
<ol type="1">
<li><p>How many ways can you arrange 12 books on a shelf if exactly 3 of them must be Mark Twain books?</p>
<blockquote>
<p><span class="math inline">\({7 \choose 3}{13 \choose 9}\)</span> ways to choose subsets.<br />
Books can be rearranged in <span class="math inline">\(12!\)</span> ways.<br />
Therefore, the answer is <span class="math inline">\(12!{7 \choose 3}{13 \choose 9}\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: Consider drawing 3 numbers at random <strong>with</strong> replacement from the digits 0 to 10. What is the probability that there is a repeated number among the 3.</p>
<blockquote>
<p>We can consider the complement, where there are no repeated numbers.<br />
<span class="math inline">\(1 - \frac{10 \cdot 9 \cdot 8}{10^3}\)</span></p>
</blockquote>
<p>Example: Suppose there are 4 passengers on an elevator that services 5 floors. Each passenger is equally likely to get off at any floor.</p>
<blockquote>
<p><span class="math inline">\(|S| = 5^4\)</span></p>
</blockquote>
<ol type="1">
<li><p>What is the probability that the passengers all get off on different floors?</p>
<blockquote>
<p><span class="math inline">\(|A| = 5^{(4)}\)</span>. Therefore <span class="math inline">\(P(A) = \frac{5^{(4)}}{5^4}\)</span>.</p>
</blockquote></li>
<li><p>2 passengers get off on floor 2, and 2 get off on floor 3.</p>
<blockquote>
<p><span class="math inline">\(|A| = {4 \choose 2}{2 \choose 2}\)</span>. Therefore <span class="math inline">\(P(A) = \frac{{4 \choose 2}{2 \choose 2}}{5^4}\)</span>.</p>
</blockquote></li>
<li><p>2 passengers get off on one floor, and 2 passengers get off on another floor.</p>
<blockquote>
<p><span class="math inline">\({5 \choose 2}{4 \choose 2}{2 \choose 2}\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: Consider rearranging the letters at random in the name “ZENYATTA” to form a single ‘word’.</p>
<ol type="1">
<li><p>How many ways can this be done?</p>
<blockquote>
<p><span class="math inline">\(|S| = {8 \choose 2}{6 \choose 2}{4 \choose 1}{3 \choose 1}{2 \choose 1}{1 \choose 1} = \frac{8!}{2!2!}\)</span>.</p>
</blockquote></li>
<li><p>What is the probaility that all of the letters appear in alphabetical order?</p>
<blockquote>
<p>A = { “AAENTTYZ” }. Therefore <span class="math inline">\(P(A) = \frac{1}{|S|}\)</span>.</p>
</blockquote></li>
<li><p>What is the probability that the word begins and ends with “T”?.</p>
<blockquote>
<p><span class="math inline">\(|A| = {6 \choose 2}{4 \choose 1}{3 \choose 1}{2 \choose 1}{1 \choose 1}\)</span>.</p>
</blockquote></li>
</ol>
<p><span class="math inline">\({n \choose n_1}{n - n_1 \choose n_2}...{n_k \choose n_k} = {n \choose n1,n2,...,n_k}\)</span>.</p>
<p>Example: Harold’s daily morning ritual is to drink 5 cans of pop. He has 2 cans of pop C, 2 cans of pop F, and 1 can of pop P. How many ways can he complete the ritual?</p>
<blockquote>
<p><span class="math inline">\(|A| = \frac{5!}{2!2!} = 30\)</span>.</p>
</blockquote>
<p>Example: Find the probability that a bridge hand (13 cards picked at random from a standard deck without replacement) has …</p>
<blockquote>
<p><span class="math inline">\(|S| = {52 \choose 13}\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>3 aces.</p>
<blockquote>
<p><span class="math inline">\(|A| = {4 \choose 3}{48 \choose 10}\)</span>. Therefore <span class="math inline">\(P(A) = \frac{{4 \choose 3}{48 \choose 10}}{52 \choose 13}\)</span>.</p>
</blockquote></li>
<li><p>At least 1 ace.</p>
<blockquote>
<p>Consider the complement. <span class="math inline">\(|A^c| = {48 \choose 13}\)</span>. Therefore <span class="math inline">\(P(A) = 1 - \frac{48 \choose 13}{52 \choose 13}\)</span>.</p>
</blockquote></li>
<li><p>6 spaces, 4 hearts, 2 diamonds, 1 club.</p>
<blockquote>
<p><span class="math inline">\(|A| = {13 \choose 6}{13 \choose 4}{13 \choose 2}{13 \choose 1}\)</span>. Therefore <span class="math inline">\(P(A) = \frac{{13 \choose 6}{13 \choose 4}{13 \choose 2}{13 \choose 1}}{52 \choose 13}\)</span>.</p>
</blockquote></li>
</ol>
<p>For verifying your answer when counting across disjoint unions, adding up the top and bottom numbers for the <span class="math inline">\({n \choose k}\)</span> in <span class="math inline">\(|A|\)</span> should add up to <span class="math inline">\(|S|\)</span>.</p>
<p>Example (<em>Team captain problem</em>): Show that <span class="math inline">\({n \choose k} \cdot k = {n -1 \choose k - 1} \cdot n\)</span>.</p>
<blockquote>
<p>We are either picking the team first and then choosing the captain, or the other way around.</p>
</blockquote>
<p>Example (<em>Vandermonde’s Identity</em>): Show that <span class="math inline">\({n + m \choose k} = \sum_{i = 0}^k {n \choose i}{m \choose k - i}\)</span> for non-negative integers <span class="math inline">\(n, m, k\)</span>.</p>
<h2 id="inclusion-exclusion-principle">Inclusion Exclusion Principle</h2>
<blockquote>
<p><span class="math display">\[P(\cup_{i = 1}^n A_i) = \cup_{i} P(A_i) - \cup_{i &lt; j} P(A_iA_j) + \cup_{i &lt; j &lt; k}P(A_iA_jA_k) - ...\]</span></p>
</blockquote>
<p>Example: Suppose that 2 fair 6 sided dce are rolled. What is the probability that at least 1 of the dice shows a 6?</p>
<blockquote>
<p><span class="math inline">\(P(A) = 1 - P(A^c) = 1 - (\frac{5}{6})^2 = 1 - \frac{25}{36} = \frac{11}{36}\)</span>.</p>
</blockquote>
<p>Example: There are 6 stops on the subway and 4 passengers on the subway car. Assume the passengers are equally likely to get off at any stop. Find the probability that:</p>
<blockquote>
<p><span class="math inline">\(|S| = 6^4\)</span>.</p>
</blockquote>
<ol type="1">
<li>The passengers all get off at different stops.</li>
<li>2 passengers get off at stop 2 and 2 passengers get off at stop 5.</li>
<li><p>2 passengers get off at 1 stop and the other 2 passengers get off at another 1 stop.</p>
<blockquote>
<p><span class="math inline">\(|A| = {6 \choose 2}{4 \choose 2}{2 \choose 2}\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: 5 tourists plan to attent Octoberfest. There are 7 locations possible. Find the probability that:</p>
<blockquote>
<p><span class="math inline">\(|S| = 7^5\)</span></p>
</blockquote>
<ol type="1">
<li>All tourist attend different locations.</li>
<li>The tourists all attend the same location.</li>
<li><p>2 tourists attend 1 location and 3 tourists attend another location.</p>
<blockquote>
<p><span class="math inline">\(|A| = {7 \choose 2} \cdot 2 \cdot {5 \choose 2}{3 \choose 3} = 7 \cdot 6 \cdot {5 \choose 2}{3 \choose 3}\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: Consider rearranging the “NOOB” at random. Let <span class="math inline">\(A\)</span> represent the event that two “O”s appear together, and let <span class="math inline">\(B\)</span> denote the event that the word starts with the letter <span class="math inline">\(N\)</span>. Determine.</p>
<ol type="1">
<li><span class="math inline">\(P(A) = \frac{3!}{\frac{4!}{2!}} = \frac{1}{2}\)</span></li>
<li><span class="math inline">\(P(B) = \frac{3!}{4!} = \frac{1}{4}\)</span></li>
<li>The probability that the resulting word does not start with “N” and that the “O”s do not appear together. <span class="math inline">\(P(\overline{A} \cap \overline{B}) = P(\overline{A \cup B})\)</span>.</li>
</ol>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>indepenedent</strong> if <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span>.</p>
<p>Example: Consider rolling 2 fair 6 sided dice. <span class="math inline">\(A\)</span> is the event where the sum is 10. <span class="math inline">\(B\)</span> is the event that the first die rolls a 6. <span class="math inline">\(C\)</span> is the event that the sum is 7. Are <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> independent?</p>
<blockquote>
<p><span class="math inline">\(B = \{(6, i) \mid i \in \{1, ..., 6\}\}\)</span>. <span class="math inline">\(A = \{(5, 5), (6, 4), (4, 6)\}\)</span>. We can see that <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> are not independent.</p>
</blockquote>
<p>If <em>knowing</em> <span class="math inline">\(A\)</span> restricts our choices for <span class="math inline">\(B\)</span>, they are not independent.</p>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>mutually exclusive</strong> if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint.</p>
<p><strong>Proposition</strong>: Suppose that not both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are trivial events. If <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> are indepenent <strong>and</strong> mutually exclusive, then either <span class="math inline">\(P(A) = 0\)</span> or <span class="math inline">\(P(B) = 0\)</span>.</p>
<p><strong>Proposition</strong>: If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, then <span class="math inline">\(\overline{A}\)</span> and <span class="math inline">\(\overline{B}\)</span> are independent, <span class="math inline">\(A\)</span> and <span class="math inline">\(\overline{B}\)</span> are independent, and <span class="math inline">\(\overline{A}\)</span> and <span class="math inline">\(B\)</span> are independent.</p>
<p><strong>Definition</strong>: Conditional probabilty of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>, so long as <span class="math inline">\(P(B) &gt; 0\)</span>, is denoted by <span class="math inline">\(P(A \mid B) = \frac{P(A \cup B)}{P(B)}\)</span>.</p>
<p><strong>Definition</strong>: For events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, <span class="math inline">\(P(A \cap B) = P(A \mid B)P(B) = P(B \mid A)P(A)\)</span>.</p>
<p><strong>Bayes Theorem</strong>: <span class="math inline">\(P(B_i | A) = \frac{P(A|B_i)P(B_i)}{\sum_{j = 1}^k P(A | B_j)P(B_j)}\)</span></p>
<p>Example: Pick Pharah, 20% chance of loss. Pick Winston, 10% chance of loss. Pick Winston 70% of the time, Pharah 30% of the time. Given that the game was lost, what is the probability that Pharah was picked?</p>
<blockquote>
<p>Let <span class="math inline">\(P\)</span> be the event that Pharah is picked. Let <span class="math inline">\(L\)</span> be the event where the game is lost. Let <span class="math inline">\(W\)</span> be the event that Winston is picked. <span class="math display">\[\begin{aligned}P(P| L) &amp;= \frac{P(L| P)P(P)}{P(L)} \\ &amp;= \frac{0.2 \cdot 0.3}{P(L| P)P(P) + P(L| W)P(W)} \\ &amp;= \frac{0.2 \cdot 0.3}{0.2 \cdot 0.3 + 0.1 \cdot 0.7} \\ &amp;= 0.461
5\end{aligned}\]</span></p>
</blockquote>
<p><strong>Probablity Function</strong>: <span class="math inline">\(f_X(x) = P(X = x)\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(0 \le f_X(x) \le 1\)</span>.</li>
<li><span class="math inline">\(\sum_{x \in X(S)}f_X(x) = 1\)</span>.</li>
</ol>
<p><strong>Cumulative Distribution Function (<em>CDF</em>)</strong>: <span class="math inline">\(F_X(x) = P(X \le x)\)</span>, <span class="math inline">\(x \in \mathbb{R}\)</span>. We can use <span class="math inline">\(P(X \le x) = P(\{\omega \in S: X(\omega) \le x\})\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(0 \le F_X(x) \le 1\)</span>.</li>
<li><span class="math inline">\(F_X(x) \le F_X(y)\)</span> for <span class="math inline">\(x &lt; y\)</span>.</li>
<li><span class="math inline">\(\lim_{x \to -\infty}F_x(x) = 0\)</span>, and <span class="math inline">\(\lim_{x \to \infty}F_X(x) = 1\)</span>.</li>
</ol>
<p>Example: <span class="math inline">\(N\)</span> balls labelled <span class="math inline">\(1, 2, ..., N\)</span> are placed in a box, and <span class="math inline">\(n \le N\)</span> are randomly selected without replacement. Find <span class="math inline">\(P(X = x)\)</span> where random variable <span class="math inline">\(X\)</span> is the largest number selected.</p>
<blockquote>
<p><strong>pdf</strong>: There is only 1 way to select <span class="math inline">\(x \in \{1, ..., n\}\)</span>. There are <span class="math inline">\({x - 1 \choose n - 1}\)</span> ways to pick the remaining <span class="math inline">\(n - 1\)</span> balls. <span class="math inline">\(P(X = x) = \frac{x - 1 \choose n - 1}{N \choose n}\)</span>, <span class="math inline">\(x \ge n\)</span>. <span class="math inline">\(P(X \le x) = \frac{x \choose n}{N \choose n}\)</span>, <span class="math inline">\(x \ge n\)</span>.</p>
<p>Now we use the property of <strong>pdf</strong>. <span class="math display">\[\begin{aligned}P(X = x) &amp;= F(x) - F(x - 1) \\ &amp;= \frac{x \choose n}{N \choose n} - \frac{x - 1 \choose n}{N \choose n} \\ &amp;= \frac{x - 1 \choose N - 1}{N \choose n}\end{aligned}\]</span></p>
</blockquote>
<p>Example: Suppose a tack when flipped has probability <span class="math inline">\(0.6\)</span> of landing point up. If the tack is flipped <span class="math inline">\(10\)</span> times, what is the probability it lands point up more than twice?</p>
<blockquote>
<p><span class="math inline">\(X \in \{0, 1, 2, ..., 10\}\)</span>, <span class="math inline">\(X \sim Bin(n = 10, p = 0.6)\)</span> .</p>
<p>We want <span class="math inline">\(P(X \ge 3) = 1 - P(X &lt; 2) = 1 - (0.6^{10} + {10 \choose 1}0.6^9 0.4 + {10 \choose 2}0.6^8 0.4^2)\)</span>.</p>
</blockquote>
<p>Example: Suppose a fair coin is flipped 17 times. Let <span class="math inline">\(X\)</span> denote the number of heads observed, and let <span class="math inline">\(Y\)</span> denote the number of tails observed. Which of the following is false?</p>
<blockquote>
<ul>
<li><span class="math inline">\(X \sim Binomial(17, 0.5)\)</span>.</li>
<li><span class="math inline">\(Y \sim Binomial(17, 0.5)\)</span>.</li>
<li><span class="math inline">\(X \sim Y\)</span> (<em>X follows the distribution of Y</em>).</li>
<li><span class="math inline">\(X + Y = 17\)</span>.</li>
<li><span class="math inline">\(X = Y\)</span> (<em>False, because RV quantity is not fixed</em>).</li>
</ul>
</blockquote>
<p>Example: Weekly lottery has a probability of 0.02 of winning a prize with a single ticket. If you buy one ticket per week for 52 weeks, what is the probability that you …</p>
<blockquote>
<p><span class="math inline">\(X \sim Bin(n=52, p=0.02)\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>Win no prizes?</p>
<blockquote>
<p>We want <span class="math inline">\(P(X = 0) = 0.98^{52}\)</span>.</p>
</blockquote></li>
<li><p>Win 3 or more prizes?</p>
<blockquote>
<p>We want <span class="math inline">\(P(X \ge 3) = 1 - (P(X = 0) + P(X = 1) + P(X = 2))\)</span>.</p>
</blockquote></li>
</ol>
<p>Binomial and hypergeometric distributions are fundamentally different, as the former picks with replacement, whereas the latter picks without replacement.</p>
<p><strong>Theorem</strong>: If <span class="math inline">\(r\)</span> and <span class="math inline">\(N\)</span> relatively larger than <span class="math inline">\(n\)</span> and <span class="math inline">\(\frac{r}{N} = p\)</span> where <span class="math inline">\(p \in [0, 1]\)</span>, then if <span class="math inline">\(X \sim Hypergeometric(N, r, n)\)</span> and <span class="math inline">\(Y \sim Binomial(n, p)\)</span> then <span class="math inline">\(P(X \le k) \approx P(Y \le k)\)</span>.</p>
<p>Example: In Overwatch there are 27 playable characters, of which 6 are considered “Tanks”. Suppose that three characters are drawn at random.</p>
<ol type="1">
<li><p>What is the probability that the selection contains exactly 2 tanks.</p>
<blockquote>
<p>We want <span class="math inline">\(P(T = 2) = \frac{{6 \choose 2}{21 \choose 1}}{27 \choose 3} \approx 0.1077\)</span>.</p>
</blockquote></li>
<li><p>Approximate this probability using binomial distribution.</p>
<blockquote>
<p><span class="math inline">\(T_{Bin} \sim Bin(3, \frac{6}{27})\)</span>.</p>
<p>We want <span class="math inline">\(P(T_{Bin} = 2) = {3 \choose 2}(\frac{6}{27})^2(\frac{21}{27}) \approx 0.1152\)</span>.</p>
</blockquote></li>
</ol>
<h2 id="negative-binomial">Negative Binomial</h2>
<p>Question: We want the number of tails until you get the first head. <span class="math display">\[P(X = x) = (1-p)^xp,\ x = 0,1...\]</span></p>
<p>Question: If I model the total number of coin flips until I get the first head, is this also a geometric distribution? <strong>Yes</strong> because it is essentially the same as the last question.</p>
<p>We generalize to <span class="math inline">\(k\)</span> successes by noticing that the last trial must produce the <span class="math inline">\(k\)</span>th success and the remaining <span class="math inline">\(k-1\)</span> successes may appear anywhere from the <span class="math inline">\(1\)</span>st to <span class="math inline">\(2\)</span>nd last trial. <span class="math display">\[{r + k - 1 \choose k - 1}p^k(1-p)^r\]</span></p>
<p>Example: There is a 50.4% change of flipping a head. What is the probability that you need more than 5 flips to get a tail?</p>
<blockquote>
<p><span class="math inline">\(1 - P(X \le 4) = 1 - \sum_{x = 0}^4 0.504^x (1 - 0.504)\)</span>.;</p>
</blockquote>
<p>Exampe: Hanzo is getting more popular. Every time I join a new game, I have a 15% change of picking Hanzo. What is the probability that …</p>
<blockquote>
<p>Let <span class="math inline">\(W\)</span> be the number of games played before I pick Hanzo. <span class="math inline">\(W \sim Geo(0.15)\)</span>. We have <span class="math inline">\(f_W(w) = (1-p)^wp\)</span>, <span class="math inline">\(w \in \mathbb{N}_0\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>I will need to play 4 games before I get to pick Hanzo?</p>
<blockquote>
<p><span class="math inline">\(P(W = 4) = f_w(4) = (0.85)^40.15 = 0.0783\)</span>.</p>
</blockquote></li>
<li><p>I will need to play at least 4 games before I get to pick Hanzo, given that I have to play at least 3 games before I pick him?</p>
<blockquote>
<p><span class="math inline">\((P(W \ge 4 | W \ge 3) = \frac{P(W \ge 4 \cap W \ge 3)}{P(W \ge 3)} = \frac{P(W \ge 4)}{P(W \ge 3)} = 0.85\)</span>.</p>
<p><span class="math display">\[\begin{aligned}P(W \ge w) &amp;= \sum_{t=w}^\infty (1-p)^tp \\ &amp;= p(1-p)^w\sum_{t=0}^\infty (1-p)^t \\ &amp;= p(1-p)^w\frac{1}{1 - (1-p)} \\ &amp;= (1-p)^w\end{aligned}\]</span></p>
</blockquote></li>
<li><p>I will need to play at least one game before I get to pick Hanzo?</p>
<blockquote>
<p><span class="math inline">\(P(W \ge 1) = 1 - P(W = 0) = 1 - p = 0.85\)</span>.</p>
</blockquote></li>
</ol>
<h2 id="memoryless-property-of-geometric">Memoryless Property of Geometric</h2>
<p>Let <span class="math inline">\(X \sim Geo(p)\)</span> and <span class="math inline">\(s, t\)</span> be non-negative integers. <span class="math display">\[P(X \ge s + t | X \ge s) = P(X \ge t)\]</span></p>
<p>Example: Mobile game “Show Me The Money”. Game released super rate item which only appears from a loot box which costs $2 per box, with the change of 0.01%.</p>
<blockquote>
<p>Let <span class="math inline">\(X\)</span> be the number of boxes without the rare item.</p>
</blockquote>
<ol type="1">
<li><p>What is the probability that he will need to buy 50 loot boxes to get 2 super rate items?</p>
<blockquote>
<p>Number of success is fixed but not the number of trials, so we use negative binomial. <span class="math inline">\(X \sim NB(2, 0.0001)\)</span>.</p>
<p>We want <span class="math inline">\(P(X = 48) = {48 + 2 - 1 \choose 2 - 1}(0.9999)^{48}(0.0001)^2 = 0.000000488\)</span>.</p>
</blockquote></li>
</ol>
<h1 id="poisson">Poisson</h1>
<p>We say that a random variable <span class="math inline">\(X \sim Poisson(\lambda)\)</span> if we have <span class="math inline">\(f_X(x)\)</span> such that. <span class="math display">\[f_X(x) = e^{-\lambda}\frac{\lambda^x}{x!}\]</span></p>
<p>We verify that this is a valid probability distribution using the exponential series. <span class="math display">\[\begin{aligned}
\sum_{x=0}^\infty f_X(x) &amp;= \sum_{x=0}^\infty e^{-\lambda}\frac{\lambda^x}{x!} \\
&amp;= e^{-\lambda}e^\lambda \\
&amp;= 1
\end{aligned}\]</span></p>
<p>One way to view poisson is to consider the limiting case of binomial, where you fix <span class="math inline">\(\lambda = np\)</span>, and let <span class="math inline">\(n \to \infty\)</span> and <span class="math inline">\(p \to 0\)</span>.</p>
<h2 id="poisson-process">Poisson Process</h2>
<ol type="1">
<li><p><strong>Independence</strong>: the number of occurences in non-overlapping intervals are independent.</p></li>
<li><p><strong>Individuality</strong>: for sufficiently short periods of time, <span class="math inline">\(\Delta t\)</span>, the probability of two or more events occuring in the interval approaches zero.</p></li>
</ol>
<p><span class="math display">\[\lim_{\Delta t \to 0} \frac{P(\text{2 of more events in } (t, t + \Delta t))}{\Delta t} = 0\]</span></p>
<ol start="3" type="1">
<li><strong>Homogeneity or Uniformity</strong>: events occur at a uniform or homogenous rate <span class="math inline">\(\lambda\)</span> and proportional to time interval <span class="math inline">\(\Delta t\)</span>.</li>
</ol>
<p><span class="math display">\[\lim_{\Delta t \to 0} \frac{P(\text{one event in } (t, t + \Delta t)) - \lambda \Delta t}{\Delta t} = 0\]</span></p>
<p>A process that satisfies the prior conditions on the occurrence of events is often called a <strong>Poisson Process</strong>. More precisely, if <span class="math inline">\(X_t\)</span>, for <span class="math inline">\(t \ge 0\)</span>, denotes the number of events that have occurred up to time <span class="math inline">\(t\)</span>, then <span class="math inline">\(X_t\)</span> is called a <strong>Poisson Process</strong>.</p>
<p>Example: Website hits for a given website occur according to a Poisson process with a rate of 100 hits per minute. Find …</p>
<ol type="1">
<li><span class="math inline">\(P(\text{1 hit is observed in a second}) = \frac{e^{-\frac{5}{3}}\frac{5}{3}^1}{1!}\)</span></li>
<li><span class="math inline">\(P(\text{90 hits are observed in a minute}) = \frac{e^{-100}100^{90}}{90!}\)</span></li>
</ol>
<h2 id="relation-to-binomial">Relation to Binomial</h2>
<p>Consider one unit of time, so that the process follows <span class="math inline">\(Poi(\lambda)\)</span>.</p>
<p>We chop up the interval into <span class="math inline">\(n\)</span> equally-sized pieces. If we chop it up finely enough, then by individuallity, the probability of two or more events occuring goes to 0.</p>
<p>This means that, over a small enough size, we approximately have either 0 or 1 event occuring with <span class="math inline">\(P(event) = p\)</span> for every piece.</p>
<p>Moreover, the event probability <span class="math inline">\(p\)</span> is proportional to the length of the interval by homogeneity. This means that as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(p \to 0\)</span>.</p>
<p>Finally, by independence, each <span class="math inline">\(n\)</span> pieces are independent, so we have <span class="math inline">\(Bin(n, p)\)</span>, where <span class="math inline">\(n\)</span> is very large and <span class="math inline">\(p\)</span> is very small.</p>
<p>We expect to see <span class="math inline">\(np\)</span> events, recall that rate <span class="math inline">\(\lambda\)</span> is the rate of occurance over 1 unit of time. So <span class="math inline">\(\lambda = np\)</span>, and as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(p \to 0\)</span>, in <span class="math inline">\(Bin(n, p)\)</span>, we approach <span class="math inline">\(Poi(np)\)</span>.</p>
<p>Example: A bit error occurs got a given data transmission method independently in out of of every 1000 bits transferred. Suppose a 64 bit message is sent using the transmission system.</p>
<ol type="1">
<li><p>What is the probability that there are exactly 2 bit errors?</p>
<blockquote>
<p><span class="math inline">\(X \sim Bin(64, \frac{1}{1000})\)</span>, <span class="math inline">\(P(X = 2) = {64 \choose 2}(\frac{999}{1000})^{62} (\frac{1}{1000})^2 \approx 0.019\)</span>.</p>
</blockquote></li>
<li><p>Approximate using Poisson.</p>
<blockquote>
<p><span class="math inline">\(X \sim Poi(\frac{64}{1000})\)</span>, <span class="math inline">\(P(X = 2) = \frac{e^{-\frac{64}{1000}}(\frac{64}{1000})^2}{2!} \approx 0.019\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: Shiny versions of Pokemon are possible to encounter and catch starting in Generation 2 (Pokemon Gold / Silver). Normal encounters with Pokemon while running in grass occur according to a Poisson process with rate 1 per minute on average. 1 in every 8192 encounters will be a Shiny Pokemon, on average.</p>
<ol type="1">
<li>If you run around in the grass for 15 hours, what is the probability you will encounter at least one Shiny Pokemon?</li>
</ol>
<blockquote>
<p><span class="math inline">\(X \sim Poi(\frac{900}{8192})\)</span>. <span class="math inline">\(P(X \ge 1) = 1 - P(X = 0) = 1 - e^{-\frac{900}{8192}}\)</span>.</p>
</blockquote>
<ol start="2" type="1">
<li>How long would you have to run around in grass to have better than 50 percent chance of encountering at least one Shiny Pokemon?</li>
</ol>
<blockquote>
<p>Solve for <span class="math inline">\(t\)</span>, where <span class="math inline">\(0.5 = P(X_t \ge 1)\)</span>.</p>
</blockquote>
<p>Example: An infinite number of Harolds are released in a gold mine. They scatter randomly, so that on average, a gold nugget is surrounded by 6 Harolds. Assume that all gold nuggets are of equal size.</p>
<blockquote>
<p>Let <span class="math inline">\(H\)</span> be the number of Harolds surrounding the nugget. <span class="math inline">\(H \sim Poi(\lambda = 6\ Harolds / nugget)\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>What is the probability that a nugget is surrounded by more than 3 Harolds?</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}P(H &gt; 3) &amp;= 1 - P(H &lt;= 3) \\ &amp;= 1 - f(0) - f(1) - f(2) - f(3) \\ &amp;=  0.8488 \\ f(x) &amp;= \frac{e^{-6}6^x}{x!}\end{aligned}\]</span></p>
</blockquote></li>
<li><p>When 10 nuggets are picked at random, what is the probability that 8 of those nuggets have more than 3 Harolds?</p>
<blockquote>
<p><span class="math inline">\(Bin(n=10, p=0.8488)\)</span>. <span class="math inline">\(P(N=8) = {10 \choose 8}(0.8488)^8(1-0.8488)^2\)</span>.</p>
</blockquote></li>
<li><p>On 2 nuggets there are <span class="math inline">\(t\)</span> Harolds in total. What is the probability that <span class="math inline">\(x\)</span> of them are on the first of the two nuggets?</p>
<blockquote>
<p>Let <span class="math inline">\(A\)</span> be the event that there are <span class="math inline">\(t\)</span> Harolds on 2 nuggets. Let <span class="math inline">\(B\)</span> be the event that there are <span class="math inline">\(x\)</span> Harolds on the first nugget. <span class="math inline">\(P(B|A) = \frac{P(A \cap B)}{P(A)}\)</span>. Where <span class="math inline">\(A \cap B\)</span> is the event where <span class="math inline">\(x\)</span> Harolds are on the first nugget, and <span class="math inline">\(t-x\)</span> Harolds are on nugget 2.</p>
<p>These events are independent, so <span class="math inline">\(P(A \cap B) = \frac{e^{-6}6^x}{x!} \cdot \frac{e^{-6}6^{t-x}}{(t-x)!} = \frac{e^{-12}6^t}{x!(t-x)!}\)</span>.</p>
<p>We can double the original rate, <span class="math inline">\(Poi(12)\)</span> for the denominator. So <span class="math inline">\(P(A) = \frac{e^{-12}12^t}{t!}\)</span>.</p>
<p>So <span class="math inline">\(P(B|A) = {t \choose x}\frac{1}{2}^t\)</span>.</p>
</blockquote></li>
</ol>
<p>If you order a set of random variables, they become <strong>order statistics</strong>.</p>
<p>Question: Let <span class="math inline">\(X_1, X_2, X_3\)</span> denote the random variables for the outcome of three independent fair random number generators. Assume that their ranges are <span class="math inline">\(\{1, 2, ..., 10\}\)</span>. Now let <span class="math inline">\(X_{max}\)</span> denote the maximum value, then <span class="math inline">\(P(X_{max} \le x) = P(X_1 \le x)P(X_2 \le x)P(X_3 \le 3)\)</span>.</p>
<h1 id="expectation">Expectation</h1>
<p><strong>Definition</strong>: Let <span class="math inline">\(x_1, x_2, ..., x_n\)</span> be outcomes of random variable <span class="math inline">\(X\)</span>. Its <strong>sample mean</strong> is defined as <span class="math inline">\(\overline{x} = \frac{\sum_{i=1}^nx_i}{n}\)</span>.</p>
<p>We can calculate a theoretical mean of <span class="math inline">\(X\)</span> directly if we know its distribution. Suppose <span class="math inline">\(X\)</span> is a discrete random variable with probability function <span class="math inline">\(f(x)\)</span>, then <span class="math inline">\(E(X)\)</span> is called the <strong>expected value</strong> of <span class="math inline">\(X\)</span> and is defined by <span class="math inline">\(E(X) = \sum_{x \in X(S)}xf(x)\)</span>. The expected value is sometimes referred to as the <strong>mean</strong>, <strong>expectation</strong>, or <strong>first moment</strong> of <span class="math inline">\(X\)</span>.</p>
<p>Example: A lottery is conducted in which 7 numbers are drawn without replacement between the numbers 1 and 50. A player wins the lottery if the numbers selected on their ticket match all 7 of the drawn numbers. A ticket to play the lottery costs $1, and the jackpot is valued at $5000000. Compute the expected return.</p>
<blockquote>
<p>If you win the lottery, the return is 4999999. If you did not win, the return is -1. Let <span class="math inline">\(R\)</span> denote the return amount. <span class="math inline">\(E(R) = (-1)P(\overline{w}) + (4999999)P(w) &lt; 0\)</span>.</p>
</blockquote>
<h2 id="law-of-the-unconscious-statistiation">“Law of the Unconscious Statistiation”</h2>
<p>If <span class="math inline">\(g: \mathbb{R} \to \mathbb{R}\)</span>, then for a random variable <span class="math inline">\(X\)</span> with probability function <span class="math inline">\(f(x)\)</span>, the expected value of <span class="math inline">\(g(x)\)</span> is given be <span class="math inline">\(\sum_{x \in S(X)}g(x)f(x)\)</span>.</p>
<p>To retrieve our original expectation function, we set <span class="math inline">\(g(x) = x\)</span>.</p>
<p>Example: If <span class="math inline">\(g(x) = x^2\)</span> and <span class="math inline">\(X\)</span> is the result of a fair six sided die roll, then compute <span class="math inline">\(E[g(X)]\)</span>.</p>
<blockquote>
<p><span class="math inline">\(E[g(x)] = E[x^2] = \sum_{x = 1}^6 x^2 \frac{1}{6}\)</span>.</p>
</blockquote>
<h2 id="linearity-of-expectation">Linearity of Expectation</h2>
<p>If <span class="math inline">\(g(x)\)</span> is a linear function <span class="math inline">\(g(x) = ax + b\)</span>, then for a random variable <span class="math inline">\(X\)</span>, <span class="math inline">\(E[aX + b] = aE[X] + b\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[aX + b] &amp;= \sum_{x \in X(S)}(ax + b)f(x) \\ &amp;= a\sum_{x \in X(S)}xf(x) + b\sum_{x \in X(S)} \\ &amp;= aE[x] + b\end{aligned}\]</span></p>
</blockquote>
<p>Note: It is <strong>not true</strong> in general that <span class="math inline">\(g(E[X]) = E[g(X)]\)</span>.</p>
<p>An extention of linearity is, <span class="math inline">\(E[af(X) + bg(X)] = aE[f(X)] + bE[g(X)]\)</span>.</p>
<h2 id="expectation-of-binomial">Expectation of Binomial</h2>
<blockquote>
<p>If <span class="math inline">\(X \sim Bin(n, p)\)</span> then <span class="math inline">\(E[X] = np\)</span>.</p>
</blockquote>
<p><span class="math display">\[\begin{aligned}
E[X] &amp;= \sum_{x = 0}^n xf(x) \\
&amp;= \sum_{x = 1}^n xf(x) \\
&amp;= \sum_{x = 1}^n x \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\
&amp;= \sum_{x=1}^n \frac{n(n-1)!}{(x-1)!((n-1) - (x-1))!}pp^{x-1}(1-p)^{(n-1)-(x-1)} \\
&amp;= np(1-p)^{n-1} \sum_{x=1}^n \left({n - 1 \choose x-1}p^{x-1}(1-p)^{-(x-1)} \right) \\
&amp;= np(1-p)^{n-1} \sum_{x=1}^n \left( {n-1\choose x-1} \left(\frac{p}{1 - p}\right)^{x-1} \right) \\
&amp;= np(1-p)^{n-1} \sum_{y=0}^{n-1} \left( {n-1\choose y} \left(\frac{p}{1 - p}\right)^{y} \right) \\
&amp;= np(1-p)^{n-1} \left(1+\frac{p}{1-p}\right)^{n-1} \\
&amp;= np(1-p)^{n-1} \left(\frac{1}{1-p}\right)^{n-1} \\
&amp;= np
\end{aligned}\]</span></p>
<p>Example: Suppose two fair six sided die are independently rolled 24 times, at let <span class="math inline">\(X\)</span> denote the number of times the sum of die rolls is 7. Compute <span class="math inline">\(E[X]\)</span>.</p>
<blockquote>
<p>36 outcomes, 6 yield a sum of 7, so <span class="math inline">\(p = \frac{1}{6}\)</span>. We are rolling 24 times, so <span class="math inline">\(n = 24\)</span>. We have <span class="math inline">\(E[X] = np = 4\)</span>.</p>
</blockquote>
<h2 id="expectation-of-poisson">Expectation of Poisson</h2>
<blockquote>
<p>If <span class="math inline">\(Y \sim Poi(\lambda)\)</span>, then <span class="math inline">\(E[Y] = \lambda\)</span>.</p>
</blockquote>
<p><span class="math display">\[\begin{aligned}
E[Y] &amp;= \sum_{y \ge 0}yf(y) \\
&amp;= \sum_{y \ge 1}y\frac{e^{-\lambda}\lambda^y}{y!} \\
&amp;= \sum_{y \ge 1}\frac{e^{-\lambda}\lambda \lambda^{y-1}}{(y-1)!} \\
&amp;= e^{-\lambda}\lambda \sum_{y \ge 1}\frac{\lambda^{y-1}}{(y-1)!} \\
&amp;= e^{-\lambda}\lambda \sum_{z \ge 0}\frac{\lambda^{z}}{z!} \\
&amp;= e^{-\lambda}\lambda e^{\lambda} \\
&amp;= \lambda
\end{aligned}\]</span></p>
<p>Example: Suppose that calls to Canadian Tire Financial call center follow a Poisson process with rate 30 calls per minute. Let <span class="math inline">\(X\)</span> denote the number of calls to the center after 1 hour. Compute <span class="math inline">\(E[X]\)</span>.</p>
<blockquote>
<p><span class="math inline">\(E[X] = 30 * 60\)</span>.</p>
</blockquote>
<h2 id="expectation-of-hypergeometric">Expectation of Hypergeometric</h2>
<blockquote>
<p>If <span class="math inline">\(X \sim hyp(N, r, n)\)</span>, then <span class="math inline">\(E[X] = n\frac{r}{N}\)</span>.</p>
</blockquote>
<h2 id="expectation-of-negative-binomial">Expectation of Negative Binomial</h2>
<blockquote>
<p>If <span class="math inline">\(Y \sim NB(k, p)\)</span>, then <span class="math inline">\(E[Y] = \frac{k(1-p)}{p}\)</span>.</p>
</blockquote>
<h1 id="variability">Variability</h1>
<p>Expectation along may not be enough. Oftentimes, we want to study how much a random variable tends to deviate from its mean.</p>
<ol type="1">
<li><strong>Deviation</strong>: <span class="math inline">\(E[X - \mu] = E[X] - \mu\)</span>.</li>
<li><strong>Absolute Deviation</strong>: <span class="math inline">\(E[|X - \mu|]\)</span>.</li>
<li><strong>Squared Deviation</strong>: <span class="math inline">\(E[(X - \mu)^2]\)</span>. Turns out to be a useful measure of variability.</li>
</ol>
<h2 id="variance">Variance</h2>
<p>The <strong>variance</strong> of a random variable <span class="math inline">\(X\)</span> is denoted <span class="math inline">\(Var(X)\)</span> and is defined by <span class="math inline">\(Var(X) = E[(X - E[X])^2]\)</span>. A simple calculation gives the <em>short cut formula</em>, <span class="math inline">\(Var(X) = E[X^2] = E[X]^2\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
E[(X - E[X])^2] &amp;= E[X^2 - 2XE[X] + E[X]^2] \\
&amp;= E[X^2] - 2E[X]^2 + E[X]^2 \\
&amp;= E[X^2] - E[X]^2
\end{aligned}\]</span></p>
<p>We know that <span class="math inline">\(E[(X - E[X])^2] \ge 0\)</span> so we can say that <span class="math inline">\(E[X^2] - E[X]^2 \ge 0\)</span>.</p>
<h3 id="variance-of-linear-combination">Variance of Linear Combination</h3>
<p>For any random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(a, b \in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[ Var(aX + b) = a^2Var(X)\]</span></p>
<p><strong>Proposition</strong>: <span class="math inline">\(Var(X = 0)\)</span> if and only if <span class="math inline">\(P(X = E[X]) = 1\)</span>.</p>
<p>Example: Let <span class="math inline">\(X\)</span> denote the outcome of a fair six sided die. Compute <span class="math inline">\(Var(X)\)</span>.</p>
<blockquote>
<p><strong>Recall</strong>: <span class="math inline">\(Var(X) = E[X^2] - E[X]^2\)</span>.<br />
We know <span class="math inline">\(X \in \{1, .., 6\}\)</span>, <span class="math inline">\(X^2 \in \{1, ..., 36\}\)</span>.</p>
<p><span class="math inline">\(E[X] = 3.5 = \frac{7}{2}\)</span>.<br />
<span class="math inline">\(E[X^2] = \sum x^2f(x) = \frac{1}{6}(1 + 4 + 9 + 16 + 25 + 36) = \frac{91}{6}\)</span>. So, <span class="math inline">\(Var(X) = \frac{91}{6} - \frac{49}{4}\)</span>.</p>
</blockquote>
<h2 id="standard-deviation">Standard Deviation</h2>
<blockquote>
<p><strong>Note</strong>: <span class="math inline">\(Var(X)\)</span> is a squared unit. To recover the original unit, we take the square root of variance.</p>
</blockquote>
<p>We define the <strong>standard deviation</strong> of a random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(SD(X)\)</span>, where <span class="math inline">\(SD(X) = \sqrt{Var(X)}\)</span>.</p>
<h2 id="variability-of-binomial">Variability of Binomial</h2>
<blockquote>
<p>Suppose that <span class="math inline">\(X \sim Bin(n, p)\)</span>. Then <span class="math inline">\(Var(X) = np(1-p)\)</span>.</p>
</blockquote>
<h2 id="variability-of-poisson">Variability of Poisson</h2>
<blockquote>
<p>Suppose that <span class="math inline">\(X \sim Poi(\lambda)\)</span>. Then <span class="math inline">\(Var(X) = \lambda\)</span>.</p>
</blockquote>
<p>Example: Suppose that <span class="math inline">\(X_n\)</span> is binomial with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p_n\)</span>, so that <span class="math inline">\(np_n \to \lambda\)</span> as <span class="math inline">\(n \to \infty\)</span>. If <span class="math inline">\(Y \sim Poi(\lambda)\)</span>, show that <span class="math inline">\(\lim_{n \to \infty} Var(X_n) = Var(Y)\)</span>.</p>
<h2 id="variability-of-hypergeometric">Variability of Hypergeometric</h2>
<blockquote>
<p>Suppose that <span class="math inline">\(X \sim hyp(N, r, n)\)</span>. Then <span class="math inline">\(Var(X) = n\frac{r}{N}(1 - \frac{r}{N})(\frac{N - n}{N - 1})\)</span>.</p>
</blockquote>
<h2 id="variability-of-hypergeometric-1">Variability of Hypergeometric</h2>
<blockquote>
<p>Suppose that <span class="math inline">\(Z \sim NB(k, p)\)</span>. Then <span class="math inline">\(Var(Z) = \frac{k(1-p)}{p^2}\)</span>.</p>
</blockquote>
<p>Example: Suppose <span class="math inline">\(X_n\)</span> is binomial with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p_n\)</span> so that <span class="math inline">\(np_n \to \lambda\)</span> as <span class="math inline">\(n \to \infty\)</span>. If <span class="math inline">\(Y \sim Poi(\lambda)\)</span>, show that <span class="math inline">\(\lim_{n \to \infty}Var(X_n) = Var(Y)\)</span>.</p>
<blockquote>
<p><strong>Recall</strong>: <span class="math inline">\(Var(X_n) = np_n(1-p_n)\)</span>. We also know that <span class="math inline">\(\lim_{n \to \infty} np_n = \lambda\)</span> and <span class="math inline">\(\lim_{n \to \infty}(1 - p_n) = 1\)</span>. We can use the limit law. <span class="math display">\[\begin{aligned}\lim_{n \to \infty} Var(X_n) &amp;= \lim_{n \to \infty}np_n \lim_{n \to \infty}(1-p_n) \\ &amp;= \lambda \\ &amp;= Var(Y)\end{aligned}\]</span></p>
</blockquote>
<h1 id="skewness">Skewness</h1>
<p><span class="math display">\[E\left[\left(\frac{X - E[X]}{SD(X)}\right)^3\right]\]</span></p>
<h1 id="kurtosis">Kurtosis</h1>
<p><span class="math display">\[E\left[\left(\frac{X - E[X]}{SD(X)}\right)^4\right]\]</span></p>
<p><strong>Remark</strong>: There exists distributions without expectation. Suppose <span class="math inline">\(X\)</span> is a random variable with <span class="math inline">\(f_X(x) = \frac{6}{(\pi x)^2}\)</span>. Then <span class="math inline">\(E(X) = \infty\)</span> and <span class="math inline">\(Var(X)\)</span> is not defined.</p>
<p>Example: Let <span class="math inline">\(X \sim Geo(p)\)</span>. Compute <span class="math inline">\(E[X]\)</span>.</p>
<blockquote>
<p><span class="math inline">\(f(x) = p(1-p)^x\)</span>, <span class="math inline">\(x = 0, 1, 2 , ...\)</span>. <span class="math display">\[\begin{aligned}E[X] &amp;= \sum_{x \ge 1} xp(1-p)^x \\ &amp;= p(1-p)\sum_{x \ge 1}x(1-p)^{x-1}\end{aligned}\]</span> <strong>Note</strong>: <span class="math inline">\(\frac{d}{d(1-p)} \frac{1}{1 - (1-p)} = \sum_{x \ge 1} x(1-p)^{x-1}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}E[X] &amp;= p(1-p) \frac{d}{d(1-p)} \frac{1}{1 - (1-p)} \\ &amp;= p(1-p)\frac{1}{p^2} \\ &amp;= \frac{1-p}{p}\end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(f(x) = p(1-p)^x\)</span>, <span class="math inline">\(x = 0,1,2...\)</span>. <span class="math display">\[\begin{aligned}\sum_{x \ge 1}P(X \ge x) &amp;= \sum_{x \ge 1}(1-P(X \le x-1)) \\ &amp;= \sum_{x \ge 1}(1-(1-(1-p)^x)) \\ &amp;= \frac{1-p}{1 - (1-p)} \\ &amp;= \frac{1-p}{p}\end{aligned}\]</span></p>
</blockquote>
<p><strong>Theorem</strong>: Darth Vader Rule. Let <span class="math inline">\(X\)</span> be a non-negative discrete random variable.</p>
<p><span class="math display">\[E[X] = \sum_{x = 1}^\infty P(X \ge x) = \sum_{x \ge 0}P(X &gt; x)\]</span></p>
<p>Example: A person plays a game in which a fair coin is tossed until the first tail occurs. The person wins <span class="math inline">\(\$2^x\)</span> if <span class="math inline">\(x\)</span> tosses are needed for <span class="math inline">\(x = 1,2,3,4,5\)</span> but loses <span class="math inline">\(\$256\)</span> if <span class="math inline">\(x &gt; 5\)</span>.</p>
<ol type="1">
<li><p>Determine the expected winnings.</p>
<blockquote>
<p>Let <span class="math inline">\(W\)</span> be the amount of winnings. <span class="math inline">\(W = \begin{cases}2^x, &amp;X = 1, 2,...5 \\ -256, &amp;x &gt; 5\end{cases}\)</span>. <span class="math inline">\(W\)</span> is a function of <span class="math inline">\(X \sim Geo(p=\frac{1}{2})\)</span>. By law of unconcious statistician. <span class="math display">\[\begin{aligned}E[W] &amp;= \sum_{x = 1}^5 2^x P(X = x) - 256P(X &gt; 5) \\ &amp;= 2p + 2^2p(1-p) + ... 2^5p(1-p)^4 - 256(1-p)^5 \\ &amp;= 1 + 1 + 1 + 1 + 1 - \frac{2^8}{2^5} \\ &amp;= 5 - 8 \\ &amp;= -3\end{aligned}\]</span></p>
</blockquote></li>
<li><p>Determine the variance of the winnings.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}Var(W) &amp;= E[W^2] - E[W]^2 \\ &amp;= 2101 \text{ Dollars}^2\end{aligned}\]</span></p>
</blockquote></li>
</ol>
<p>Example: Yasmin and Zack are BMath students taking the exact same courses. Let <span class="math inline">\(X\)</span> be the number of assignments that they have in one week. The probability function of <span class="math inline">\(X\)</span> is.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(f(x)\)</span></td>
<td>0.09</td>
<td>0.1</td>
<td>0.25</td>
<td>0.4</td>
<td>0.15</td>
<td>0.01</td>
</tr>
</tbody>
</table>
<p>Yasmin drinks <span class="math inline">\(2X^2\)</span> cups per week and Zack drinks <span class="math inline">\(|2X - 1|\)</span> cups per week.</p>
<ol type="1">
<li><p>Compute the expected number of cups that Yasmin and Zack individually drink in a week.</p>
<blockquote>
<p><span class="math inline">\(E[2X^2] = \sum_{x = 0}^5 2x^2 P(X = x)\)</span></p>
<p><span class="math inline">\(E[|2X-1|] = \sum_{x = 0}^5 |2x - 1|P(X = x)\)</span>.</p>
</blockquote></li>
<li><p>Compute the variance individually.</p></li>
</ol>
<h1 id="continuous-random-variables">Continuous Random Variables</h1>
<h2 id="expectation-1">Expectation</h2>
<p>If <span class="math inline">\(X\)</span> is a continuous random variable with pdf <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g: \mathbb{R} \to \mathbb{R}\)</span>.</p>
<p><span class="math display">\[E[g(X)] = \int_{-\infty}^\infty g(x)f(x)dx\]</span></p>
<p><span class="math display">\[\begin{aligned}Var(X) &amp;= E[(X - E[X])^2] \\ &amp;= \int_{-\infty}^\infty(x-E[X])^2 f(x)dx \\ &amp;= E[X^2] - E[X]^2\end{aligned}\]</span></p>
<p>Example: <span class="math inline">\(X\)</span> has pdf <span class="math inline">\(f(x) = \begin{cases}6x(1-x), &amp;0 \le x \le 1 \\ 0, &amp;\text{otherwise}\end{cases}\)</span>. Compute <span class="math inline">\(E[X]\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[X] &amp;= \int_0^1 xf(x)dx \\ &amp;=\int_0^1 x(6x-1)dx \\ &amp;= 6\int_0^1 x^2 - x^3 dx \\ &amp;= 6\left[\frac{x^3}{3} - \frac{x^4}{4}\right]_0^1 \\ &amp;= 6\left(\frac{1}{3} - \frac{1}{4}\right) \\ &amp;= \frac{1}{2} \end{aligned}\]</span></p>
</blockquote>
<p>Question: Suppose <span class="math inline">\(X\)</span> has pdf <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f\)</span> is an even function around the origin on <span class="math inline">\(\mathbb{R}\)</span>. If <span class="math inline">\(E[X]\)</span> is well defined, what can we say about it?</p>
<blockquote>
<p><span class="math inline">\(E[X] = 0\)</span>.</p>
</blockquote>
<p>Example: Suppose <span class="math inline">\(X\)</span> has CDF <span class="math inline">\(F(x) = \begin{cases}0, &amp;x&lt;0\\\frac{x^2}{2}, &amp;0\le x&lt;\frac{1}{2} \\ \frac{7x}{4} - \frac{3}{4}, &amp;\frac{1}{2} \le x &lt; 1 \\ 1, &amp;x \ge 1 \end{cases}\)</span>. Compute <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(Var(X)\)</span>.</p>
<blockquote>
<p>We are given <strong>cdf</strong>, but <span class="math inline">\(f(x) = \frac{d}{dx} F(x)\)</span>, so we can obtain the pdf.</p>
<p><span class="math inline">\(f(x) = \begin{cases}0, &amp;x &lt; 0\\x, &amp;0\le x &lt; \frac{1}{2} \\ \frac{7}{4} &amp;\frac{1}{2} \le x &lt; 1 \\ 0, &amp;x \ge 1\end{cases}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}E[X] &amp;= \int_0^1 xf(x)dx \\ &amp;= \int_0^\frac{1}{2}x^2dx + \int_\frac{1}{2}^1 \frac{7x}{4}dx \\ &amp;= \left(\frac{x^3}{3}\right)_0^\frac{1}{2} + \left(\frac{7x^2}{8}\right)_\frac{1}{2}^1 \\ &amp;= 0.6979\end{aligned}\]</span></p>
</blockquote>
<p>If we have a function <span class="math inline">\(g\)</span> which has an inverse over the range of <span class="math inline">\(X\)</span>, then we have a fairly easy way of obtaining <span class="math inline">\(Y = g(X)\)</span>. In short, the method is as follows.</p>
<ol type="1">
<li>Write the CDF of <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(X\)</span>.</li>
<li>Use the CDF of <span class="math inline">\(X\)</span> to find the CDF of <span class="math inline">\(Y\)</span>. If you want the pdf, take derivatives.</li>
<li>Find the range of <span class="math inline">\(Y\)</span>.</li>
</ol>
<p>Example: Let <span class="math inline">\(X\)</span> be a continous random variable with the following pdf and cdf.</p>
<p><span class="math display">\[f(x) = \begin{cases}\frac{1}{4}, &amp;0 &lt; x \le 4\\ 0, &amp;\text{otherwise}\end{cases}\]</span></p>
<p><span class="math display">\[F(x) = \begin{cases}1, &amp;x \le 0\\ \frac{x}{4} &amp;0&lt;x&lt; 4\\ 1, &amp;x \ge 4\end{cases}\]</span></p>
<p>Find the pdf of <span class="math inline">\(Y = X^{-1}\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}P(Y \le y) &amp;= P(\frac{1}{X} \le y) \\ &amp;= P(\frac{1}{y} \le X) \\ &amp;= 1 - P(X \le \frac{1}{y} \\&amp;= 1 - F_x\left(\frac{1}{y}\right) \\ &amp;= -f_x\left(\frac{1}{y}\right) \frac{d}{dy} \cdot \frac{1}{y} \\ &amp;= -\frac{1}{4} \cdot \frac{-1}{y^2}, \text{ when } 0 &lt; \frac{1}{y} \le 4 \\ &amp;= \frac{1}{4y}, \text {when } \frac{1}{4} \le y &lt; \infty\end{aligned}\]</span></p>
</blockquote>
<h2 id="continuous-uniform-distribution">Continuous Uniform Distribution</h2>
<p>We say that <span class="math inline">\(X\)</span> has a <strong>continuous uniform distribution</strong> on interval <span class="math inline">\((a, b)\)</span> if <span class="math inline">\(X\)</span> has pdf <span class="math inline">\(f(x) = \begin{cases}\frac{1}{b-a}, &amp;x \in (a, b)\\ 0, &amp;\text{otherwise}\end{cases}\)</span>.</p>
<p>This is abbreviated <span class="math inline">\(X \sim U(a, b)\)</span>.</p>
<p>Example: Let $X U(a, b). Show that <span class="math inline">\(E[X] = \frac{a+b}{2}\)</span> and the <span class="math inline">\(V(X) = \frac{(b-a)^2}{12}\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[X] &amp;= \int_a^b x\frac{1}{b-a}dx \\ &amp;= \frac{1}{b-a} \int_a^b xdx \\ &amp;= \frac{1}{b-a}\left[\frac{x^2}{2}\right]_a^b \\ &amp;= \frac{b^2 - a^2}{2(b - a)} \\ &amp;= \frac{a+b}{2}\end{aligned}\]</span></p>
</blockquote>
<h2 id="exponential-distribution">Exponential Distribution</h2>
<p>We say that <span class="math inline">\(X\)</span> has an exponential distribution with paramter <span class="math inline">\(\lambda\)</span> (<span class="math inline">\(X \sim \exp(\lambda)\)</span>) with <span class="math inline">\(f(x)\)</span>.</p>
<p><span class="math display">\[f(x) = \begin{cases}\lambda e^{-\lambda x}, &amp;x &gt; 0 \\ 0, &amp;x \le 0\end{cases}\]</span></p>
<p>Consider the CDF of <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
F(x) &amp;= P(X \le x) \\
&amp;= 1 - P(\text{no occurances between }(0, x)) \\
&amp;= 1 - e^{-\lambda x} \\
\end{aligned}\]</span></p>
<p>We can then the derivative to obtain the pdf.</p>
<p><span class="math display">\[f(x) = \frac{d}{dx}F(x) = \lambda e^{-\lambda x}\]</span></p>
<p>We can use a <span class="math inline">\(\theta\)</span> parameterization of exponetial distribution (where <span class="math inline">\(\lambda = \frac{1}{\theta})\)</span> where <span class="math inline">\(\theta\)</span> can represent the expected waiting time.</p>
<p><span class="math display">\[f(x) = \frac{1}{\theta}e^{-\frac{x}{\theta}}\]</span></p>
<p><strong>Theorem</strong>: If <span class="math inline">\(X\)</span> is the time to the first event of Poisson process with paramter <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(X \sim \exp(\frac{1}{\lambda})\)</span>.</p>
<p>Example: Let <span class="math inline">\(X \sim \exp(\theta)\)</span>. Find <span class="math inline">\(E[X]\)</span>.</p>
<blockquote>
<p>By definition, <span class="math inline">\(\int_0^\infty x \frac{1}{\theta} e^{-\frac{x}{\theta}} dx\)</span>.</p>
</blockquote>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[X] &amp;= \int_0^\infty x \frac{1}{\theta} e^{-\frac{x}{\theta}} dx \\ &amp;= \left(x \frac{1}{\theta} e^{-\frac{x}{\theta}}(-\theta)\right)_0^\infty - \int_0^\infty \frac{1}{\theta}e^{-\frac{x}{\theta}}(-\theta) dx \\ &amp;= 0 + \theta \\ &amp;= \theta \end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[X^2] &amp;= \left(x^2 \frac{1}{\theta} e^{-\frac{x}{\theta}} (-\theta)\right)_0^\infty \int_0^\infty 2x e^{-\frac{x}{\theta}} dx \\ &amp;= 0 + \left(2xe^{-\frac{x}{\theta}}(-\theta)\right)_0^\infty + \int_0^\infty 2\theta e^{-\frac{x}{\theta}}dx \\ &amp;= 0 + 0 + 2\theta^2 \\ &amp;= 2\theta^2 \end{aligned}\]</span> <span class="math inline">\(Var(X) = E[X^2] - E[X]^2 = \theta^2\)</span>.</p>
</blockquote>
<h2 id="gamma-function">Gamma Function</h2>
<p><span class="math display">\[\Gamma(\alpha) = \int_0^\infty y^{\alpha-1}e^{-y}dy,\ \alpha &gt; 0\]</span></p>
<ul>
<li><span class="math inline">\(\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)\)</span>, for <span class="math inline">\(\alpha &gt; 1\)</span>.</li>
<li><span class="math inline">\(\Gamma(\alpha) = (\alpha - 1)!\)</span>, for <span class="math inline">\(\alpha \in \mathbb{N}\)</span></li>
<li><span class="math inline">\(\Gamma(\frac{1}{2}) = \sqrt \pi\)</span>.</li>
</ul>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[X] &amp;= \int_0^\infty x \frac{1}{\theta} e^{-\frac{x}{\theta}}dx \\ &amp;= \int_0^\infty ye^{-y}\theta dy \\ &amp;= \theta \int_0^\infty y^{2 - 1}e^{-y} dy \\ &amp;= \theta \Gamma(2) \\ &amp;= \theta(2 - 1)! \\ &amp;= \theta\end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[X^2] &amp;= \int_0^\infty x^2 \frac{1}{\theta} e^{-\frac{x}{\theta}}dx \\ &amp;= \int_0^\infty x \frac{x}{\theta} e^{-\frac{x}{\theta}} dx \\ &amp;= \int_0^\infty \theta y^2 e^{-y} \\ &amp;= \theta^2 \int_0^\infty y^{3-1} e^{-y} \\ &amp;= \theta^2 \Gamma(3) \\ &amp;= \theta^2 2! \\ &amp;= 2\theta^2 \end{aligned}\]</span> <span class="math inline">\(Var(X) = E[X^2] - E[X]^2 = \theta^2\)</span>.</p>
</blockquote>
<p>Example: Busses arrive according to Poisson process with an average of 3 busses per hour.</p>
<blockquote>
<p>Let <span class="math inline">\(T\)</span> be the waiting time for the first bus. <span class="math inline">\(T \sim \exp(\theta = \frac{1}{3})\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>Find the probability of waiting at least 15 minutes.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}P(T &gt; \frac{1}{4} \text{hour}) &amp;= 1 - P(T \le \frac{1}{4} \text{hour}) \\ &amp;= 1 - F_t(\frac{1}{4}) \\ &amp;= 1 - (1 - e^{-\frac{\frac{1}{4}}{\frac{1}{3}}}) \\&amp;= e^{-\frac{3}{4}}\end{aligned}\]</span></p>
</blockquote></li>
<li><p>Find the probability of waiting at least another 15 minutes given that you have already been waiting for 6 minutes.</p>
<blockquote>
$$
</blockquote></li>
</ol>
<h2 id="memoryless-exponential">Memoryless Exponential</h2>
<blockquote>
<p>If <span class="math inline">\(X \sim \exp(\theta)\)</span>, we have <span class="math inline">\(P(X &gt; s + t | X &gt; s) = P(X &gt; t)\)</span>.</p>
</blockquote>
<ul>
<li>If a continuous random variable has memoryless property, it must follow exponential distribution.</li>
</ul>
<h1 id="normal">Normal</h1>
<p>Example:</p>
<ol type="1">
<li><p>75th percentile of the standard normal distribution.</p>
<blockquote>
<p>We want <span class="math inline">\(x\)</span> such that <span class="math inline">\(P(Z \le x) = 0.75\)</span>. In the <span class="math inline">\(Z\)</span> table, <span class="math inline">\(P(Z \le 0.67) = 0.74857\)</span> and <span class="math inline">\(P(Z \le 0.68) = 0.75175\)</span>, so we know that <span class="math inline">\(x \in [0.67, 0.68]\)</span>.</p>
</blockquote></li>
<li><p>58th percentile of the <span class="math inline">\(N(5, 9)\)</span> distribution.</p>
<blockquote>
<p>Again, we want <span class="math inline">\(x\)</span> such that <span class="math inline">\(P(X \le x) = 0.58\)</span>. So <span class="math inline">\(P(Z \le \frac{x - 5}{3}) = 0.58\)</span>. We have <span class="math inline">\(\frac{x - 5}{3} \in (0.2, 0.21)\)</span>. So <span class="math inline">\(x \in [5.6, 5.63]\)</span>.</p>
</blockquote></li>
<li><p>Let <span class="math inline">\(Z \sim N(0, 1)\)</span>. Find <span class="math inline">\(c\)</span> such that <span class="math inline">\(P(-c \le Z \le c) = 0.95\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}P(-c \le Z \le c) &amp;= P(Z \le c) - P(Z \le -c) \\ &amp;= 2P(Z \le c) - 1 \\ &amp;= 0.95\end{aligned}\]</span> So <span class="math inline">\(c = 1.96\)</span>.</p>
</blockquote></li>
</ol>
<p>An interesting empirical rule about normal distributino is the <strong>68-95-99.7</strong> rule, which states the probability of <span class="math inline">\(P(\mu - \alpha \sigma \le X \le \mu + \alpha \sigma)\)</span>, for <span class="math inline">\(\alpha \in \{1, 2, 3\}\)</span> respectively.</p>
<h1 id="inverse-transform-method">Inverse Transform Method</h1>
<p><strong>Theorem</strong>: Let <span class="math inline">\(U \sim U(0, 1)\)</span>, and <span class="math inline">\(X\)</span> be a continuous random variable with cdf <span class="math inline">\(F\)</span>. Then <span class="math inline">\(F^{-1}(U)\)</span> the same distribution as <span class="math inline">\(X\)</span>.</p>
<h1 id="multivariate-distributions">Multivariate Distributions</h1>
<p><strong>Definition</strong>: Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables defined on the same sample space. The <strong>joint probability function</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is.</p>
<p><span class="math display">\[f(x, y) = P(\{X = x\} \cap \{Y = y\}), x \in X(S), y \in Y(S)\]</span></p>
<p>A shorthand for this is.</p>
<p><span class="math display">\[f(x, y) = P(X = x, Y = y)\]</span></p>
<p>Example: Two fair six sided die are rolled. Let <span class="math inline">\(X\)</span> denote the outcome of the first roll, and let <span class="math inline">\(Y\)</span> denote the outcome of the second die roll. Compute the joint probability of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<blockquote>
<p><span class="math inline">\(X, Y\)</span> are independent, so <span class="math inline">\(f(x,y) = \frac{1}{36}\)</span> for all $(x, y) in the same space.</p>
</blockquote>
<h2 id="properties-of-joint-probability-function">Properties of Joint Probability Function</h2>
<ol type="1">
<li><span class="math inline">\(f(x, y) \ge 0\)</span>.</li>
<li><span class="math inline">\(\sum_{x, y}f(x, y) = 1\)</span>.</li>
</ol>
<p>Example: Suppose a fair coin is tossed 3 times. Define the random variables <span class="math inline">\(X\)</span> as the number of heads, and <span class="math inline">\(Y\)</span> as whether a head occured on the first toss. Find the joint probability function for <span class="math inline">\((X, Y)\)</span>.</p>
<h2 id="marginal-probability-function">Marginal Probability Function</h2>
<p>Suppose that <span class="math inline">\(X, Y\)</span> are discrete random variables. The <strong>marginal probability function</strong> of <span class="math inline">\(X\)</span> is.</p>
<p><span class="math display">\[f_X(x) = P(X = x) = \sum_{y \in Y(S)} f(x, y)\]</span></p>
<p>Example: Suppose <span class="math inline">\(X, Y\)</span> has joint probability function.</p>
<p><span class="math display">\[f(x, y) = \frac{1}{6}\left(\frac{1}{2}\right)^x \left(\frac{2}{3}\right)^y\]</span></p>
<ol type="1">
<li><p>Compute the marginal probability functions <span class="math inline">\(f_X(x)\)</span> and <span class="math inline">\(f_Y(y)\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}f_X(x) &amp;= \sum_{y}f(x,y) \\ &amp;= \sum_{y = 0}^\infty \frac{1}{6}\frac{1}{2^x} \sum_{y = 0}^\infty \left(\frac{2}{3}\right)^y \\ &amp;= \frac{1}{6} \frac{1}{2^x} \frac{1}{1 - \frac{2}{3}} \\ &amp;= \frac{1}{2^{x+1}} \end{aligned}\]</span></p>
</blockquote></li>
<li><p>Compute <span class="math inline">\(P(X &lt; Y)\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}P(X &lt; Y) &amp;= \sum_{x = 0}^\infty \left(\sum_{y = x + 1}^\infty f(x, y)\right) \\ &amp;= \sum_{x = 0}^\infty \frac{1}{6} \frac{1}{2^x} \sum_{y = x + 1}^\infty \left(\frac{2}{3}\right)^y \\ &amp;= \sum_{x = 0}^\infty 3\cdot \frac{1}{6} \frac{1}{2^{x}} \left(\frac{2}{3}\right)^{x + 1} \end{aligned}\]</span></p>
</blockquote></li>
</ol>
<h2 id="conditional-probability-function">Conditional Probability Function</h2>
<p>The <strong>conditional probability function</strong> of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span> is denoted <span class="math inline">\(f_X(x|y)\)</span> and is defined to be.</p>
<p><span class="math display">\[f_X(x|y)= P(X = x | Y = y) = \frac{P(X = x, Y = y)}{P(Y = y)} = \frac{f(x,y)}{f_Y(y)}\]</span></p>
<p><strong>Proposition</strong>: If <span class="math inline">\(X \sim Poi(\lambda_1)\)</span> and <span class="math inline">\(Y \sim Poi(\lambda_2)\)</span>, then <span class="math inline">\(T = X + Y \sim Poi(\lambda_1 + \lambda_2)\)</span>.</p>
<p><strong>Proposition</strong>: If <span class="math inline">\(X \sim Bin(n, p)\)</span> and <span class="math inline">\(Y \sim Bin(m, p)\)</span> independently, then <span class="math inline">\(T = X + Y \sim Bin(n + m, p)\)</span>.</p>
<h2 id="multinomial-distribution">Multinomial Distribution</h2>
<p>If <span class="math inline">\((X_1, X_2, .., X_k) \sim Mult(n, p_1, ..., p_k)\)</span>, then <span class="math inline">\(X_j \sim Bin(n, p_j)\)</span>. Also, <span class="math inline">\(X_i + X_j \sim Bin(n, p_i + p_j)\)</span>.</p>
<ul>
<li>In multinomial distribution, the trials are independent, but the marginal random variables are not! That is because <span class="math inline">\(x_1 + ... + x_k = n\)</span>.</li>
</ul>
<p>Suppose <span class="math inline">\(X, Y\)</span> are discrete random variables with joint probability function <span class="math inline">\(f(x,y)\)</span>, Then for a function <span class="math inline">\(g: \mathbb{R}^2 \to \mathbb{R}\)</span> then <span class="math inline">\(E[g(X, Y)] = \sum_{(x, y)}g(x,y)f(x,y)\)</span>. This generalizes to multiple variables.</p>
<p>Linearity of expectation carries through as well.</p>
<ol type="1">
<li><span class="math inline">\(E[ag_1(X,Y) + bg_2(X,Y)] = aE[g_1(X, Y)] + bE[g_2(X, Y)]\)</span>.</li>
<li><span class="math inline">\(E[X + Y] = E[X] + E[Y]\)</span>.</li>
</ol>
<p>Example: Let <span class="math inline">\((X_1, X_2, X_3) \sim Mult(n, p_1, p_2, p_3)\)</span>. Show that <span class="math inline">\(E[X_1X_2] = n(n-1)p_1p_2\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[X_1X_2] &amp;= \frac{1}{2}E[2X_1X_2] \\ &amp;= \frac{1}{2}E[(X_1 + X_2)^2 - X_1^2 - X_2^2] \\ &amp;= \frac{1}{2}(E[(X_1 + X_2)^2] - E[X_1^2] - E[X_2^2]) \\ &amp;= \frac{1}{2}(Var(X_1 + X_2) + E[X_1 + X_2]^2 - Var(X_1) - E[X_1]^2 - Var(X_2) - E[X_2]^2) \\ &amp;= \frac{1}{2}(n(p_1 + p_2)(1 - p_1 - p_2) + (n(p_1+p_2))^2 - np_1(1-p_1) - (np_1)^2 - np_2(1-p_2) - (np_2)^2) \\ &amp;= n(n-1)p_1p_2\end{aligned}\]</span></p>
</blockquote>
<h1 id="covariance">Covariance</h1>
<p>If <span class="math inline">\(X, Y\)</span> are joint distribution, then <span class="math inline">\(Cov(X, Y)\)</span> denotes the <strong>covariance</strong> between <span class="math inline">\(X, Y\)</span>.</p>
<p><span class="math display">\[Cov(X, Y) = E[(X - E[X])(Y - E[Y])]\]</span></p>
<p>Shortcut formula.</p>
<p><span class="math display">\[Cov(X, Y) = E[X]E[Y] - E[XY]\]</span></p>
<p><strong>Theorem</strong>: If <span class="math inline">\(X,Y\)</span> are independent, then <span class="math inline">\(Cov(X, Y) = 0\)</span>. The converse is <strong>false</strong> with counter example <span class="math inline">\(X \sim N(0,1)\)</span>, <span class="math inline">\(Y \sim X^2 - 1\)</span>.</p>
<h2 id="correlation">Correlation</h2>
<p>The <strong>correlation</strong> of <span class="math inline">\(X, Y\)</span> is denoted <span class="math inline">\(corr(X, Y)\)</span>.</p>
<p><span class="math display">\[corr(X, Y) = \rho = \frac{Cov(X, Y)}{SD(X)SD(Y)}\]</span></p>
<p>It follows from the Cauchy-Schawrz inequality that <span class="math inline">\(-1 \le corr(X, Y) \le 1\)</span> and if <span class="math inline">\(|corr(X, Y)| = 1\)</span>, <span class="math inline">\(X = aY + b\)</span>.</p>
<p>We say that <span class="math inline">\(X, Y\)</span> are uncorrelated if <span class="math inline">\(Cov(X, Y) = 0\)</span>.</p>
<p><strong>Remark</strong>: If <span class="math inline">\(X, Y\)</span> are independent, then <span class="math inline">\(X, Y\)</span> are uncorrelated.</p>
<p><strong>Remark</strong>: <span class="math inline">\(Cov(X, X) = Var(X)\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(\rho = corr(X, Y)\)</span> has the same sign as <span class="math inline">\(Cov(X, Y)\)</span>.</li>
<li><span class="math inline">\(-1 \le \rho \le 1\)</span>.</li>
<li><span class="math inline">\(|\rho| = 1 \Rightarrow X = aY + b\)</span>.</li>
<li><span class="math inline">\(X, Y\)</span> independent means that <span class="math inline">\(corr(X, Y) = 0\)</span>.</li>
<li><span class="math inline">\(corr(X, X) = \frac{Cov(X, X)}{SD(X)^2} = \frac{Var(X)}{Var(X)} = 1\)</span>.</li>
</ol>
<h1 id="linear-combinations">Linear Combinations</h1>
<p>Suppose <span class="math inline">\(X_1, ..., X_n\)</span> are jointly distributed RVs with joint probability function <span class="math inline">\(f(x_1, ..., x_n)\)</span>. A <strong>linear combination</strong> of the RVs is any random variable of the form <span class="math inline">\(\sum_{i = 1}^n a_i X_i\)</span>. Where <span class="math inline">\(a_i \in R\)</span>.</p>
<p><span class="math display">\[E\left(\sum_{i = 1}^n a_i X_i\right) = \sum_{i = 1}^n a_iE(X_i)\]</span></p>
<p>Example: Let <span class="math inline">\(P_1, ..., P_7\)</span> represent number of cans of pop that Harold drinks each day. If each random variable has mean <span class="math inline">\(\mu = 6\)</span>, what is the expected number of cans consumed during those 7 days?</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[\overline{P}] &amp;= E\left[\frac{1}{7}\sum_{i=1}^7 P_i\right] \\ &amp;= \frac{1}{7}\sum_{i = 1}^7 6 \\ &amp;= 6\end{aligned}\]</span></p>
</blockquote>
<p>Example: Suppose <span class="math inline">\(X \sim N(1, 1)\)</span>, <span class="math inline">\(Y \sim U(0, 1)\)</span>. Compute <span class="math inline">\(E(2X - 4Y)\)</span>.</p>
<blockquote>
<p><span class="math inline">\(E(2X - 4Y) = 0\)</span>.</p>
</blockquote>
<p><strong>Proposition</strong>: Let <span class="math inline">\(X, Y, U, V\)</span> be random variables and <span class="math inline">\(a, b, c, d \in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[Cov(aX + bY, cU + dV) = acCov(X, U) + adCov(X, V) + bcCov(Y, U) + bdCov(Y, V)\]</span></p>
<p><strong>Proposition</strong>: Let <span class="math inline">\(X, Y\)</span> be random variables and <span class="math inline">\(a, b \in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X, Y)\]</span></p>
<p>In general,</p>
<p><span class="math display">\[Var(\sum_{i = 1}^n a_iX_i) = \sum_{i = 1}^n a_i^2 Var(X_i) + 2\sum_{1 \le i &lt; j \le n}a_ia_jCov(X_i, X_j)\]</span></p>
<p>If <span class="math inline">\(X, Y\)</span> are independent, then <span class="math inline">\(Var(aX + bY) = a^2Var(X) + b^2Var(Y)\)</span>.</p>
<p><strong>Proposition</strong>: A linear function of normal is normal. Let <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> and <span class="math inline">\(Y = aX + b\)</span>, <span class="math inline">\(a, b \in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[Y \sim N(a\mu + b, a^2 \sigma^2)\]</span></p>
<p>Let <span class="math inline">\(X_i \sim N(\mu_i, \sigma_i^2)\)</span> <strong>independently</strong>.</p>
<p><span class="math display">\[\sum_{i = 1}^n a_iX_i + b_i \sim N\left(\sum_{i = 1}^n a_i\mu_i + b_i, \sum_{i = 1}^n a_i^2\sigma_i^2\right)\]</span></p>
<p><strong>Proposition</strong>: Sample mean of normal is normal. If all <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span> <strong>independently</strong>.</p>
<p><span class="math display">\[\sum_{i = 1}X_i \sim N(n\mu, n\sigma^2)\]</span></p>
<p><span class="math display">\[\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i \sim N\left(\mu, \frac{\sigma^2}{n}\right)\]</span></p>
<p>Example: Compute <span class="math inline">\(Var(\sum_{i=1}^n X_i)\)</span> and <span class="math inline">\(Var(\overline{X})\)</span>.</p>
<blockquote>
<p><span class="math inline">\(\begin{aligned}Var(\sum_{i = 1}^n X_i) &amp;= \sum_{i = 1}Var(X_i) \\ &amp;= n\sigma^2\end{aligned}\)</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\begin{aligned}Var(\overline{X}) &amp;= \frac{1}{n^2}Var(\sum_{i = 1}^n X_i) \\ &amp;= \frac{\sigma}{n}\end{aligned}\)</span></p>
</blockquote>
<h1 id="indicator-variables">Indicator Variables</h1>
<p><span class="math display">\[E[\mathbb{I}_A] = P(A)\]</span></p>
<p><span class="math display">\[E[\mathbb{I}_A^2] = P(A)\]</span></p>
<p>So,</p>
<p><span class="math display">\[Var(\mathbb{I}_A) = P(A)(1 - P(A))\]</span></p>
<p>Example: <span class="math inline">\(N\)</span> letters to <span class="math inline">\(N\)</span> different people, there are <span class="math inline">\(N\)</span> envelopes. One letter is put in each envelope at random. Find the mean and variance of the number of letters placed in the right envelope.</p>
<blockquote>
<p>Let <span class="math inline">\(\mathbb{I}_i = \begin{cases}1, &amp;\text{Letter is in the correct envelope}\\ 0, &amp;\text{Otherwise}\end{cases}\)</span></p>
<p>This is an indicator RV. <span class="math inline">\(P(\mathbb{I}_i) = \frac{1}{N}\)</span>. So <span class="math inline">\(E[\mathbb{I}_i] = \frac{1}{N}\)</span>, <span class="math inline">\(Var(\mathbb{I}_i) = \frac{1}{N}(1 - \frac{1}{N})\)</span>.</p>
<p>Let <span class="math inline">\(X = \sum_{i=1}^N \mathbb{I}_i\)</span>. Now we need <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(Var(X)\)</span>.</p>
<p><span class="math inline">\(E[X] = \sum_{i=1}^NE[\mathbb{I}_i] = 1\)</span>.</p>
<p><span class="math display">\[\begin{aligned}Var(X) &amp;= Var(\sum_{i = 1}^n \mathbb{I}_i) \\ &amp;= \sum_{i = 1}^N Var(\mathbb{I}_i) + \sum_{i \neq j} Cov(\mathbb{I}_i, \mathbb{I}_j) \\ &amp;= (1 - \frac{1}{N}) + \sum_{i \neq j} (E[\mathbb{I}_i \mathbb{I}_j] - E[\mathbb{I}_i]E[\mathbb{I}_j]) \\ &amp;= (1 - \frac{1}{N}) + \sum_{i \neq j} (P(\mathbb{I}_i = 1, \mathbb{I}_j = 1) - \frac{1}{N^2}) \\ &amp;= (1 - \frac{1}{N}) + \sum_{i \neq j}(\frac{1}{N(N-1)} - \frac{1}{N^2}) \\ &amp;= 1 \end{aligned}\]</span></p>
</blockquote>
<h1 id="life-is-normal">Life is Normal</h1>
<p><strong>Proposition</strong>: Law of <strong>Large</strong> Numbers. Let <span class="math inline">\(X_i\)</span> be independent and indentically distributed random variables with mean <span class="math inline">\(\mu\)</span>. Then their sample mean converges to the true mean.</p>
<p><span class="math display">\[\lim_{n \to \infty} \overline{X}_n = \mu\]</span></p>
<p><strong>Theorem</strong>: Central Limit Theorem. Suppose that <span class="math inline">\(X_i\)</span> are independent random variables, with a common distribution function <span class="math inline">\(F\)</span>. Suppose further than <span class="math inline">\(E(X_i) = \mu\)</span> and <span class="math inline">\(Var(X_i) = \sigma^2 &lt; \infty\)</span>. Then for all <span class="math inline">\(x \in \mathbb{R}\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
<p><span class="math display">\[P\left(\frac{\overline{X} - \mu}{\sigma / \sqrt n} \le x \right) \to \Sigma(x),\]</span></p>
<p>In other words, if <span class="math inline">\(n\)</span> is large.</p>
<p><span class="math display">\[\overline{X} \approx N(\mu, \frac{\sigma^2}{n}), \sum_{i = 1}^n X_i \approx N(n\mu, n\sigma^2)\]</span></p>
<p>Example: Eating a box of chocolate. Each box has 20 cubes of chocolate. The weight of each cube varies. Weight <span class="math inline">\(W\)</span> of each cube is a random variable with mean <span class="math inline">\(\mu = 25\)</span> and <span class="math inline">\(\sigma = 0.1\)</span>. Find the probability that the box has at least 500 grams of chocolate in it, assuming the weight of each cube is independent.</p>
<blockquote>
<p><span class="math inline">\(S_{20} = \sum_{i = 1}^{20} W_i\)</span>. By CLT, <span class="math inline">\(S_{20} \approx N(20 \cdot 25, 20 \cdot 0.1^2)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}P(S_{20} \ge 500) &amp;= P(Z_{20} \ge \frac{500 - 500}{\sqrt{0.2}}) \\ &amp;= P(Z_{20} \ge 0)\end{aligned}\]</span></p>
</blockquote>
<p>Example: Jason rolls a six sided die 1000 times, and records the results. If the die is a fair die, estimate the probability that the sum of the die is less than 3400.</p>
<blockquote>
<p><span class="math inline">\(E[X_i] = 3.5\)</span>, <span class="math inline">\(Var(X_i) = 2.92\)</span>. By CLT, <span class="math inline">\(S_{1000} = \sum_{X_i} \approx N(1000 \cdot 3.5, 1000 \cdot 2.92)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}P(S_{1000} &lt; 3400) &amp;= P(Z_{1000} &lt; \frac{3400 - 3500}{\sqrt {2920}}) \\ &amp;= 0.03216\end{aligned}\]</span></p>
</blockquote>
<p><strong>Theorem</strong>: If <span class="math inline">\(X_n \sim Binomial(n, p)\)</span>, then for large <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[\frac{X_n - np}{\sqrt{np(1-p)}} \approx N(0,1)\]</span></p>
<p><strong>Theorem</strong>: If <span class="math inline">\(X_\lambda \sim Poi(\lambda)\)</span>, then for large <span class="math inline">\(lambda\)</span>.</p>
<p><span class="math display">\[\frac{X_\lambda - \lambda}{\lambda} \approx N(0, 1)\]</span></p>
<p>When approximating discrete random variables with normal distribution, then discrete distribution will never be truly “normal”. In this case, we use <strong>continuity correction</strong>.</p>
<p><span class="math display">\[\begin{aligned}
P(S_n = s) &amp;\approx P(s - 0.5 &lt; S_n &lt; s + 0.5) \\
&amp;\approx P\left(\frac{(s - 0.5) - \mu_{S_n}}{SD(S_n)} &lt; Z &lt; \frac{(s + 0.5) - \mu_{S_n}}{SD(S_n)}\right)
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
P(a \le X \le b) &amp;\approx P(a - 0.5 \le X \le b + 0.5) \\
&amp;\approx P\left(\frac{(a - 0.5) - \mu_x}{SD(X)} \le Z \le \frac{(b + 0.5) - \mu_x}{SD(X)}\right)
\end{aligned}\]</span></p>
<p>Example: <span class="math inline">\(X \sim Poi(\lambda)\)</span>. Use normal approximation to estimate <span class="math inline">\(P(X = \lambda)\)</span> and <span class="math inline">\(P(X &gt; \lambda)\)</span>. Compare this approximation with the true value when <span class="math inline">\(\lambda = 9\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}P(X = \lambda) &amp;\approx P(\lambda - 0.5 \le X \le \lambda + 0.5) \\ &amp;\approx P\left(\frac{(\lambda - 0.5) - \lambda}{\sqrt \lambda} \le Z \le \frac{(\lambda + 0.5) - \lambda}{\sqrt \lambda}\right) \\ &amp;= P\left(\frac{-0.5}{\sqrt \lambda} \le Z \le \frac{0.5}{\sqrt \lambda}\right)\end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p><span class="math display">\[\begin{aligned}P(X &gt; x) &amp;\approx P(X \ge \lambda - 0.5) \\ &amp;\approx P\left(Z \ge \frac{(\lambda - 0.5) - \lambda}{\sqrt \lambda}\right) \\ &amp;=1 - P\left(Z &lt; \frac{-0.5}{\sqrt \lambda}\right)\end{aligned}\]</span></p>
</blockquote>
<p>Example: Suppose <span class="math inline">\(X_1, .., X_50\)</span> are independent Geometric random variablces with parameter 0.5. Estimate the probability that <span class="math inline">\(\overline{X}_50 &gt; 6.5\)</span>.</p>
<blockquote>
<p>We want <span class="math inline">\(P(\overline{X}_{50} &gt; 6.55)\)</span>. We need <span class="math inline">\(E[\overline{X}_{50}]\)</span> and <span class="math inline">\(SD(\overline{X}_{50})\)</span>.</p>
<p><span class="math inline">\(\begin{aligned}E[\overline{X}_{50}] &amp;= \frac{1}{50}\sum_{i = 1}^{50} E[X_i] \\ &amp;= \frac{1}{50} \sum_{i=1}^{50} 1 \\ &amp;= 1\end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned}Var(\overline{X}_{50}) &amp;= \left(\frac{1}{50}\right)^2 \sum_{i = 1}^{50} Var(X_i) \\ &amp;= \left(\frac{1}{50}\right)^2 \sum_{i = 1}^{50} 2 \\ &amp;= \frac{1}{25} \end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned} P(\overline{X}_{50} &gt; 6.5) &amp;\approx P(\overline{X}_{50} &gt; 6) \\ &amp;\approx P\left(Z &gt; \frac{(6 - 1)}{\sqrt \frac{1}{25}}\right) \\ &amp;= 1 - P(Z &lt; 25) \\ &amp;\approx 0 \end{aligned}\)</span></p>
</blockquote>
<p><strong>Rules of Thumb</strong>.</p>
<ol type="1">
<li>If the number of observations exceeds 30, then CLT provides a reasonable approximation.</li>
<li>If the distribution of observations is “close” to being unimodal and “close” to being continuous, then CLT can be reasonable for even smaller values of <span class="math inline">\(n\)</span>.</li>
<li>If the distribution is highly screwed or very discrete, then a large value of <span class="math inline">\(n\)</span> might be necessary.</li>
<li>When approximating a <strong>continuous</strong> distribution with normal, we do not use continuity correction.</li>
</ol>
<h1 id="moment-generating-function">Moment Generating Function</h1>
<p><strong>Definition</strong>: The <strong>moment generating function</strong> or MGF of a random variable <span class="math inline">\(X\)</span> is given by.</p>
<p><span class="math display">\[M_X(t) = E[e^{tX}], t \in \mathbb{R}\]</span></p>
<p>In particular, if <span class="math inline">\(X\)</span> is discrete with probability function <span class="math inline">\(f(x)\)</span>, then.</p>
<p><span class="math display">\[M_X(t) = \sum_{x \in X(S)} e^{tx} f(x), t \in \mathbb{R}\]</span></p>
<p><strong>Properties</strong>.</p>
<ol type="1">
<li><span class="math display">\[M_x(t) = \sum_{j = 0}^\infty \frac{t^j E[X^j]}{j!}\]</span></li>
<li><p>So long as <span class="math inline">\(M_x(t)\)</span> is defined in a neighbourhood of <span class="math inline">\(t = 0\)</span>.</p>
<p><span class="math display">\[\frac{d}{d_t^k}M_x(0) = E[X^k]\]</span></p></li>
</ol>
<p>The significance of MGF is that, under certain conditions, it can show equivalence of two distributions.</p>
<p><strong>Proposition</strong>: Continuity theorem. If <span class="math inline">\(X, Y\)</span> have MFGs <span class="math inline">\(M_X(t), M_Y(t)\)</span> defined in neighbourhoods of the origin, and satisfying <span class="math inline">\(M_X(t) = M_Y(t)\)</span> for all <span class="math inline">\(t\)</span> where they are defined.</p>
<p><span class="math display">\[X \stackrel{D}{=} Y\]</span></p>
<p>This means that the MGF uniquely characterises a distribution.</p>
<p>Example: Let <span class="math inline">\(X \sim Poi(\lambda)\)</span>. Derive the MGF of <span class="math inline">\(X\)</span>, and use it to show that <span class="math inline">\(E[X] = \lambda = Var(X)\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}M_X(t) &amp;= E[e^{tx}] \\ &amp;= \sum_{x = 0}^\infty e^{tx} P(X = x) \\ &amp;= \sum_{x = 0}^\infty e^{tx} \frac{e^{-\lambda}\lambda^x}{x!} \\ &amp;= e^{\lambda(e^t - 1)}, \forall t \in \mathbb{R} \end{aligned}\]</span></p>
<p><span class="math inline">\(\begin{aligned}E[X] &amp;= M_X^\prime (t) |_{t = 0} \\ &amp;= e^{\lambda(e^t - 1)}\lambda e^t |_{t = 0} \\ &amp;= \lambda\end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned}E[X^2] &amp;= M_X^{\prime \prime}(t) |_{t = 0} \\ &amp;= (e^{\lambda(e^t - 1)}(\lambda e^{t})^2 + e^{\lambda(e^t - 1)}\lambda e^t) |_{t = 0} \\ &amp;= \lambda ^2 + \lambda\end{aligned}\)</span></p>
<p><span class="math inline">\(Var(X) = (\lambda^2 + \lambda) - \lambda^2 = \lambda\)</span></p>
</blockquote>
</body>
</html>
