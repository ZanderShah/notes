<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>stat230</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/contrib/auto-render.min.js"></script><script>document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body);
  });</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="stat-230">STAT 230</h1>
<p>Example: Suppose you have 20 distinct books, 7 of which are written by Mark Twain.</p>
<ol type="1">
<li><p>How many ways can you arrange 12 books on a shelf if exactly 3 of them must be Mark Twain books?</p>
<blockquote>
<p><span class="math inline">\({7 \choose 3}{13 \choose 9}\)</span> ways to choose subsets.<br />
Books can be rearranged in <span class="math inline">\(12!\)</span> ways.<br />
Therefore, the answer is <span class="math inline">\(12!{7 \choose 3}{13 \choose 9}\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: Consider drawing 3 numbers at random <strong>with</strong> replacement from the digits 0 to 10. What is the probability that there is a repeated number among the 3.</p>
<blockquote>
<p>We can consider the complement, where there are no repeated numbers.<br />
<span class="math inline">\(1 - \frac{10 \cdot 9 \cdot 8}{10^3}\)</span></p>
</blockquote>
<p>Example: Suppose there are 4 passengers on an elevator that services 5 floors. Each passenger is equally likely to get off at any floor.</p>
<blockquote>
<p><span class="math inline">\(|S| = 5^4\)</span></p>
</blockquote>
<ol type="1">
<li><p>What is the probability that the passengers all get off on different floors?</p>
<blockquote>
<p><span class="math inline">\(|A| = 5^{(4)}\)</span>. Therefore <span class="math inline">\(P(A) = \frac{5^{(4)}}{5^4}\)</span>.</p>
</blockquote></li>
<li><p>2 passengers get off on floor 2, and 2 get off on floor 3.</p>
<blockquote>
<p><span class="math inline">\(|A| = {4 \choose 2}{2 \choose 2}\)</span>. Therefore <span class="math inline">\(P(A) = \frac{{4 \choose 2}{2 \choose 2}}{5^4}\)</span>.</p>
</blockquote></li>
<li><p>2 passengers get off on one floor, and 2 passengers get off on another floor.</p>
<blockquote>
<p><span class="math inline">\({5 \choose 2}{4 \choose 2}{2 \choose 2}\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: Consider rearranging the letters at random in the name “ZENYATTA” to form a single ‘word’.</p>
<ol type="1">
<li><p>How many ways can this be done?</p>
<blockquote>
<p><span class="math inline">\(|S| = {8 \choose 2}{6 \choose 2}{4 \choose 1}{3 \choose 1}{2 \choose 1}{1 \choose 1} = \frac{8!}{2!2!}\)</span>.</p>
</blockquote></li>
<li><p>What is the probaility that all of the letters appear in alphabetical order?</p>
<blockquote>
<p>A = { “AAENTTYZ” }. Therefore <span class="math inline">\(P(A) = \frac{1}{|S|}\)</span>.</p>
</blockquote></li>
<li><p>What is the probability that the word begins and ends with “T”?.</p>
<blockquote>
<p><span class="math inline">\(|A| = {6 \choose 2}{4 \choose 1}{3 \choose 1}{2 \choose 1}{1 \choose 1}\)</span>.</p>
</blockquote></li>
</ol>
<p><span class="math inline">\({n \choose n_1}{n - n_1 \choose n_2}...{n_k \choose n_k} = {n \choose n1,n2,...,n_k}\)</span>.</p>
<p>Example: Harold’s daily morning ritual is to drink 5 cans of pop. He has 2 cans of pop C, 2 cans of pop F, and 1 can of pop P. How many ways can he complete the ritual?</p>
<blockquote>
<p><span class="math inline">\(|A| = \frac{5!}{2!2!} = 30\)</span>.</p>
</blockquote>
<p>Example: Find the probability that a bridge hand (13 cards picked at random from a standard deck without replacement) has …</p>
<blockquote>
<p><span class="math inline">\(|S| = {52 \choose 13}\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>3 aces.</p>
<blockquote>
<p><span class="math inline">\(|A| = {4 \choose 3}{48 \choose 10}\)</span>. Therefore <span class="math inline">\(P(A) = \frac{{4 \choose 3}{48 \choose 10}}{52 \choose 13}\)</span>.</p>
</blockquote></li>
<li><p>At least 1 ace.</p>
<blockquote>
<p>Consider the complement. <span class="math inline">\(|A^c| = {48 \choose 13}\)</span>. Therefore <span class="math inline">\(P(A) = 1 - \frac{48 \choose 13}{52 \choose 13}\)</span>.</p>
</blockquote></li>
<li><p>6 spaces, 4 hearts, 2 diamonds, 1 club.</p>
<blockquote>
<p><span class="math inline">\(|A| = {13 \choose 6}{13 \choose 4}{13 \choose 2}{13 \choose 1}\)</span>. Therefore <span class="math inline">\(P(A) = \frac{{13 \choose 6}{13 \choose 4}{13 \choose 2}{13 \choose 1}}{52 \choose 13}\)</span>.</p>
</blockquote></li>
</ol>
<p>For verifying your answer when counting across disjoint unions, adding up the top and bottom numbers for the <span class="math inline">\({n \choose k}\)</span> in <span class="math inline">\(|A|\)</span> should add up to <span class="math inline">\(|S|\)</span>.</p>
<p>Example (<em>Team captain problem</em>): Show that <span class="math inline">\({n \choose k} \cdot k = {n -1 \choose k - 1} \cdot n\)</span>.</p>
<blockquote>
<p>We are either picking the team first and then choosing the captain, or the other way around.</p>
</blockquote>
<p>Example (<em>Vandermonde’s Identity</em>): Show that <span class="math inline">\({n + m \choose k} = \sum_{i = 0}^k {n \choose i}{m \choose k - i}\)</span> for non-negative integers <span class="math inline">\(n, m, k\)</span>.</p>
<h2 id="inclusion-exclusion-principle">Inclusion Exclusion Principle</h2>
<blockquote>
<p><span class="math display">\[P(\cup_{i = 1}^n A_i) = \cup_{i} P(A_i) - \cup_{i &lt; j} P(A_iA_j) + \cup_{i &lt; j &lt; k}P(A_iA_jA_k) - ...\]</span></p>
</blockquote>
<p>Example: Suppose that 2 fair 6 sided dce are rolled. What is the probability that at least 1 of the dice shows a 6?</p>
<blockquote>
<p><span class="math inline">\(P(A) = 1 - P(A^c) = 1 - (\frac{5}{6})^2 = 1 - \frac{25}{36} = \frac{11}{36}\)</span>.</p>
</blockquote>
<p>Example: There are 6 stops on the subway and 4 passengers on the subway car. Assume the passengers are equally likely to get off at any stop. Find the probability that:</p>
<blockquote>
<p><span class="math inline">\(|S| = 6^4\)</span>.</p>
</blockquote>
<ol type="1">
<li>The passengers all get off at different stops.</li>
<li>2 passengers get off at stop 2 and 2 passengers get off at stop 5.</li>
<li><p>2 passengers get off at 1 stop and the other 2 passengers get off at another 1 stop.</p>
<blockquote>
<p><span class="math inline">\(|A| = {6 \choose 2}{4 \choose 2}{2 \choose 2}\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: 5 tourists plan to attent Octoberfest. There are 7 locations possible. Find the probability that:</p>
<blockquote>
<p><span class="math inline">\(|S| = 7^5\)</span></p>
</blockquote>
<ol type="1">
<li>All tourist attend different locations.</li>
<li>The tourists all attend the same location.</li>
<li><p>2 tourists attend 1 location and 3 tourists attend another location.</p>
<blockquote>
<p><span class="math inline">\(|A| = {7 \choose 2} \cdot 2 \cdot {5 \choose 2}{3 \choose 3} = 7 \cdot 6 \cdot {5 \choose 2}{3 \choose 3}\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: Consider rearranging the “NOOB” at random. Let <span class="math inline">\(A\)</span> represent the event that two “O”s appear together, and let <span class="math inline">\(B\)</span> denote the event that the word starts with the letter <span class="math inline">\(N\)</span>. Determine.</p>
<ol type="1">
<li><span class="math inline">\(P(A) = \frac{3!}{\frac{4!}{2!}} = \frac{1}{2}\)</span></li>
<li><span class="math inline">\(P(B) = \frac{3!}{4!} = \frac{1}{4}\)</span></li>
<li>The probability that the resulting word does not start with “N” and that the “O”s do not appear together. <span class="math inline">\(P(\overline{A} \cap \overline{B}) = P(\overline{A \cup B})\)</span>.</li>
</ol>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>indepenedent</strong> if <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span>.</p>
<p>Example: Consider rolling 2 fair 6 sided dice. <span class="math inline">\(A\)</span> is the event where the sum is 10. <span class="math inline">\(B\)</span> is the event that the first die rolls a 6. <span class="math inline">\(C\)</span> is the event that the sum is 7. Are <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> independent?</p>
<blockquote>
<p><span class="math inline">\(B = \{(6, i) \mid i \in \{1, ..., 6\}\}\)</span>. <span class="math inline">\(A = \{(5, 5), (6, 4), (4, 6)\}\)</span>. We can see that <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> are not independent.</p>
</blockquote>
<p>If <em>knowing</em> <span class="math inline">\(A\)</span> restricts our choices for <span class="math inline">\(B\)</span>, they are not independent.</p>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>mutually exclusive</strong> if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint.</p>
<p><strong>Proposition</strong>: Suppose that not both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are trivial events. If <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> are indepenent <strong>and</strong> mutually exclusive, then either <span class="math inline">\(P(A) = 0\)</span> or <span class="math inline">\(P(B) = 0\)</span>.</p>
<p><strong>Proposition</strong>: If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, then <span class="math inline">\(\overline{A}\)</span> and <span class="math inline">\(\overline{B}\)</span> are independent, <span class="math inline">\(A\)</span> and <span class="math inline">\(\overline{B}\)</span> are independent, and <span class="math inline">\(\overline{A}\)</span> and <span class="math inline">\(B\)</span> are independent.</p>
<p><strong>Definition</strong>: Conditional probabilty of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>, so long as <span class="math inline">\(P(B) &gt; 0\)</span>, is denoted by <span class="math inline">\(P(A \mid B) = \frac{P(A \cup B)}{P(B)}\)</span>.</p>
<p><strong>Definition</strong>: For events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, <span class="math inline">\(P(A \cap B) = P(A \mid B)P(B) = P(B \mid A)P(A)\)</span>.</p>
<p><strong>Bayes Theorem</strong>: <span class="math inline">\(P(B_i | A) = \frac{P(A|B_i)P(B_i)}{\sum_{j = 1}^k P(A | B_j)P(B_j)}\)</span></p>
<p>Example: Pick Pharah, 20% chance of loss. Pick Winston, 10% chance of loss. Pick Winston 70% of the time, Pharah 30% of the time. Given that the game was lost, what is the probability that Pharah was picked?</p>
<blockquote>
<p>Let <span class="math inline">\(P\)</span> be the event that Pharah is picked. Let <span class="math inline">\(L\)</span> be the event where the game is lost. Let <span class="math inline">\(W\)</span> be the event that Winston is picked. <span class="math display">\[\begin{aligned}P(P| L) &amp;= \frac{P(L| P)P(P)}{P(L)} \\ &amp;= \frac{0.2 \cdot 0.3}{P(L| P)P(P) + P(L| W)P(W)} \\ &amp;= \frac{0.2 \cdot 0.3}{0.2 \cdot 0.3 + 0.1 \cdot 0.7} \\ &amp;= 0.461
5\end{aligned}\]</span></p>
</blockquote>
<p><strong>Probablity Function</strong>: <span class="math inline">\(f_X(x) = P(X = x)\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(0 \le f_X(x) \le 1\)</span>.</li>
<li><span class="math inline">\(\sum_{x \in X(S)}f_X(x) = 1\)</span>.</li>
</ol>
<p><strong>Cumulative Distribution Function (<em>CDF</em>)</strong>: <span class="math inline">\(F_X(x) = P(X \le x)\)</span>, <span class="math inline">\(x \in \mathbb{R}\)</span>. We can use <span class="math inline">\(P(X \le x) = P(\{\omega \in S: X(\omega) \le x\})\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(0 \le F_X(x) \le 1\)</span>.</li>
<li><span class="math inline">\(F_X(x) \le F_X(y)\)</span> for <span class="math inline">\(x &lt; y\)</span>.</li>
<li><span class="math inline">\(\lim_{x \to -\infty}F_x(x) = 0\)</span>, and <span class="math inline">\(\lim_{x \to \infty}F_X(x) = 1\)</span>.</li>
</ol>
<p>Example: <span class="math inline">\(N\)</span> balls labelled <span class="math inline">\(1, 2, ..., N\)</span> are placed in a box, and <span class="math inline">\(n \le N\)</span> are randomly selected without replacement. Find <span class="math inline">\(P(X = x)\)</span> where random variable <span class="math inline">\(X\)</span> is the largest number selected.</p>
<blockquote>
<p><strong>pdf</strong>: There is only 1 way to select <span class="math inline">\(x \in \{1, ..., n\}\)</span>. There are <span class="math inline">\({x - 1 \choose n - 1}\)</span> ways to pick the remaining <span class="math inline">\(n - 1\)</span> balls. <span class="math inline">\(P(X = x) = \frac{x - 1 \choose n - 1}{N \choose n}\)</span>, <span class="math inline">\(x \ge n\)</span>. <span class="math inline">\(P(X \le x) = \frac{x \choose n}{N \choose n}\)</span>, <span class="math inline">\(x \ge n\)</span>.</p>
<p>Now we use the property of <strong>pdf</strong>. <span class="math display">\[\begin{aligned}P(X = x) &amp;= F(x) - F(x - 1) \\ &amp;= \frac{x \choose n}{N \choose n} - \frac{x - 1 \choose n}{N \choose n} \\ &amp;= \frac{x - 1 \choose N - 1}{N \choose n}\end{aligned}\]</span></p>
</blockquote>
<p>Example: Suppose a tack when flipped has probability <span class="math inline">\(0.6\)</span> of landing point up. If the tack is flipped <span class="math inline">\(10\)</span> times, what is the probability it lands point up more than twice?</p>
<blockquote>
<p><span class="math inline">\(X \in \{0, 1, 2, ..., 10\}\)</span>, <span class="math inline">\(X \sim Bin(n = 10, p = 0.6)\)</span> .</p>
<p>We want <span class="math inline">\(P(X \ge 3) = 1 - P(X &lt; 2) = 1 - (0.6^{10} + {10 \choose 1}0.6^9 0.4 + {10 \choose 2}0.6^8 0.4^2)\)</span>.</p>
</blockquote>
<p>Example: Suppose a fair coin is flipped 17 times. Let <span class="math inline">\(X\)</span> denote the number of heads observed, and let <span class="math inline">\(Y\)</span> denote the number of tails observed. Which of the following is false?</p>
<blockquote>
<ul>
<li><span class="math inline">\(X \sim Binomial(17, 0.5)\)</span>.</li>
<li><span class="math inline">\(Y \sim Binomial(17, 0.5)\)</span>.</li>
<li><span class="math inline">\(X \sim Y\)</span> (<em>X follows the distribution of Y</em>).</li>
<li><span class="math inline">\(X + Y = 17\)</span>.</li>
<li><span class="math inline">\(X = Y\)</span> (<em>False, because RV quantity is not fixed</em>).</li>
</ul>
</blockquote>
<p>Example: Weekly lottery has a probability of 0.02 of winning a prize with a single ticket. If you buy one ticket per week for 52 weeks, what is the probability that you …</p>
<blockquote>
<p><span class="math inline">\(X \sim Bin(n=52, p=0.02)\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>Win no prizes?</p>
<blockquote>
<p>We want <span class="math inline">\(P(X = 0) = 0.98^{52}\)</span>.</p>
</blockquote></li>
<li><p>Win 3 or more prizes?</p>
<blockquote>
<p>We want <span class="math inline">\(P(X \ge 3) = 1 - (P(X = 0) + P(X = 1) + P(X = 2))\)</span>.</p>
</blockquote></li>
</ol>
<p>Binomial and hypergeometric distributions are fundamentally different, as the former picks with replacement, whereas the latter picks without replacement.</p>
<p><strong>Theorem</strong>: If <span class="math inline">\(r\)</span> and <span class="math inline">\(N\)</span> relatively larger than <span class="math inline">\(n\)</span> and <span class="math inline">\(\frac{r}{N} = p\)</span> where <span class="math inline">\(p \in [0, 1]\)</span>, then if <span class="math inline">\(X \sim Hypergeometric(N, r, n)\)</span> and <span class="math inline">\(Y \sim Binomial(n, p)\)</span> then <span class="math inline">\(P(X \le k) \approx P(Y \le k)\)</span>.</p>
<p>Example: In Overwatch there are 27 playable characters, of which 6 are considered “Tanks”. Suppose that three characters are drawn at random.</p>
<ol type="1">
<li><p>What is the probability that the selection contains exactly 2 tanks.</p>
<blockquote>
<p>We want <span class="math inline">\(P(T = 2) = \frac{{6 \choose 2}{21 \choose 1}}{27 \choose 3} \approx 0.1077\)</span>.</p>
</blockquote></li>
<li><p>Approximate this probability using binomial distribution.</p>
<blockquote>
<p><span class="math inline">\(T_{Bin} \sim Bin(3, \frac{6}{27})\)</span>.</p>
<p>We want <span class="math inline">\(P(T_{Bin} = 2) = {3 \choose 2}(\frac{6}{27})^2(\frac{21}{27}) \approx 0.1152\)</span>.</p>
</blockquote></li>
</ol>
<h2 id="negative-binomial">Negative Binomial</h2>
<p>Question: We want the number of tails until you get the first head. <span class="math display">\[P(X = x) = (1-p)^xp,\ x = 0,1...\]</span></p>
<p>Question: If I model the total number of coin flips until I get the first head, is this also a geometric distribution? <strong>Yes</strong> because it is essentially the same as the last question.</p>
<p>We generalize to <span class="math inline">\(k\)</span> successes by noticing that the last trial must produce the <span class="math inline">\(k\)</span>th success and the remaining <span class="math inline">\(k-1\)</span> successes may appear anywhere from the <span class="math inline">\(1\)</span>st to <span class="math inline">\(2\)</span>nd last trial. <span class="math display">\[{r + k - 1 \choose k - 1}p^k(1-p)^r\]</span></p>
<p>Example: There is a 50.4% change of flipping a head. What is the probability that you need more than 5 flips to get a tail?</p>
<blockquote>
<p><span class="math inline">\(1 - P(X \le 4) = 1 - \sum_{x = 0}^4 0.504^x (1 - 0.504)\)</span>.;</p>
</blockquote>
<p>Exampe: Hanzo is getting more popular. Every time I join a new game, I have a 15% change of picking Hanzo. What is the probability that …</p>
<blockquote>
<p>Let <span class="math inline">\(W\)</span> be the number of games played before I pick Hanzo. <span class="math inline">\(W \sim Geo(0.15)\)</span>. We have <span class="math inline">\(f_W(w) = (1-p)^wp\)</span>, <span class="math inline">\(w \in \mathbb{N}_0\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>I will need to play 4 games before I get to pick Hanzo?</p>
<blockquote>
<p><span class="math inline">\(P(W = 4) = f_w(4) = (0.85)^40.15 = 0.0783\)</span>.</p>
</blockquote></li>
<li><p>I will need to play at least 4 games before I get to pick Hanzo, given that I have to play at least 3 games before I pick him?</p>
<blockquote>
<p><span class="math inline">\((P(W \ge 4 | W \ge 3) = \frac{P(W \ge 4 \cap W \ge 3)}{P(W \ge 3)} = \frac{P(W \ge 4)}{P(W \ge 3)} = 0.85\)</span>.</p>
<p><span class="math display">\[\begin{aligned}P(W \ge w) &amp;= \sum_{t=w}^\infty (1-p)^tp \\ &amp;= p(1-p)^w\sum_{t=0}^\infty (1-p)^t \\ &amp;= p(1-p)^w\frac{1}{1 - (1-p)} \\ &amp;= (1-p)^w\end{aligned}\]</span></p>
</blockquote></li>
<li><p>I will need to play at least one game before I get to pick Hanzo?</p>
<blockquote>
<p><span class="math inline">\(P(W \ge 1) = 1 - P(W = 0) = 1 - p = 0.85\)</span>.</p>
</blockquote></li>
</ol>
<h2 id="memoryless-property-of-geometric">Memoryless Property of Geometric</h2>
<p>Let <span class="math inline">\(X \sim Geo(p)\)</span> and <span class="math inline">\(s, t\)</span> be non-negative integers. <span class="math display">\[P(X \ge s + t | X \ge s) = P(X \ge t)\]</span></p>
<p>Example: Mobile game “Show Me The Money”. Game released super rate item which only appears from a loot box which costs $2 per box, with the change of 0.01%.</p>
<blockquote>
<p>Let <span class="math inline">\(X\)</span> be the number of boxes without the rare item.</p>
</blockquote>
<ol type="1">
<li><p>What is the probability that he will need to buy 50 loot boxes to get 2 super rate items?</p>
<blockquote>
<p>Number of success is fixed but not the number of trials, so we use negative binomial. <span class="math inline">\(X \sim NB(2, 0.0001)\)</span>.</p>
<p>We want <span class="math inline">\(P(X = 48) = {48 + 2 - 1 \choose 2 - 1}(0.9999)^{48}(0.0001)^2 = 0.000000488\)</span>.</p>
</blockquote></li>
</ol>
<h1 id="poisson">Poisson</h1>
<p>We say that a random variable <span class="math inline">\(X \sim Poisson(\lambda)\)</span> if we have <span class="math inline">\(f_X(x)\)</span> such that. <span class="math display">\[f_X(x) = e^{-\lambda}\frac{\lambda^x}{x!}\]</span></p>
<p>We verify that this is a valid probability distribution using the exponential series. <span class="math display">\[\begin{aligned}
\sum_{x=0}^\infty f_X(x) &amp;= \sum_{x=0}^\infty e^{-\lambda}\frac{\lambda^x}{x!} \\
&amp;= e^{-\lambda}e^\lambda \\
&amp;= 1
\end{aligned}\]</span></p>
<p>One way to view poisson is to consider the limiting case of binomial, where you fix <span class="math inline">\(\lambda = np\)</span>, and let <span class="math inline">\(n \to \infty\)</span> and <span class="math inline">\(p \to 0\)</span>.</p>
<h2 id="poisson-process">Poisson Process</h2>
<ol type="1">
<li><p><strong>Independence</strong>: the number of occurences in non-overlapping intervals are independent.</p></li>
<li><p><strong>Individuality</strong>: for sufficiently short periods of time, <span class="math inline">\(\Delta t\)</span>, the probability of two or more events occuring in the interval approaches zero.</p></li>
</ol>
<p><span class="math display">\[\lim_{\Delta t \to 0} \frac{P(\text{2 of more events in } (t, t + \Delta t))}{\Delta t} = 0\]</span></p>
<ol start="3" type="1">
<li><strong>Homogeneity or Uniformity</strong>: events occur at a uniform or homogenous rate <span class="math inline">\(\lambda\)</span> and proportional to time interval <span class="math inline">\(\Delta t\)</span>.</li>
</ol>
<p><span class="math display">\[\lim_{\Delta t \to 0} \frac{P(\text{one event in } (t, t + \Delta t)) - \lambda \Delta t}{\Delta t} = 0\]</span></p>
<p>A process that satisfies the prior conditions on the occurrence of events is often called a <strong>Poisson Process</strong>. More precisely, if <span class="math inline">\(X_t\)</span>, for <span class="math inline">\(t \ge 0\)</span>, denotes the number of events that have occurred up to time <span class="math inline">\(t\)</span>, then <span class="math inline">\(X_t\)</span> is called a <strong>Poisson Process</strong>.</p>
<p>Example: Website hits for a given website occur according to a Poisson process with a rate of 100 hits per minute. Find …</p>
<ol type="1">
<li><span class="math inline">\(P(\text{1 hit is observed in a second}) = \frac{e^{-\frac{5}{3}}\frac{5}{3}^1}{1!}\)</span></li>
<li><span class="math inline">\(P(\text{90 hits are observed in a minute}) = \frac{e^{-100}100^{90}}{90!}\)</span></li>
</ol>
<h2 id="relation-to-binomial">Relation to Binomial</h2>
<p>Consider one unit of time, so that the process follows <span class="math inline">\(Poi(\lambda)\)</span>.</p>
<p>We chop up the interval into <span class="math inline">\(n\)</span> equally-sized pieces. If we chop it up finely enough, then by individuallity, the probability of two or more events occuring goes to 0.</p>
<p>This means that, over a small enough size, we approximately have either 0 or 1 event occuring with <span class="math inline">\(P(event) = p\)</span> for every piece.</p>
<p>Moreover, the event probability <span class="math inline">\(p\)</span> is proportional to the length of the interval by homogeneity. This means that as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(p \to 0\)</span>.</p>
<p>Finally, by independence, each <span class="math inline">\(n\)</span> pieces are independent, so we have <span class="math inline">\(Bin(n, p)\)</span>, where <span class="math inline">\(n\)</span> is very large and <span class="math inline">\(p\)</span> is very small.</p>
<p>We expect to see <span class="math inline">\(np\)</span> events, recall that rate <span class="math inline">\(\lambda\)</span> is the rate of occurance over 1 unit of time. So <span class="math inline">\(\lambda = np\)</span>, and as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(p \to 0\)</span>, in <span class="math inline">\(Bin(n, p)\)</span>, we approach <span class="math inline">\(Poi(np)\)</span>.</p>
<p>Example: A bit error occurs got a given data transmission method independently in out of of every 1000 bits transferred. Suppose a 64 bit message is sent using the transmission system.</p>
<ol type="1">
<li><p>What is the probability that there are exactly 2 bit errors?</p>
<blockquote>
<p><span class="math inline">\(X \sim Bin(64, \frac{1}{1000})\)</span>, <span class="math inline">\(P(X = 2) = {64 \choose 2}(\frac{999}{1000})^{62} (\frac{1}{1000})^2 \approx 0.019\)</span>.</p>
</blockquote></li>
<li><p>Approximate using Poisson.</p>
<blockquote>
<p><span class="math inline">\(X \sim Poi(\frac{64}{1000})\)</span>, <span class="math inline">\(P(X = 2) = \frac{e^{-\frac{64}{1000}}(\frac{64}{1000})^2}{2!} \approx 0.019\)</span>.</p>
</blockquote></li>
</ol>
<p>Example: Shiny versions of Pokemon are possible to encounter and catch starting in Generation 2 (Pokemon Gold / Silver). Normal encounters with Pokemon while running in grass occur according to a Poisson process with rate 1 per minute on average. 1 in every 8192 encounters will be a Shiny Pokemon, on average.</p>
<ol type="1">
<li>If you run around in the grass for 15 hours, what is the probability you will encounter at least one Shiny Pokemon?</li>
</ol>
<blockquote>
<p><span class="math inline">\(X \sim Poi(\frac{900}{8192})\)</span>. <span class="math inline">\(P(X \ge 1) = 1 - P(X = 0) = 1 - e^{-\frac{900}{8192}}\)</span>.</p>
</blockquote>
<ol start="2" type="1">
<li>How long would you have to run around in grass to have better than 50 percent chance of encountering at least one Shiny Pokemon?</li>
</ol>
<blockquote>
<p>Solve for <span class="math inline">\(t\)</span>, where <span class="math inline">\(0.5 = P(X_t \ge 1)\)</span>.</p>
</blockquote>
<p>Example: An infinite number of Harolds are released in a gold mine. They scatter randomly, so that on average, a gold nugget is surrounded by 6 Harolds. Assume that all gold nuggets are of equal size.</p>
<blockquote>
<p>Let <span class="math inline">\(H\)</span> be the number of Harolds surrounding the nugget. <span class="math inline">\(H \sim Poi(\lambda = 6\ Harolds / nugget)\)</span>.</p>
</blockquote>
<ol type="1">
<li><p>What is the probability that a nugget is surrounded by more than 3 Harolds?</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}P(H &gt; 3) &amp;= 1 - P(H &lt;= 3) \\ &amp;= 1 - f(0) - f(1) - f(2) - f(3) \\ &amp;=  0.8488 \\ f(x) &amp;= \frac{e^{-6}6^x}{x!}\end{aligned}\]</span></p>
</blockquote></li>
<li><p>When 10 nuggets are picked at random, what is the probability that 8 of those nuggets have more than 3 Harolds?</p>
<blockquote>
<p><span class="math inline">\(Bin(n=10, p=0.8488)\)</span>. <span class="math inline">\(P(N=8) = {10 \choose 8}(0.8488)^8(1-0.8488)^2\)</span>.</p>
</blockquote></li>
<li><p>On 2 nuggets there are <span class="math inline">\(t\)</span> Harolds in total. What is the probability that <span class="math inline">\(x\)</span> of them are on the first of the two nuggets?</p>
<blockquote>
<p>Let <span class="math inline">\(A\)</span> be the event that there are <span class="math inline">\(t\)</span> Harolds on 2 nuggets. Let <span class="math inline">\(B\)</span> be the event that there are <span class="math inline">\(x\)</span> Harolds on the first nugget. <span class="math inline">\(P(B|A) = \frac{P(A \cap B)}{P(A)}\)</span>. Where <span class="math inline">\(A \cap B\)</span> is the event where <span class="math inline">\(x\)</span> Harolds are on the first nugget, and <span class="math inline">\(t-x\)</span> Harolds are on nugget 2.</p>
<p>These events are independent, so <span class="math inline">\(P(A \cap B) = \frac{e^{-6}6^x}{x!} \cdot \frac{e^{-6}6^{t-x}}{(t-x)!} = \frac{e^{-12}6^t}{x!(t-x)!}\)</span>.</p>
<p>We can double the original rate, <span class="math inline">\(Poi(12)\)</span> for the denominator. So <span class="math inline">\(P(A) = \frac{e^{-12}12^t}{t!}\)</span>.</p>
<p>So <span class="math inline">\(P(B|A) = {t \choose x}\frac{1}{2}^t\)</span>.</p>
</blockquote></li>
</ol>
<p>If you order a set of random variables, they become <strong>order statistics</strong>.</p>
<p>Question: Let <span class="math inline">\(X_1, X_2, X_3\)</span> denote the random variables for the outcome of three independent fair random number generators. Assume that their ranges are <span class="math inline">\(\{1, 2, ..., 10\}\)</span>. Now let <span class="math inline">\(X_{max}\)</span> denote the maximum value, then <span class="math inline">\(P(X_{max} \le x) = P(X_1 \le x)P(X_2 \le x)P(X_3 \le 3)\)</span>.</p>
<h1 id="expectation">Expectation</h1>
<p><strong>Definition</strong>: Let <span class="math inline">\(x_1, x_2, ..., x_n\)</span> be outcomes of random variable <span class="math inline">\(X\)</span>. Its <strong>sample mean</strong> is defined as <span class="math inline">\(\overline{x} = \frac{\sum_{i=1}^nx_i}{n}\)</span>.</p>
<p>We can calculate a theoretical mean of <span class="math inline">\(X\)</span> directly if we know its distribution. Suppose <span class="math inline">\(X\)</span> is a discrete random variable with probability function <span class="math inline">\(f(x)\)</span>, then <span class="math inline">\(E(X)\)</span> is called the <strong>expected value</strong> of <span class="math inline">\(X\)</span> and is defined by <span class="math inline">\(E(X) = \sum_{x \in X(S)}xf(x)\)</span>. The expected value is sometimes referred to as the <strong>mean</strong>, <strong>expectation</strong>, or <strong>first moment</strong> of <span class="math inline">\(X\)</span>.</p>
<p>Example: A lottery is conducted in which 7 numbers are drawn without replacement between the numbers 1 and 50. A player wins the lottery if the numbers selected on their ticket match all 7 of the drawn numbers. A ticket to play the lottery costs $1, and the jackpot is valued at $5000000. Compute the expected return.</p>
<blockquote>
<p>If you win the lottery, the return is 4999999. If you did not win, the return is -1. Let <span class="math inline">\(R\)</span> denote the return amount. <span class="math inline">\(E(R) = (-1)P(\overline{w}) + (4999999)P(w) &lt; 0\)</span>.</p>
</blockquote>
<h2 id="law-of-the-unconscious-statistiation">“Law of the Unconscious Statistiation”</h2>
<p>If <span class="math inline">\(g: \mathbb{R} \to \mathbb{R}\)</span>, then for a random variable <span class="math inline">\(X\)</span> with probability function $f(x), the expected value of $g(x) is given be <span class="math inline">\(\sum_{x \in S(X)}g(x)f(x)\)</span>.</p>
<p>To retrieve our original expectation function, we set <span class="math inline">\(g(x) = x\)</span>.</p>
<p>Example: If <span class="math inline">\(g(x) = x^2\)</span> and <span class="math inline">\(X\)</span> is the result of a fair six sided die roll, then compute <span class="math inline">\(E[g(X)]\)</span>.</p>
<blockquote>
<p><span class="math inline">\(E[g(x)] = E[x^2] = \sum_{x = 1}^6 x^2 \frac{1}{6}\)</span>.</p>
</blockquote>
<h2 id="linearity-of-expectation">Linearity of Expectation</h2>
<p>If <span class="math inline">\(g(x)\)</span> is a linear function <span class="math inline">\(g(x) = ax + b\)</span>, then for a random variable <span class="math inline">\(X\)</span>, <span class="math inline">\(E[aX + b] = aE[X] + b\)</span>.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}E[aX + b] &amp;= \sum_{x \in X(S)}(ax + b)f(x) \\ &amp;= a\sum_{x \in X(S)}xf(x) + b\sum_{x \in X(S)} \\ &amp;= aE[x] + b\end{aligned}\]</span></p>
</blockquote>
<p>Note: It is <strong>not true</strong> in general that <span class="math inline">\(g(E[X]) = E[g(X)]\)</span>.</p>
<p>An extention of linearity is, <span class="math inline">\(E[af(X) + bg(X)] = aE[f(X)] + bE[g(X)]\)</span>.</p>
<h2 id="expectation-of-binomial">Expectation of Binomial</h2>
<blockquote>
<p>If <span class="math inline">\(X \sim Bin(n, p)\)</span> then <span class="math inline">\(E[X] = np\)</span>.</p>
</blockquote>
<p><span class="math display">\[\begin{aligned}
E[X] &amp;= \sum_{x = 0}^n xf(x) \\
&amp;= \sum_{x = 1}^n xf(x) \\
&amp;= \sum_{x = 1}^n x \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\
&amp;= \sum_{x=1}^n \frac{n(n-1)!}{(x-1)!((n-1) - (x-1))!}pp^{x-1}(1-p)^{(n-1)-(x-1)} \\
&amp;= np(1-p)^{n-1} \sum_{x=1}^n \left({n - 1 \choose x-1}p^{x-1}(1-p)^{-(x-1)} \right) \\
&amp;= np(1-p)^{n-1} \sum_{x=1}^n \left( {n-1\choose x-1} \left(\frac{p}{1 - p}\right)^{x-1} \right) \\
&amp;= np(1-p)^{n-1} \sum_{y=0}^{n-1} \left( {n-1\choose y} \left(\frac{p}{1 - p}\right)^{y} \right) \\
&amp;= np(1-p)^{n-1} \left(1+\frac{p}{1-p}\right)^{n-1} \\
&amp;= np(1-p)^{n-1} \left(\frac{1}{1-p}\right)^{n-1} \\
&amp;= np
\end{aligned}\]</span></p>
<p>Example: Suppose two fair six sided die are independently rolled 24 times, at let <span class="math inline">\(X\)</span> denote the number of times the sum of die rolls is 7. Compute <span class="math inline">\(E[X]\)</span>.</p>
<blockquote>
<p>36 outcomes, 6 yield a sum of 7, so <span class="math inline">\(p = \frac{1}{6}\)</span>. We are rolling 24 times, so <span class="math inline">\(n = 24\)</span>. We have <span class="math inline">\(E[X] = np = 4\)</span>.</p>
</blockquote>
<h2 id="expectation-of-poisson">Expectation of Poisson</h2>
<blockquote>
<p>If <span class="math inline">\(Y \sim Poi(\lambda)\)</span>, then <span class="math inline">\(E[Y] = \lambda\)</span>.</p>
</blockquote>
<p><span class="math display">\[\begin{aligned}
E[Y] &amp;= \sum_{y \ge 0}yf(y) \\
&amp;= \sum_{y \ge 1}y\frac{e^{-\lambda}\lambda^y}{y!} \\
&amp;= \sum_{y \ge 1}\frac{e^{-\lambda}\lambda \lambda^{y-1}}{(y-1)!} \\
&amp;= e^{-\lambda}\lambda \sum_{y \ge 1}\frac{\lambda^{y-1}}{(y-1)!} \\
&amp;= e^{-\lambda}\lambda \sum_{z \ge 0}\frac{\lambda^{z}}{z!} \\
&amp;= e^{-\lambda}\lambda e^{\lambda} \\
&amp;= \lambda
\end{aligned}\]</span></p>
<p>Example: Suppose that calls to Canadian Tire Financial call center follow a Poisson process with rate 30 calls per minute. Let <span class="math inline">\(X\)</span> denote the number of calls to the center after 1 hour. Compute <span class="math inline">\(E[X]\)</span>.</p>
<blockquote>
<p><span class="math inline">\(E[X] = 30 * 60\)</span>.</p>
</blockquote>
<h2 id="expectation-of-hypergeometric">Expectation of Hypergeometric</h2>
<blockquote>
<p>If <span class="math inline">\(X \sim hyp(N, r, n)\)</span>, then <span class="math inline">\(E[X] = n\frac{r}{N}\)</span>.</p>
</blockquote>
<h2 id="expectation-of-negative-binomial">Expectation of Negative Binomial</h2>
<blockquote>
<p>If <span class="math inline">\(Y \sim NB(k, p)\)</span>, then <span class="math inline">\(E[Y] = \frac{k(1-p)}{p}\)</span>.</p>
</blockquote>
<h1 id="variability">Variability</h1>
<p>Expectation along may not be enough. Oftentimes, we want to study how much a random variable tends to deviate from its mean.</p>
<ol type="1">
<li><strong>Deviation</strong>: <span class="math inline">\(E[X - \mu] = E[X] - \mu\)</span>.</li>
<li><strong>Absolute Deviation</strong>: <span class="math inline">\(E[|X - \mu|]\)</span>.</li>
<li><strong>Squared Deviation</strong>: <span class="math inline">\(E[(X - \mu)^2]\)</span>. Turns out to be a useful measure of variability.</li>
</ol>
<h2 id="variance">Variance</h2>
<p>The <strong>variance</strong> of a random variable <span class="math inline">\(X\)</span> is denoted <span class="math inline">\(Var(X)\)</span> and is defined by <span class="math inline">\(Var(X) = E[(X - E[X])^2]\)</span>. A simple calculation gives the <em>short cut formula</em>, <span class="math inline">\(Var(X) = E[X^2] = E[X]^2\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
E[(X - E[X])^2] &amp;= E[X^2 - 2XE[X] + E[X]^2] \\
&amp;= E[X^2] - 2E[X]^2 + E[X]^2 \\
&amp;= E[X^2] - E[X]^2
\end{aligned}\]</span></p>
<p>We know that <span class="math inline">\(E[(X - E[X])^2] \ge 0\)</span> so we can say that <span class="math inline">\(E[X^2] - E[X]^2 \ge 0\)</span>.</p>
<h3 id="variance-of-linear-combination">Variance of Linear Combination</h3>
<p>For any random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(a, b \in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[ Var(aX + b) = a^2Var(X)\]</span></p>
<p><strong>Proposition</strong>: <span class="math inline">\(Var(X = 0)\)</span> if and only if <span class="math inline">\(P(X = E[X]) = 1\)</span>.</p>
<p>Example: Let <span class="math inline">\(X\)</span> denote the outcome of a fair six sided die. Compute <span class="math inline">\(Var(X)\)</span>.</p>
<blockquote>
<p><strong>Recall</strong>: <span class="math inline">\(Var(X) = E[X^2] - E[X]^2\)</span>.<br />
We know <span class="math inline">\(X \in \{1, .., 6\}\)</span>, <span class="math inline">\(X^2 \in \{1, ..., 36\}\)</span>.</p>
<p><span class="math inline">\(E[X] = 3.5 = \frac{7}{2}\)</span>.<br />
<span class="math inline">\(E[X^2] = \sum x^2f(x) = \frac{1}{6}(1 + 4 + 9 + 16 + 25 + 36) = \frac{91}{6}\)</span>. So, <span class="math inline">\(Var(X) = \frac{91}{6} - \frac{49}{4}\)</span>.</p>
</blockquote>
<h2 id="standard-deviation">Standard Deviation</h2>
<blockquote>
<p><strong>Note</strong>: <span class="math inline">\(Var(X)\)</span> is a squared unit. To recover the original unit, we take the square root of variance.</p>
</blockquote>
<p>We define the <strong>standard deviation</strong> of a random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(SD(X)\)</span>, where <span class="math inline">\(SD(X) = \sqrt{Var(X)}\)</span>.</p>
<h2 id="variability-of-binomial">Variability of Binomial</h2>
<blockquote>
<p>Suppose that <span class="math inline">\(X \sim Bin(n, p)\)</span>. Then <span class="math inline">\(Var(X) = np(1-p)\)</span>.</p>
</blockquote>
<h2 id="variability-of-poisson">Variability of Poisson</h2>
<blockquote>
<p>Suppose that <span class="math inline">\(X \sim Poi(\lambda)\)</span>. Then <span class="math inline">\(Var(X) = \lambda\)</span>.</p>
</blockquote>
<p>Example: Suppose that <span class="math inline">\(X_n\)</span> is binomial with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p_n\)</span>, so that <span class="math inline">\(np_n \to \lambda\)</span> as <span class="math inline">\(n \to \infty\)</span>. If <span class="math inline">\(Y \sim Poi(\lambda)\)</span>, show that <span class="math inline">\(\lim_{n \to \infty} Var(X_n) = Var(Y)\)</span>.</p>
<h2 id="variability-of-hypergeometric">Variability of Hypergeometric</h2>
<blockquote>
<p>Suppose that <span class="math inline">\(X \sim hyp(N, r, n)\)</span>. Then <span class="math inline">\(Var(X) = n\frac{r}{N}(1 - \frac{r}{N})(\frac{N - n}{N - 1})\)</span>.</p>
</blockquote>
<h2 id="variability-of-hypergeometric-1">Variability of Hypergeometric</h2>
<blockquote>
<p>Suppose that <span class="math inline">\(Z \sim NB(k, p)\)</span>. Then <span class="math inline">\(Var(Z) = \frac{k(1-p)}{p^2}\)</span>.</p>
</blockquote>
<p>Example: Suppose <span class="math inline">\(X_n\)</span> is binomial with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p_n\)</span> so that <span class="math inline">\(np_n \to \lambda\)</span> as <span class="math inline">\(n \to \infty\)</span>. If <span class="math inline">\(Y \sim Poi(\lambda)\)</span>, show that <span class="math inline">\(\lim_{n \to \infty}Var(X_n) = Var(Y)\)</span>.</p>
<blockquote>
<p><strong>Recall</strong>: <span class="math inline">\(Var(X_n) = np_n(1-p_n)\)</span>. We also know that <span class="math inline">\(\lim_{n \to \infty} np_n = \lambda\)</span> and <span class="math inline">\(\lim_{n \to \infty}(1 - p_n) = 1\)</span>. We can use the limit law. <span class="math display">\[\begin{aligned}\lim_{n \to \infty} Var(X_n) &amp;= \lim_{n \to \infty}np_n \lim_{n \to \infty}(1-p_n) \\ &amp;= \lambda \\ &amp;= Var(Y)\end{aligned}\]</span></p>
</blockquote>
<h1 id="skewness">Skewness</h1>
<p><span class="math display">\[E\left[\left(\frac{X - E[X]}{SD(X)}\right)^3\right]\]</span></p>
<h1 id="kurtosis">Kurtosis</h1>
<p><span class="math display">\[E\left[\left(\frac{X - E[X]}{SD(X)}\right)^4\right]\]</span></p>
<p><strong>Remark</strong>: There exists distributions without expectation. Suppose <span class="math inline">\(X\)</span> is a random variable with <span class="math inline">\(f_X(x) = \frac{6}{(\pi x)^2}\)</span>. Then <span class="math inline">\(E(X) = \infty\)</span> and <span class="math inline">\(Var(X)\)</span> is not defined.</p>
<p>Example: Let <span class="math inline">\(X \sim Geo(p)\)</span>. Compute <span class="math inline">\(E[X]\)</span>.</p>
<blockquote>
<p><span class="math inline">\(f(x) = p(1-p)^x\)</span>, <span class="math inline">\(x = 0, 1, 2 , ...\)</span>. <span class="math display">\[\begin{aligned}E[X] &amp;= \sum_{x \ge 1} xp(1-p)^x \\ &amp;= p(1-p)\sum_{x \ge 1}x(1-p)^{x-1}\end{aligned}\]</span> <strong>Note</strong>: <span class="math inline">\(\frac{d}{d(1-p)} \frac{1}{1 - (1-p)} = \sum_{x \ge 1} x(1-p)^{x-1}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}E[X] &amp;= p(1-p) \frac{d}{d(1-p)} \frac{1}{1 - (1-p)} \\ &amp;= p(1-p)\frac{1}{p^2} \\ &amp;= \frac{1-p}{p}\end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p><span class="math inline">\(f(x) = p(1-p)^x\)</span>, <span class="math inline">\(x = 0,1,2...\)</span>. <span class="math display">\[\begin{aligned}\sum_{x \ge 1}P(X \ge x) &amp;= \sum_{x \ge 1}(1-P(X \le x-1)) \\ &amp;= \sum_{x \ge 1}(1-(1-(1-p)^x)) \\ &amp;= \frac{1-p}{1 - (1-p)} \\ &amp;= \frac{1-p}{p}\end{aligned}\]</span></p>
</blockquote>
<p><strong>Theorem</strong>: Darth Vader Rule. Let <span class="math inline">\(X\)</span> be a non-negative discrete random variable.</p>
<p><span class="math display">\[E[X] = \sum_{x = 1}^\infty P(X \ge x) = \sum_{x \ge 0}P(X &gt; x)\]</span></p>
<p>Example: A person plays a game in which a fair coin is tossed until the first tail occurs. The person wins <span class="math inline">\(\$2^x\)</span> if <span class="math inline">\(x\)</span> tosses are needed for <span class="math inline">\(x = 1,2,3,4,5\)</span> but loses <span class="math inline">\(\$256\)</span> if <span class="math inline">\(x &gt; 5\)</span>.</p>
<ol type="1">
<li><p>Determine the expected winnings.</p>
<blockquote>
<p>Let <span class="math inline">\(W\)</span> be the amount of winnings. <span class="math inline">\(W = \begin{cases}2^x, &amp;X = 1, 2,...5 \\ -256, &amp;x &gt; 5\end{cases}\)</span>. <span class="math inline">\(W\)</span> is a function of <span class="math inline">\(X \sim Geo(p=\frac{1}{2})\)</span>. By law of unconcious statistician. <span class="math display">\[\begin{aligned}E[W] &amp;= \sum_{x = 1}^5 2^x P(X = x) - 256P(X &gt; 5) \\ &amp;= 2p + 2^2p(1-p) + ... 2^5p(1-p)^4 - 256(1-p)^5 \\ &amp;= 1 + 1 + 1 + 1 + 1 - \frac{2^8}{2^5} \\ &amp;= 5 - 8 \\ &amp;= -3\end{aligned}\]</span></p>
</blockquote></li>
<li><p>Determine the variance of the winnings.</p>
<blockquote>
<p><span class="math display">\[\begin{aligned}Var(W) &amp;= E[W^2] - E[W]^2 \\ &amp;= 2101 \text{ Dollars}^2\end{aligned}\]</span></p>
</blockquote></li>
</ol>
<p>Example: Yasmin and Zack are BMath students taking the exact same courses. Let <span class="math inline">\(X\)</span> be the number of assignments that they have in one week. The probability function of <span class="math inline">\(X\)</span> is.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(f(x)\)</span></td>
<td>0.09</td>
<td>0.1</td>
<td>0.25</td>
<td>0.4</td>
<td>0.15</td>
<td>0.01</td>
</tr>
</tbody>
</table>
<p>Yasmin drinks <span class="math inline">\(2X^2\)</span> cups per week and Zack drinks <span class="math inline">\(|2X - 1|\)</span> cups per week.</p>
<ol type="1">
<li><p>Compute the expected number of cups that Yasmin and Zack individually drink in a week.</p>
<blockquote>
<p><span class="math inline">\(E[2X^2] = \sum_{x = 0}^5 2x^2 P(X = x)\)</span></p>
<p><span class="math inline">\(E[|2X-1|] = \sum_{x = 0}^5 |2x - 1|P(X = x)\)</span>.</p>
</blockquote></li>
<li><p>Compute the variance individually.</p></li>
</ol>
</body>
</html>
